{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import wandb\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamW import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/homedepot/homedepot/runs/dgupdu59\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/homedepot/homedepot/runs/dgupdu59"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"homedepot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    " \n",
    "data_transforms = {\n",
    "    'train':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]),\n",
    "    'valid':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])}\n",
    " \n",
    "image_datasets = {\n",
    "    'train':\n",
    "        ImageFolder(os.path.join(path,'train'), data_transforms['train']),\n",
    "    'valid':\n",
    "        ImageFolder(os.path.join(path,'valid'), data_transforms['valid'])}\n",
    " \n",
    "dataloaders = {\n",
    "    'train':\n",
    "        torch.utils.data.DataLoader(\n",
    "            image_datasets['train'],\n",
    "            batch_size=8,\n",
    "            shuffle=True,\n",
    "            num_workers=4),\n",
    "    'valid':\n",
    "        torch.utils.data.DataLoader(\n",
    "            image_datasets['valid'],\n",
    "            batch_size=8,\n",
    "            shuffle=False,\n",
    "            num_workers=4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished setup\n"
     ]
    }
   ],
   "source": [
    "#im lazy\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.resnet50(pretrained=True).to(device)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, 8).to(device)\n",
    "    model.load_state_dict(torch.load('testsave.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize])\n",
    "    print('Finished setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Learning rate scheduler...]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=True).to(device)\n",
    " \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "ct = []\n",
    "for name, child in model.named_children():\n",
    "    if \"maxpool\" in ct:\n",
    "        for params in child.parameters():\n",
    "           params.requires_grad = True\n",
    "    ct.append(name)\n",
    "\n",
    "lr = 1e-5\n",
    "#optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model.parameters())), lr=lr, momentum=0.5, weight_decay=1e-4)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 8).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"[Creating Learning rate scheduler...]\")\n",
    "\n",
    "steps = len(dataloaders['train'])\n",
    "exp_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6e801608b34f63b3520bc2ddca4a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(dataloaders['train'], end_lr=1, num_iter=100)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler=None, num_epochs=3,cycle_mult=1):\n",
    "\n",
    "    steps = len(dataloaders['train'])\n",
    "    completed = 0\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    dataset_sizes = {'train':len(dataloaders['train'].dataset),'valid':len(dataloaders['valid'].dataset)}\n",
    "    \n",
    "    #wandb.watch(model)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        epoch_loss = 1.0\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                #if scheduler is not None:\n",
    "                #    scheduler.step(epoch_loss)\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    " \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    " \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    " \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    " \n",
    "                if phase == 'train':\n",
    "                    steps -= 1\n",
    "                    scheduler.step()\n",
    "                    #lrval = scheduler.get_lr()\n",
    "                    #wandb.log({'lr':lrval})\n",
    "                    #print(\"lr: \", lrval)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    " \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    " \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "            if phase == 'valid':\n",
    "                wandb.log({'epoch_loss':epoch_loss,'epoch_acc':epoch_acc})\n",
    "                \n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "                \n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "        lrval = scheduler.get_lr()[0]\n",
    "        print('steps: %d lr: %.4f'%(steps,lrval))\n",
    "        \n",
    "        if (steps <= 0):\n",
    "            completed += 1\n",
    "            steps = len(dataloaders['train']) * (completed * cycle_mult)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "            print(completed)\n",
    "\n",
    "        #steps = len(dataloaders['train'])\n",
    "        \n",
    "        if (completed == 11):\n",
    "            break;\n",
    "            \n",
    "        lrval = scheduler.get_lr()[0]\n",
    "        print('steps: %d lr: %.4f'%(steps,lrval))\n",
    "      \n",
    "    model = model.load_state_dict(best_model_wts)\n",
    "    return best_acc, model\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular sgd\n",
    "def train_model(model, criterion, optimizer, dataloaders, scheduler=None, num_epochs=3,cycle_mult=1):\n",
    "\n",
    "    steps = len(dataloaders['train'])\n",
    "    completed = 0\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    dataset_sizes = {'train':len(dataloaders['train'].dataset),'valid':len(dataloaders['valid'].dataset)}\n",
    "    \n",
    "    #wandb.watch(model)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    " \n",
    "        epoch_loss = 1.0\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    " \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    " \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    " \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    " \n",
    "                if phase == 'train':\n",
    "                    if scheduler is not None:\n",
    "                        steps -= 1\n",
    "                        scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    " \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    " \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "            if phase == 'valid':\n",
    "                #wandb.log({'epoch_loss':epoch_loss,'epoch_acc':epoch_acc})\n",
    "                \n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "        \n",
    "        \n",
    "        if scheduler is not None:\n",
    "            lrval = scheduler.get_lr()[0]\n",
    "            print('steps: %d lr: %.8f'%(steps,lrval))\n",
    "            \n",
    "        if (steps <= 0):\n",
    "            steps = len(dataloaders['train'])\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "\n",
    "        print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                    epoch_loss,\n",
    "                                                    epoch_acc))    \n",
    "    \n",
    "    torch.save(best_model_wts, 'testsave.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "###Using our saved model for inference and the creation of the confusion matrix\n",
    "classLabels = ['bathroom','bedroom','dining_room','entryway','kitchen','living_room','office','outdoor']\n",
    "\n",
    "predValues = []\n",
    "trueValues = []\n",
    "\n",
    "for inputs, labels in dataloaders['valid']:\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    print(type(inputs))\n",
    " \n",
    "    outputs = model(inputs)\n",
    "    _,predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    predList = predicted.tolist()\n",
    "    labelList = labels.tolist()\n",
    "    \n",
    "    for i in range(len(predList)):\n",
    "        predValues.append(classLabels[predList[i]])\n",
    "        trueValues.append(classLabels[labelList[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "#response = requests.get('https://files.slack.com/files-pri/T03PB1F2E-FLK0DNAFP/img_20190718_133448.jpg')\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "img = Image.open('../TestImages/test4.jpg')\n",
    "img = img.convert('RGB')\n",
    "img = data_transforms['valid'](img).unsqueeze(0)\n",
    "img = img.to(device)\n",
    "outputs = model(img)\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-20-a8168f806f61>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-a8168f806f61>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    plt\"\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(95.09375, 0.5, 'Actual'), Text(0.5, 68.09375, 'Predicted')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAJRCAYAAACtPMpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XmcFOW1//Hv6VlkR4nMoIHIHhXUqBi3EAFFDSJhEw2Ju5IYiZqEa0QSvXhdolnMTXKjwS3qz3g1bnG/MYKAu6AoJK4YRBQGRRBkm5me8/uje8ioMN3NTD1V0/N5++rXUNU9fU7Xq6p9OJx6HnN3AQAAAK1ZKu4EAAAAgLgxKAYAAECrx6AYAAAArR6DYgAAALR6DIoBAADQ6jEoBgAAQKvHoBgAAACtHoNiAAAAtHoMigEAANDqlcadQGN2/d49LLfXiLd/PzbuFAAAOdSk6+JOIdHKSqjP5dKmVBZ3Dp/Vdt/JwcZoG1/6fZDPz5kIAACAVo9BMQAAAFq9RLdPAAAAIIGs+OqqxfeJAAAAgAJRKQYAAEBhLHH3/jUZlWIAAAC0elSKAQAAUBh6igEAAIDiQ6UYAAAAhaGnGAAAACg+VIoBAABQGHqKAQAAgOLDoBgAAACtHu0TAAAAKAw32gEAAADFh0oxAAAACsONdgAAAEDxoVIMAACAwtBTDAAAABQfKsUAAAAoDD3FAAAAQPGhUgwAAIDC0FMMAAAAJIuZlZjZS2b2YHa7l5k9Z2ZvmtkdZlae6z0YFAMAAKAwlgr3yM+5kl5tsH2lpKvdvZ+k1ZJOz/UGDIoBAADQYplZd0nHSLo+u22Shkm6K/uSmyWNzvU+9BQDAACgMAF7is1skqRJDXbNcPcZDbZ/I+l8SR2z21+QtMbda7PbyyR9MVccBsUAAABIrOwAeMbWnjOzkZJWuvt8MxtSv3trb5MrDoNiAAAAFCY58xQfKmmUmY2Q1EZSJ2UqxzuaWWm2Wtxd0vu53igxnwgAAAAohLtPdffu7t5T0gmSZrr7tyXNkjQ++7KTJf0113sxKN6GHUpTeuiCIXrsp8M066IjNGXkHpKkX524nx776TD9/aeHa8akA9Vuh5KYM02Gp+bO0ahjjtLIo4frhuu2+i8crR7HqHEcn9w4RrlxjBo3/aJpGn7YoZow5ti4U0kszqGi8RNJPzKzt5TpMb4h1y8wKN6GzbV1Ou7quRp+6UwNv/RxDRlQqf167aSL//KKhl86U0dc+rje+2iDThvSJ+5UY5dOp3X5ZZfoD9der3vvf0iPPvygFr/1VtxpJQrHqHEcn9w4RrlxjHI7dtRo/e4aBnrbwjlUgORNySZ3f8LdR2b//La7f9Xd+7r7ce6+OdfvMyhuxIbNaUlSWUlKZSUpuUufbKrd8nybshJ5zrbt4rdo4Svq0WM3de/RQ2Xl5Tp6xDF6YtbjcaeVKByjxnF8cuMY5cYxym2/QQeoU+cd404jsTiHWrfIb7Qzsx0lnSSpZ8N47n5O1LGbKmXS/104TD27dtCfZi/WS0tWS5KuPml/DRtYqTeWr9Mldy2MOcv4rayqUrddum3Zrqis1MJXXokxo+ThGDWO45Mbxyg3jhGainOoACmWed4eDyszIF4oaX6Dx1aZ2SQzm2dm8zb8828B0tu2OpeGXzZT+099RF/p2UVf3rWTJOmHt8zXvj95WG+uWKdRg7rHmmMS+FZmObEiXBO9KThGjeP45MYxyo1jhKbiHGrdQkzJ1sbdf5TvixvORbfr9+5JRHPC2o01euaNDzR0QKVef3+tpMyA+f55y3TW8H6645l3Ys4wXpWV3bRi+Yot2yurqlRRURFjRsnDMWocxyc3jlFuHCM0FedQAZIzJVuzCfGJbjWzM81sFzPrUv8IELdJunQoV6e2ZZKkNmUpDd69QotXrFPPru23vObIvbtpcdW6uFJMjAED99LSpUu0bNm7qqmu1qMPP6TDhg6LO61E4Rg1juOTG8coN44RmopzqHULUSmulvQLSdP079VEXFLvALG3W2XnNvrvkwcplTKlTHpg/nv6+6IVum/KYerQplQm6Z/vfawL/rwg7lRjV1paqqnTLtJZk85QXV1ao8eMU9++/eJOK1E4Ro3j+OTGMcqNY5Tbhef/WPPnPa81a9ZoxBFDNOn7kzV67Pjcv9hKcA4VoAjbSswjnj7BzBZLOtDdPyz0d5PSPpFUb/9+bNwpAAByqEnXxZ1CopWVFN8/wze3bC0uUdoefnmwMdrGxy8M8vlDVIr/IWlDgDgAAAAIoQh7ikMMitOSFpjZLElbJk5uCVOyAQAAoHUIMSi+L/sAAABAMSjCnuLIB8XufrOZlUvqn931urvXRB0XAAAAyFeIFe2GSLpZ0hJJJqmHmZ3s7nOijg0AAIAI0FO8XX4l6Uh3f12SzKy/pNsl7R8gNgAAAJBTiEFxWf2AWJLc/Q0zKwsQFwAAAFGgp3i7zDOzGyTdmt3+tqT5AeICAAAAeQkxKD5L0tmSzlGmp3iOpD8EiAsAAADkJcTsE5vN7PeSHlNmeWdmnwAAAGjJuNGucMw+AQAAgKRj9gkAAAAUpghvtAtR+/7c7BOSmH0CAAAAicHsEwAAACgMPcXbhdknAAAAkGiRDorNrETSDe7+HUm/jjIWAAAAAqGnuDDunpbU1czKo4wDAAAANEWI9oklkp4ys/slra/f6e5UjgEAAFoieoq3y/vZR0pSxwDxAAAAgIKEWNFuetQxAAAAEBCV4sJlF+uYIqlnw3juPizq2AAAAEA+QrRP/EXStZKul5QOEA8AAABRKsLZJ0IMimvd/ZoAcQAAAIDtEtmg2My6ZP/4gJl9X9K9kjbXP+/uH0UVGwAAABGip7gg8yW5MqvYSdJ/NHjOJfWOMDYAAACQt8gGxe7eS5LMrI27b2r4nJm1iSouAAAAIlaEPcUhat9P57kPAAAAiEWUPcXdJH1RUlsz21f/bqPoJKldVHEBAACAQkXZU3yUpFMkdZfUcEnndZIujDAuAAAAolSEN9qZu0cbwGycu9+9Pb+7qVbRJtfC7TR+RtwpJN7quybFnUKirfqkOu4UEu8LHcrjTiHRatJ1caeQeGUlxTd4QFhtSpW4Bt62Y64PNkbbeO8ZQT5/iGWe7zazYyQNkNSmwf5Loo4NAACACHCjXeHM7FpJx0v6gTJ9xcdJ2i3quAAAAEC+QvybziHufpKk1e4+XdLBknoEiAsAAIAImFmwRyghBsUbsz83mNmukmok9QoQFwAAAMhL5D3Fkh40sx0lXaXMKneSdH2AuAAAAIhAyApuKCEGxb+UdJakwZKekTRX0jUB4gIAAAB5CTEovlmZuYl/m93+lqRbJE0IEBsAAADNrfgKxUEGxV92930abM8ys5cDxAUAAADyEmJQ/JKZHeTuz0qSmR0o6akAcQEAABABeooLYGYLJbmkMkknmdnS7PZukv4ZVVwAAACgUFFWikdG+N4AAACICZXiArj7O1G9NwAAANCcQvQUAwAAoIgUY6U4xIp2AAAAQKIxKAYAAECrR/sEAAAACkL7BAAAAFCEqBQDAACgMMVXKKZSDAAAAFApBgAAQEHoKQYAAACKEJViAAAAFIRKMQAAAFCEqBQDAACgIFSKAQAAgCJEpRgAAAAFoVIMAAAAFCEqxQAAAChM8RWKqRQDAAAADIrz9NTcORp1zFEaefRw3XDdjLjTid0OZSWae9VoPXf1OM3/7Xj99IT9JUl/v/xYPXv1WD179Vi9feO3defUI2PONDk4h3JLp9OadOJxuvBHZ8edSiJxDjVu+kXTNPywQzVhzLFxp5JYnEO5cYzyY2bBHqHQPpGHdDqtyy+7RH+87iZVVlZq4vHjNWToMPXp2zfu1GKzuSatoy96UOs31aq0xDTzim/qby++qyMufGDLa27/yXA98NyS+JJMEM6h/Nxzx//Tl3r20ob16+NOJXE4h3I7dtRoHX/CRF007YK4U0kkzqHcOEatG5XiPCxa+Ip69NhN3Xv0UFl5uY4ecYyemPV43GnFbv2mWklSWUlKpSUpufuW5zq0KdNhe+3KoDiLcyi3D6pW6Nmn5mrEN8fFnUoicQ7ltt+gA9Sp845xp5FYnEO5cYxat8gHxWbWy8x+bWb3mNn99Y+o4zanlVVV6rZLty3bFZWVqqqqijGjZEilTM9ePVZLbz5JM19ephfe/GDLc6MO6qknXnlP6zbWxJhhcnAO5fY/V1+l707+oVLG39W3hnMITcU5lBvHKH/F2D4R4v8+90laIul3kn7V4LFVZjbJzOaZ2byk9PK4/HP7inF+vkLV1bkO+uE96nvGbRrUr0J7fmmnLc9NGNxHd85dHGN2ycI51LhnnpytHbt0Uf89BsSdSmJxDqGpOIdy4xi1biF6ije5+2/zfbG7z5A0Q5I21W7l7IxBZWU3rVi+Ysv2yqoqVVRUxJhRsny8vlpzFr2vI/ftoX8uXa0uHXfQoH4VOv7nj8WdWmJwDjVu0csv6ek5s/Tc03NVvXmzNqxfr8svvkAXTv953KklBucQmopzKDeOUf6K8S8LISrF/21mF5vZwWa2X/0jQNxmM2DgXlq6dImWLXtXNdXVevThh3TY0GFxpxWrnTu1Uef25ZKkNuUlGrbPF/X6e2skSWMP6a1H5i3V5pp0nCkmCudQ4848+zzd+eDjuv2+/9PPLv2F9h30VQbEn8E5hKbiHMqNY9S6hagU7yXpREnDJNVl93l2u0UoLS3V1GkX6axJZ6iuLq3RY8apb99+cacVq247tdN15w5RScqUMtPdT72tR+YtlSQdN7iPfnn3gpgzTBbOITQV51BuF57/Y82f97zWrFmjEUcM0aTvT9bosePjTisxOIdy4xgVoPgKxbKGMwZEEsDsNUl7u3t1ob+blPaJpNppfDJ6rpNs9V2T4k4h0VZ9UvBl2ep8oUN53CkkWk26LveLWrmyEm4eRdO0KU3eELTi9DuDjdFW3jAhyOcPUSl+WdKOklYGiAUAAICIFWNPcYhBcaWk18zsBUmb63e6+6gAsQEAAICcQgyKLw4QAwAAAIFQKd4O7j7bzColHZDd9by700oBAACAxAixot0ESc9LOk7SBEnPmRm3AwMAALRQxbiiXYj2iWmSDqivDptZV0l/l3RXgNgAAABATiEGxanPtEusUphFQwAAABABeoq3z6Nm9n+Sbs9uHy/p4QBxAQAAgLyEuNHuP8xsrKSvKbP+yQx3vzfquAAAAIhI8RWKg1SKJekpSTXKLO/8fKCYAAAAQF5Czj4xXsw+AQAAgARi9gkAAAAUpBhvtAsxCwSzTwAAACDRmH0CAAAABSnGSjGzTwAAAKDVi3RQbGYlkv7P3Y+QdE+UsQAAABBGMVaKI+3tdfe0pA1m1jnKOAAAAEBThOgp3iRpoZk9Jml9/U53PydAbAAAADS34isUBxkUP5R9AAAAAIkU4ka7m6OOAQAAgHCS1FNsZm0kzZG0gzJj27vc/WIzu03SIGVWVX5e0nfdvWZb7xPZoNjMFiqzrPNWufveUcUGAABAq7FZ0jB3/8TMyiQ9aWaPSLpN0neyr/mzpDMkXbOtN4myUjwy+/Ps7M9bsz+/LWlDhHEBAAAQoSRVit3dJX2S3SzLPtzdt6yLYWbPS+re2PtENvuEu7/j7u9IOtTdz3f3hdnHBZKOiiouAAAAWhczKzGzBZJWSnrM3Z9r8FyZpBMlPdrYe4RYbrm9mX2tfsPMDpHUPkBcAAAARMDMQj4mmdm8Bo9Jn83H3dPu/hVlqsFfNbOBDZ7+g6Q57j63sc8UYvaJ0yXdmJ2r2CV9LOm0AHEBAADQwrn7DEkz8nztGjN7QtLRkhaZ2cWSukr6bq7fDTH7xHxJ+5hZJ0nm7h9HHRMAAADRSVJPsZl1lVSTHRC3lXSEpCvN7AxlWnYPd/e6XO8T+aDYzColXS5pV3f/hpntKelgd78h6tgAAAAoertIutnMSpRpDb7T3R80s1pJ70h6JjuIv8fdL9nWm4Ron/iTpJskTctuvyHpDkkMigEAANAk7v6KpH23sr+gcW6IG+12dvc7JdVJkrvXSkoHiAsAAIAoWMBHICEqxevN7AvKLuRhZgcpc7MdmmjlHWfEnULi7XT0z+NOIdFWP3pB3CkkXk06Zxtaq1ab3uYaTdiCcwhN06Y0RA0TIQbFP5J0v6TeZvaUMncAjg8QFwAAABFI0o12zSXEoPifku5VZhW7dZLuU6avGAAAAEiEEIPiWyStVWYGCkn6ljJLPh8XIDYAAACaGZXi7fNld9+nwfYsM3s5QFwAAAAgLyE6t1/K3lwnSTKzAyU9FSAuAAAAImAW7hFKZJViM1uozIwTZZJOMrOl2e3dlOkzBgAAABIhyvaJkRG+NwAAAGJCT3EB3P2dqN4bAAAAaE4hbrQDAABAESnCQnGQG+0AAACARKNSDAAAgIIUY08xlWIAAAC0elSKAQAAUJAiLBRTKQYAAAAYFAMAAKDVo30CAAAABUmliq9/gkoxAAAAWj0qxQAAACgIN9oBAAAARYhKMQAAAArC4h0AAABAEaJSDAAAgIIUYaGYSjEAAABApRgAAAAFoacYAAAAKEJUigEAAFAQKsUAAABAEaJSDAAAgIIUYaGYSjEAAABApThPT82doyt/fpnq0nUaM+44nX7mpLhTSpTpF03Tk7Of0E5duujOex+IO51E2KGsRH+/+tsqLytVaYnp3jmv69JbntRhX9lNV3x3qMpLS/TSmyv0vV8+rHSdx51u7LjGGsc1llvViuWa/rOpWrXqQ6XMNHrcBB0/8cS400oUzqPGcXzyR09xK5VOp3X5ZZfoD9der3vvf0iPPvygFr/1VtxpJcqxo0brd9fMiDuNRNlck9bRU27Xgd+9UQd+9yYdeUBvHbTnF3X9+cfopEv/qkFn3qClVWv1nSP3ijvV2HGN5cY1lltJSanO+dH5uuOeB3X9Lf+ru+74s/61mPOoIc6jxnF8WjcGxXlYtPAV9eixm7r36KGy8nIdPeIYPTHr8bjTSpT9Bh2gTp13jDuNxFm/qUaSVFaaUmlpSum6Om2uSeut91ZLkmbO/5dGD/5ynCkmAtdYblxjue3ctat232NPSVL79u3Vs1dvrfxgZcxZJQvnUeM4Pq1bkEGxme1kZnub2X71jxBxm8vKqip126Xblu2KykpVVVXFmBFailTK9Oy1p2rpXedo5vwleuG15SorTWm//pnzaczXd1f3io4xZxk/rjE0t/fff09vvP6qBg7cO+5UgKJkFu4RSuQ9xWb2X5JOkbRYUn3jpEsato3XT5I0SZJ+/4c/JqKv0PX5fs9i7KVB86urcx30vZvUuf0OumP6WO3Zc2eddOlfddVZh2d6juf/S7Vp+om5xtCcNmxYr6lTztV5U6aqfYcOcacDoIUIcaPdBEl93L06nxe7+wxJMyRpU+1W/k8Zg8rKblqxfMWW7ZVVVaqoqIgxI7Q0H6/frDkvL9WRB/TWb/7yvI744W2SpMP376l+3bvEnF38uMbQXGprajR1ynk66hsjNfTw4XGnAxStYixchGifWCSpRTfoDBi4l5YuXaJly95VTXW1Hn34IR02dKuFbmCLnTu3Vef2O0iS2pSXath+PfX60lXqumM7SVJ5WYl+fPxBuu6BBXGmmQhcY2gO7q7Lpv9MPXv11sQTT4k7HQAtTIhK8RWSXjKzRZI21+9091EBYjeL0tJSTZ12kc6adIbq6tIaPWac+vbtF3daiXLh+T/W/HnPa82aNRpxxBBN+v5kjR47Pu60YtWtSwdd95ORKkmZUma6e/ZreuS5xbp80lB948A+SqVM1z3wkmYveCfuVGPHNZYb11huLy94UY88dL/69OuvE48fI0k6a/J5OmTwYTFnlhycR43j+OSvCAvFMvdoOxTM7B+S/ihpoaS6+v3uPjvX7yalfSKpatJ1uV/UylUcc1XcKSTa6kcviDuFxOM6axw98bmVlhTh6AFBddwhlbiTaNCls4Jd/PN+OjTI5w9RKf7Q3X8bIA4AAAACKMae4hCD4vlmdoWk+/Xp9okXA8QGAAAAcgoxKN43+/OgBvu2OSUbAAAAkq0IC8XRD4rdfWjUMQAAAICmCLF4R2dJF0v6enbXbEmXuPvHUccGAABA8yvGnuIQ8xTfKGmdMot4TJC0VtJNAeICAAAAeQnRU9zH3cc12J5uZqxWAAAA0EIVYaE4SKV4o5l9rX7DzA6VtDFAXAAAACAvISrFZ0m6OdtbbJI+knRygLgAAACIQDH2FIeYfWKBpH3MrFN2e23UMQEAAIBCRN4+YWadzezXkmZKmmlmv8pWjQEAAIBEYPYJAAAAFMQs3CMUZp8AAABAqxdiULzRzL7m7k9KzD4BAADQ0nGj3fb5nqRbGvQRrxazTwAAACBBIh0Um1lK0pfdndknAAAAikQRFoqjvdHO3eskTc7+eS0DYgAAACRRiPaJx8xsiqQ7JK2v3+nuHwWIDQAAgGZGT/H2OS378+wG+1xS7wCxAQAAgJxCrGjXq7HnzWy4uz8WdR4AAABoHsVYKQ6xeEcuV8adAAAAAFq3EO0TuRTfXzUAAACKWBEWihNRKfa4EwAAAEDrloRKMQAAAFoQeoqjsSTuBAAAANC6RV4pNrOxW9n9saSF7r7S3bf2PAAAABKqCAvFQdonTpd0sKRZ2e0hkp6V1N/MLnH3WwPkAAAAAGxTiEFxnaQ93L1KksysUtI1kg6UNEcSg2IAAADEKsSguGf9gDhrpaT+7v6RmdUEiA8AAIBmVIw32oUYFM81swcl/SW7PU7SHDNrL2lNY7+4sToddW4tWtvykrhTSLz37/+PuFNItLtfXhZ3Cok3bp/ucaeQaBurqW3kUsP/yhrVqW1Z3CkAksIMis9WZiB8qDILddwi6W53d0lDA8QHAABAMyrCQnH0g+Ls4Peu7AMAAABInFBTsl0pqUKZSrEpM1buFHVsAAAANL9UEZaKQ7RPXCXpWHd/NUAsAAAAoGAhBsVVDIgBAACKRxEWioMMiueZ2R2S7pO0uX6nu98TIDYAAACQU4hBcSdJGyQd2WCfS2JQDAAA0AIxT/F2cPdTo44BAAAANEVkg2IzO9/drzKz3ylTGf4Udz8nqtgAAACITqr4CsWRVorrb66bF2EMAAAAoMkiGxS7+wPZnzdHFQMAAADh0VO8Hcysv6Qpkno2jOfuw6KODQAAAOQjxOwTf5F0raTrJaUDxAMAAECEirBQHGRQXOvu1wSIAwAAAGyXVIAYD5jZ981sFzPrUv8IEBcAAADIS4hK8cnZn//RYJ9L6h0gNgAAAJqZqfj6J0Is3tEr6hgAAABAU0S5eMcwd59pZmO39ry7s8wzAABAC8TiHYX5uqSZko5Vpl3CPvOTQTEAAAASIcpB8Toz+5GkRfr3YFjaypLPAAAAaDlYvKMwHbI/vyzpAEl/VWZgfKykORHGBQAAAAoS5TLP0yXJzP4maT93X5fd/k9lFvQAAABAC1SEheIg8xR/SVJ1g+1qZZZ8BgAAABIhxDzFt0p63szuVaafeIykmwPEBQAAQARSRVgqDjFP8WVm9oikwdldp7r7S1HHBQAAAPIVolIsd39R0oshYgEAACBaSSoUm1kPSbdI6iapTtIMd//vBs9PkfQLSV3d/cNtvU+QQTEAAAAQkVpJP3b3F82so6T5ZvaYu/8zO2AeLmlprjcJcaMdAAAAioiZBXvk4u7Ls10Jys529qqkL2afvlrS+cpjnQwGxQAAAEgsM5tkZvMaPCY18tqekvaV9JyZjZL0nru/nE8c2ifyULViuab/bKpWrfpQKTONHjdBx088Me60EuWpuXN05c8vU126TmPGHafTz9zm+doqcQ5t3f1//IXeeOlZte+0o8666gZJ0hN33ayXZj2kdp12lCQNm3C6+u17YJxpJgbXWeM2b96sH5x5sqprqpVOpzXk8OE6/buT404rMTg++eE6y0/InmJ3nyFpRq7XmVkHSXdLOk+Zloppko7MNw6D4jyUlJTqnB+dr9332FPr16/XKRPH66sHHqxeffrGnVoipNNpXX7ZJfrjdTepsrJSE48fryFDh6lPX45PPc6hrdvn60fpgCO/qfuuufJT+w/8xngdMnJCTFklE9dZbuXl5frNtTeqXbt2qq2t0fdPP0kHHTJYA/baJ+7UEoHjkxvXWctlZmXKDIhvc/d7zGwvSb0kvZxtwegu6UUz+6q7r9jae0TePmFmXaKOEbWdu3bV7nvsKUlq3769evbqrZUfrIw5q+RYtPAV9eixm7r36KGy8nIdPeIYPTHr8bjTShTOoa3bbY+91bZDp7jTaBG4znIzM7Vr106SVFtbq9ra2mTdIh8zjk9uXGf5S5kFe+RimVHvDZJedfdfS5K7L3T3Cnfv6e49JS1TZoXlrQ6IpTA9xc+Z2V/MbITl0y2dcO+//57eeP1VDRy4d9ypJMbKqip126Xblu2KykpVVVXFmFGycQ7l9sLf7tO1PzlD9//xF9r4ybq400kErrP8pNNpnTpxnEYN/7oOOPBgDeA6+xSOT+O4zlqsQyWdKGmYmS3IPkYU+iYhBsX9lekDOVHSW2Z2uZn139aLGzZT/+nG6wKkl78NG9Zr6pRzdd6UqWrfoUPc6SSGb+WGziL4+08kOIdyGzT8WP3gN7fqu1fMUIcdu+ix266NO6VE4DrLT0lJiW768926++HH9eo/Furtt96MO6VE4fg0juusZXL3J93d3H1vd/9K9vHwZ17Ts7E5iqUAg2LPeMzdvyXpDEknK7Ps82wzO3grr5/h7oPcfdApp50ZdXp5q62p0dQp5+mob4zU0MOHx51OolRWdtOK5f/+14iVVVWqqKiIMaNk4hzKT4fOXZRKlchSKe037Bi9t/i1uFNKBK6zwnTs2En77n+AnnvmybhTSSSOz9ZxneXPAj5CCdFT/AUzO9fM5kmaIukHknaW9GNJf446fnNwd102/Wfq2au3Jp54StzpJM6AgXtp6dIlWrbsXdVUV+vRhx/SYUOHxZ1WonAO5W/d6lVb/vzaC0+qonvP+JJJEK6z3Fav/kjr1q2VJG3etEnznn9WX+rZK+askoPjkxvXWesWYvbMnTiSAAAgAElEQVSJZyTdKmm0uy9rsH+embWIfxd9ecGLeuSh+9WnX3+dePwYSdJZk8/TIYMPizmzZCgtLdXUaRfprElnqK4urdFjxqlv335xp5UonENbd/fvLtU7r76sDes+1tWTj9eQcSdryasvq+qdxZKkHbt20zGn/zDmLJOB6yy3VR9+oMsvnqZ0XVpe5xo6/CgdOnhI3GklBscnN66z/BVjW4m551zgo2kBzMy3M8jqDelok2vh2paXxJ1C4m2sTsedQqI9/OryuFNIvHH7dI87hURbu7Em7hTQwnVqWxZ3ConXpjRoF0FevnXLgmBjtNtP+kqQzx+iUryzmZ0vaYCkNvU73Z1/jwAAAGiBUokbpjddiNknbpP0mjITKE+XtETSCwHiAgAAAHkJMSj+grvfIKnG3We7+2mSDgoQFwAAABEws2CPUEK0T9Q3nC03s2Mkva/MUnsAAABAIoQYFF9qZp2VmYLtd5I6SeJ2cgAAgBaqCCefCDIo/ru7b5L0saShAeIBAAAABQkxKF5kZlWS5kqaI+kpd/84QFwAAABEoBjnKQ6xzHNfSd+StFDSSEkvm9mCqOMCAAAA+Yq8Umxm3SUdKmmwpH0k/UMSi60DAAC0UMU4T3GI9omlysxLfLm7fy9APAAAAKAgIQbF+0r6mqSJZnaBpDclzc7OXQwAAIAWphh7iiMfFLv7y2a2WNJiZVooviPp65IYFAMAACARQvQUz5O0g6Snlekl/rq7vxN1XAAAACBfIdonRrj7ygBxAAAAEEDxNU8EmJJN0nNm9gsz2zNALAAAAKBgISrFe0s6QdL1ZpaSdKOk/3X3tQFiAwAAoJmlWtONdmb2gCTf1vPuPiqfAO6+TtJ1kq4zs69Lul3S1WZ2l6T/cve3CksZAAAAaF6NVYp/2RwBzKxE0jGSTpXUU9KvJN2mzEwUD0vq3xxxAAAAEEYRFoq3PSh299nNFONNSbMk/cLdn26w/65s5RgAAACIVc6eYjPrJ+kKSXtKalO/39175/G7JZL+5O6XbO15dz8n/1QBAACQBMW4eEc+s0/cJOkaSbWShkq6RdKt+by5u6ezvwMAAAAkVj6D4rbu/rgkc/d33P0/JQ0rIMbTZvZ7MxtsZvvVP7YrWwAAAMTOLNwjlHymZNuUnUrtTTObLOk9SRUFxDgk+7NhC4WrsIE1AAAAEJl8BsXnSWon6RxJ/6XMYPbkAmKc7u5vN9xhZjn7kQEAAJBMrWqe4nru/kL2j58oM61aoe6S9Nl2ib9I2n873gsAAABodvnMPjFLW1nEw90bbX8ws90lDZDU2czGNniqkxrMYgEAAICWpQgLxXm1T0xp8Oc2ksYpMxNFLl+WNFLSjpKObbB/naQz800QAAAAiFo+7RPzP7PrKTPLubCHu/9V0l/N7GB3f2Z7EwQAAECyFOM8xfm0T3RpsJlSphe4WwEx3jKzC5VZ4nlLPHc/rYD3AAAAACKTT/vEfGV6ik2Ztol/STq9gBh/lTRX0t8lpQtNEEB0Rg3cNe4UEm+n8TPiTiHRVt81Ke4UEq8mXRd3CgDykM+geA9339Rwh5ntUECMdu7+k8LSAgAAQFLls/pbS5PPZ3p6K/sK6RF+0MxGFPB6AAAAIKhtVorNrJukL0pqa2b7KtM+IWWmVGtXQIxzJU01s2pJNdn3cXfvtH0pAwAAIE6t7Ua7oySdIqm7pF/p34PitZIuLCBGZ0nfltTL3S8xsy9J2qXwVAEAAIBobHNQ7O43S7rZzMa5+91NiPE/kuqUWR76EmXmKb5b0gFNeE8AAADEJFV8heK8eor3N7Md6zfMbCczu7SAGAe6+9mSNkmSu6+WVF5YmgAAAEB08hkUf8Pd19RvZAe1hdw4V2NmJcouFW1mXZWpHAMAAKAFSlm4R7DPlMdrShpOwWZmbSUVMiXbbyXdK6nCzC6T9KSkywvKEgAAAIhQPvMU/z9Jj5vZTdntUyXdnG8Ad7/NzOZLOlyZm/VGu/urBWcKAACARGhts09Iktz9KjN7RdIRygxqH5W0WyFB3P01Sa9tV4YAAABAxPKpFEvSCmX6gCcos8xzU2ajAAAAQAtWjLNPNLZ4R39JJ0j6lqRVku6QZO4+NFBuAAAAQBCNVYpfkzRX0rHu/pYkmdkPg2QFAACAxCrCluJGZ58Yp0zbxCwzu87M6m+UAwAAAIpKYyva3SvpXjNrL2m0pB9KqjSzayTd6+5/C5QjAAAAEiRVhKXinPMUu/t6d7/N3UdK6i5pgaQLIs8MAAAACCSfxTu2cPeP3P2P7j4sqoQAAACA0PKdkg0AAACQVGBVtYUoxs8EAAAAFIRKMQAAAApShPfZUSkGAAAAqBQDAACgIK1ySjYAAACg2FEpBgAAQEGKsFBMpRgAAACgUgwAAICCpKgUAwAAAMWHSjEAAAAKUoyzTzAozkPViuWa/rOpWrXqQ6XMNHrcBB0/8cS400qUp+bO0ZU/v0x16TqNGXecTj9zUtwpJQrnUG7TL5qmJ2c/oZ26dNGd9z4QdzqJsENZif5+2bEqLytRaYnp3qf/pUv/d77+fvmx6tC2TJJU0bmt5r35gSZc8beYs00Gvosax3WWG+dQ68WgOA8lJaU650fna/c99tT69et1ysTx+uqBB6tXn75xp5YI6XRal192if543U2qrKzUxOPHa8jQYerTl+NTj3Mot2NHjdbxJ0zURdMuiDuVxNhck9bRFz2o9ZtqVVpimnnFN/W3F9/VERf+ezBz+0+G64HnlsSXZILwXZQb11njOIfyV4SFYnqK87Fz167afY89JUnt27dXz169tfKDlTFnlRyLFr6iHj12U/cePVRWXq6jRxyjJ2Y9HndaicI5lNt+gw5Qp847xp1G4qzfVCtJKitJqbQkJXff8lyHNmU6bK9dGRRn8V2UG9dZ4ziHWjcGxQV6//339Mbrr2rgwL3jTiUxVlZVqdsu3bZsV1RWqqqqKsaMko1zCIVIpUzPXj1WS28+STNfXqYX3vxgy3OjDuqpJ155T+s21sSYYXLwXYSm4hzKX8rCPYJ9pqgDmNlYM3vTzD42s7Vmts7M1kYdNwobNqzX1Cnn6rwpU9W+Q4e400kMl39unxXjv6s0A84hFKquznXQD+9R3zNu06B+FdrzSztteW7C4D66c+7iGLNLFr6L0FScQ61biErxVZJGuXtnd+/k7h3dvdO2Xmxmk8xsnpnN+9ON1wVILz+1NTWaOuU8HfWNkRp6+PC400mUyspuWrF8xZbtlVVVqqioiDGjZOIcQlN8vL5acxa9ryP37SFJ6tJxBw3qV6FH5i2NObPk4LsITcU51LqFGBRXufur+b7Y3We4+yB3H3TKaWdGmVfe3F2XTf+ZevbqrYknnhJ3OokzYOBeWrp0iZYte1c11dV69OGHdNjQYXGnlSicQ9geO3dqo87tyyVJbcpLNGyfL+r199ZIksYe0luPzFuqzTXpOFNMFL6L0FScQ/mzgP+FEmL2iXlmdoek+yRtrt/p7vcEiN0sXl7woh556H716ddfJx4/RpJ01uTzdMjgw2LOLBlKS0s1ddpFOmvSGaqrS2v0mHHq27df3GklCudQbhee/2PNn/e81qxZoxFHDNGk70/W6LHj404rVt12aqfrzh2ikpQpZaa7n3p7S2X4uMF99Mu7F8ScYbLwXZQb11njOIdaN2t4J3MkAcxu2spud/fTcv3u6g3paJNr4dqWl8SdQuJtrKaK1pjSEnrlcqk4/vq4U0i01Xcxh2suNem6uFNItLIS7vnPpU1pwHJpnn4+c3GwMdoFw/oE+fyRV4rd/dSoYwAAAABNEWL2if5m9riZLcpu721mP406LgAAAKLBlGzb5zpJUyXVSJK7vyLphABxAQAAgLyEuNGunbs//5l5/moDxAUAAEAEinH+5hCV4g/NrI+UmRHbzMZLWh4gLgAAAJCXEJXisyXNkLS7mb0n6V+SvhMgLgAAACIQstc3lBCzT7wt6Qgzay8p5e7roo4JAAAAFCLyQbGZ7SBpnKSekkrre1Dc/ZKoYwMAAKD5FWFLcZD2ib9K+ljSfDVY0Q4AAABIihCD4u7ufnSAOAAAAAggVYSl4hCzTzxtZnsFiAMAAABsl8gqxWa2UJlp2EolnWpmbyvTPmGS3N33jio2AAAAosPsE4UZGeF7AwAAAM0msvYJd3/H3d+RtIukjxpsfySpW1RxAQAAgEKF6Cm+RtInDbbXZ/cBAACgBTIL9wglxKDY3N3rN9y9TmFmvQAAAADyEmJQ/LaZnWNmZdnHuZLeDhAXAAAAEUjJgj3CfabofU/SIZLek7RM0oGSzgwQFwAAAMhLiDaGfu5+QsMdZnaopA8CxAYAAEAzK8K1O4JUin+X5z4AAAAgFlEu3nGwMm0TXc3sRw2e6iSpJKq4AAAAiBaLdxSmXFKHbIyODfavlTQ+wrgAAABAQSIbFLv7bEmzzexP2UU7AAAAUARSRdhUHGX7xG/c/TxJvzcz/+zz7j4qqtgAAABoHczsRkkjJa1094EN9v9A0mRJtZIecvfzG3ufKNsnbs3+nC3phc881ynCuAAAAIhQwgrFf5L0e0m31O8ws6GSvilpb3ffbGYVud4kstkn3H1+9o8TJX3k7rOzLRW7SvppVHEBAADQerj7HEkffWb3WZJ+7u6bs69Zmet9QkzJNl7SzWa2h5mdKen7ko4MEBcAAAARSJkFe2yn/pIGm9lzZjbbzA7I9QuRL97h7m+b2QmS7pP0rqQj3X1j1HEBAADQ8pnZJEmTGuya4e4zcvxaqaSdJB0k6QBJd5pZb3f/3H1uDX8hEma2UFLDwF2UmZ/4OTOTu+8dVWwAAABEJ2RPcXYAnGsQ/FnLJN2THQQ/b2Z1knZWIysqR1kpHhnhewMAAADbcp+kYZKeMLP+yqyf8WFjvxDlPMVNnpu4bTkL3zWmJl0XdwqJV1qSrNtj0fKsvmtS7he1YkN+OTvuFBLv/rMPiTuFRCtrG+L2JhQzM7td0hBJO5vZMkkXS7pR0o1mtkhStaSTG2udkAL0FAMAAKC4JOmvMu7+rW089Z1C3idJnwkAAACIBZViAAAAFMQStnpHc6BSDAAAgFaPSjEAAAAKUnx1YirFAAAAAJViAAAAFKYJyy8nFpViAAAAtHpUigEAAFCQ4qsTUykGAAAAqBQDAACgMEXYUkylGAAAAKBSDAAAgIKwoh0AAABQhKgUAwAAoCDFWFUtxs8EAAAAFIRBMQAAAFo92icAAABQEG60AwAAAIoQlWIAAAAUpPjqxFSKAQAAACrFAAAAKAw9xQAAAEARolIMAACAghRjVbUYPxMAAABQECrFAAAAKAg9xQAAAEARolIMAACAghRfnZhBcd6emjtHV/78MtWl6zRm3HE6/cxJcaeUKNMvmqYnZz+hnbp00Z33PhB3OonD8cmNY5Qb30OfV9FxB108cnd9oX2Z6ly67+XlunPee+pX0V4/Oaq/yktTSte5fvG3N/XP5eviTjdWmzdv1g/OPFnVNdVKp9Macvhwnf7dyXGnlThcZ60X7RN5SKfTuvyyS/SHa6/Xvfc/pEcfflCL33or7rQS5dhRo/W7a2bEnUZicXxy4xg1ju+hrUvXuX47c7FOuH6ezrj1JY3fb1f1/EI7TR7aWzc89Y5Oumm+ZsxdoslDe8edauzKy8v1m2tv1J9uv0c3/fkuPff0U/rHwpfjTitRuM7yZxbuEQqD4jwsWviKevTYTd179FBZebmOHnGMnpj1eNxpJcp+gw5Qp847xp1GYnF8cuMYNY7voa1btb5ar1d9IknaUJ3WklUbVNFxB7lL7ctLJEkddijRB+s2x5lmIpiZ2rVrJ0mqra1VbW1t2BFHC8B11roFaZ8ws50k9WgYz91fDBG7OaysqlK3Xbpt2a6orNTCV16JMSMArQ3fQ7nt0nkH9a/ooEXvr9VvHl+s30zYSz8Y1ltmpkm3vhR3eomQTqd1xokT9N67SzXmuG9pwMC9404pUbjO8pcqwq7iyAfFZvZfkk6RtFiSZ3e7pGFRx24uviXtfyvGqUgAJBffQ41rW5bSFWMG6DePL9aG6rTG7ruL/nvmYs16/UMdvntXTRvxZf3gfxnclJSU6KY/361169Zq2pRz9fZbb6p3335xp5UYXGetW4j2iQmS+rj7EHcfmn1sc0BsZpPMbJ6ZzbvhumT0F1ZWdtOK5Su2bK+sqlJFRUWMGQFobfge2raSlOmKMQP0f/9YqSfe+FCSNGJgN816PfPnx1/7QHvu0jHOFBOnY8dO2nf/A/TcM0/GnUqicJ21biEGxYsk5d0o6O4z3H2Quw9Kyh2fAwbupaVLl2jZsndVU12tRx9+SIcNbTGFbgBFgO+hbZs2or+WrNqg219YtmXfh59s1n5f6ixJGrTbjnp39ca40kuM1as/0rp1ayVJmzdt0rznn9WXevaKOatk4TrLXzHeaBeip/gKSS+Z2SJJW+50cPdRAWI3i9LSUk2ddpHOmnSG6urSGj1mnPryz02fcuH5P9b8ec9rzZo1GnHEEE36/mSNHjs+7rQSg+OTG8eocXwPbd0+3TtpxMBuemvlJ7rl1P0lSdfM/peuePQN/fCIvipJmapr63TFI2/EnGn8Vn34gS6/eJrSdWl5nWvo8KN06OAhcaeVKFxnrZu5f75/plkDmP1D0h8lLZRUV7/f3Wfn+t1NtVtp7sEWNem63C8C0CRlJUzS05ghv8z5Vd7q3X/2IXGnkGid2pbFnULitSlN3l1tDy1aGWyMdszAiiCfP0Sl+EN3/22AOAAAAMB2CTEonm9mV0i6X59un2gxU7IBAADg34pxUo4Qg+J9sz8ParCvRU3JBgAAgOIW+aDY3YdGHQMAAADhFOPiHZHfQWJmnc3s1/VzD5vZr8ysc9RxAQAAgHyFuK36RknrlFnEY4KktZJuChAXAAAAEWCe4u3Tx93HNdiebmYLAsQFAAAA8hKiUrzRzL5Wv2Fmh0piaSEAAIAWikrx9jlL0s3ZPmKT9JGkkwPEBQAAAPISYvaJBZL2MbNO2e21UccEAABAdIzZJwpXP/uEpJmSZjL7BAAAAJKG2ScAAABQkJSFe4TC7BMAAABo9Zh9AgAAAK1eiErx9yTd0qCPeLWYfQIAAKDFKsYb7SIdFJtZStKX3Z3ZJwAAAJBYkbZPuHudpMnZP69lQAwAANDyFePiHSF6ih8zsylm1sPMutQ/AsQFAAAA8hKip/i07M+zG+xzSb0DxAYAAEAzo6d4O7h7r8aeN7Ph7v5Y1HkAAAAA2xKiUpzLlZIYFAMAALQQIRfVCCVET3EuRXhYAQAA0JIkoVLscScAAACA/BVjT3ESKsUAAABArJJQKV4SdwIAAADIX8j5g0OJfFBsZmO3svtjSQvdfaW7b+15AAAAIJgQleLTJR0saVZ2e4ikZyX1N7NL3P3WADkAAACgmRRhoTjIoLhO0h7uXiVJZlYp6RpJB0qaI4lBMQAAAGIVYlDcs35AnLVSUn93/8jMagLEBwAAQDNKFWFTcYhB8Vwze1DSX7Lb4yTNMbP2ktY09osbq9NR59bitS0viTsFtHBrN/J308aUtWWSnsY8MeUwvqtzGHz5zLhTSLx5/zk87hSAIIPis5UZCB+qTAvKLZLudneXNDRA/KLFgBhNxYAYTcWAGE3FgLhlKr46cYBBcXbwe1f2AQAAACRO5P8uaGZjzexNM/vYzNaa2TozWxt1XAAAACBfIdonrpJ0rLu/GiAWAAAAolaE/RMh7iCpYkAMAACAJAtRKZ5nZndIuk/S5vqd7n5PgNgAAABoZlaEpeIQg+JOkjZIOrLBPpfEoBgAAACJEGL2iVOjjgEAAIBwinDtjugGxWZ2vrtfZWa/U6Yy/Cnufk5UsQEAAIBCRFkprr+5bl6EMQAAABBYERaKoxsUu/sD2T++4u4vRRUHAAAAaKoQU7L92sxeM7P/MrMBAeIBAAAgShbwEUjkg2J3HyppiKQPJM0ws4Vm9tOo4wIAAAD5ClEplruvcPffSvqepAWSLgoRFwAAAM3PAv4XSuSDYjPbw8z+08wWSfq9pKcldY86LgAAAJCvEIt33CTpdklHuvv7AeIBAAAgQsxTvB3c/aCoYwAAAABNEeXiHXe6+wQzW6hPL95hktzd944qNgAAAKJThIXiSCvF52Z/jowwBgAAANBkUS7esTz7852oYgAAAADNIcr2iXX6dNvElqeUaZ/oFFVsAAAARKgI+yeirBR3jOq9AQAAgOYUYko2AAAAFJGQi2qEEmRFOwAAACDJqBQDAACgIMW4eAeVYgAAALR6VIoBAABQkCIsFFMpBgAAAKgUAwAAoDBFWCpmUJyHqhXLNf1nU7Vq1YdKmWn0uAk6fuKJcaeVKE/NnaMrf36Z6tJ1GjPuOJ1+5qS4U0ocjlHjNm/erB+cebKqa6qVTqc15PDhOv27k+NOK1E4hxrHd/XnlZemdPMZg1ReklJJyvTYP6r0PzPf1hd3aqNfTNhbnduW6dXla3XBXYtUm97aelutD9dZy2RmP5R0hjILxy2UdKq7byrkPRgU56GkpFTn/Oh87b7Hnlq/fr1OmTheXz3wYPXq0zfu1BIhnU7r8ssu0R+vu0mVlZWaePx4DRk6TH36cnzqcYxyKy8v12+uvVHt2rVTbW2Nvn/6STrokMEasNc+caeWCJxDufFd/XnVtXU67cb52lidVmnKdMuZB2juG6t00qFf0q1Pv6NHFlbpolF7aNz+X9Qdzy+LO93YcZ3lL0nzFJvZFyWdI2lPd99oZndKOkHSnwp5H3qK87Bz167afY89JUnt27dXz169tfKDlTFnlRyLFr6iHj12U/cePVRWXq6jRxyjJ2Y9HndaicIxys3M1K5dO0lSbW2tamtri3POn+3EOZQb39Vbt7E6LUkqLTGVlphcrgN7d9Hf/pE5Nn996X0N26NrnCkmBtdZi1Yqqa2ZlUpqJ+n9Qt8gyKDYzNqa2ZdDxIra+++/pzdef1UDB+4ddyqJsbKqSt126bZlu6KyUlVVVTFmlDwco/yk02mdOnGcRg3/ug448GAN4DrbgnOoMHxX/1vKpLvOPkhzLjhMz7y1Su9+tFHrNtUqXZdpl6hau0kVndrEnGUycJ3lzyzkwyaZ2bwGj0/1tLj7e5J+KWmppOWSPnb3vxX6mSIfFJvZsZIWSHo0u/0VM7s/6rhR2LBhvaZOOVfnTZmq9h06xJ1OYrg+34dmVPg+hWOUn5KSEt3057t198OP69V/LNTbb70Zd0qJwTmUP76rP63OpfH/86wO/8Vc7dW9s3p3bf+517jTTyxxnSWVu89w90ENHjMaPm9mO0n6pqReknaV1N7MvlNonBCV4v+U9FVJayTJ3RdI6rmtFzf828CfbrwuQHr5qa2p0dQp5+mob4zU0MOHx51OolRWdtOK5Su2bK+sqlJFRUWMGSUPx6gwHTt20r77H6Dnnnky7lQSg3MoP3xXb9u6TbV64V+rtU+PzurYplQlqcxgr7JTG32wbnPM2SUD11n+LOAjD0dI+pe7f+DuNZLukXRIoZ8pxKC41t0/zvfFDf82cMppZ0aZV97cXZdN/5l69uqtiSeeEnc6iTNg4F5aunSJli17VzXV1Xr04Yd02NBhcaeVKByj3Fav/kjr1q2VJG3etEnznn9WX+rZK+askoNzKDe+qz9vp3Zl6tgmc0/9DqUpHdSni97+YL2e/9dqHTkgM9j75r67auarH8SZZmJwnbVYSyUdZGbtLFPaP1zSq4W+SYjZJxaZ2URJJWbWT5m7A58OELfZvLzgRT3y0P3q06+/Tjx+jCTprMnn6ZDBh8WcWTKUlpZq6rSLdNakM1RXl9boMePUt2+/uNNKFI5Rbqs+/ECXXzxN6bq0vM41dPhROnTwkLjTSgzOof/f3p3Hy1GVaRz/PVkg7ARJAiKroJHAsAUHIUBgFAGBCYsiA4MBnYw6MK6oEIQgijqMzkgYhLCEzQFBCRMQCWsSwLBlIQmbLGERNYosQmRN3vnjnJt0bu7tvp3cqu7b9/nm059Una6qc+q9VV2nT586VZs/q1c0aJ3V+d7hw+jbR0hiyvyFTHv8RZ760yLOPnJ7Tvzo1jz6h9e4buYLjS5qU/B51jNFxH2SfgHMAt4FZgMTqq+1IhXdj0jSmsBYYL+cNAX4blfGjnv5bx40sZo1Vuvb6CJYD/fXN95pdBGa3rpr9G90EZpa28gG1rk9z7qj0UVoag+OczeXWgb0a6Lxz7L5L7xeWh1tu03WLmX/C28pjoi/kSrFY4vOy8zMzMxsZZQx+sStktavmB8oaUrR+ZqZmZlZMVTiv7KUcaPdhhHxSttMRLwM+FZOMzMzM2saZdxot0TSZhHxHICkzaGDgQDNzMzMrEdoxeGby6gUjwXuljQtz+8FjKmyvJmZmZlZqcq40e5mSTsDu5HGYP5KRLxYdL5mZmZmVowWbCgurk+xpKH5/52BzYDfAy8Am+U0MzMzM7OmUGRL8VdJ3SR+1MF7AfgRMWZmZmY9UQs2FRdZKb41///ZiHi6wHzMzMzMzFZJkUOynZz//0WBeZiZmZlZyVpxnOIiW4pfknQnsJWkye3fjIhDCszbzMzMzKzLiqwUHwjsDFxBx/2KzczMzKwH8jjF9bk4Iv5Z0oURMa324mZmZmZmjVFkpXiX/PS6oyVdSLv7FCPipQLzNjMzM7OCtGBDcaGV4vOBm4GtgJkV6SINybZVgXmbmZmZmXVZYZXiiDgHOEfST0kV5L3yW9Mj4qGi8jUzMzMzq1eRQ7K1eQy4EtgQGARcIenEEvI1MzMzsyKoxFdJiuw+0eazwG4RsQhA0g+BGcD4EvI2MzMzM6upjEqxgMUV84tpzf7ZZmZmZr1CmQ/VKEsZleKJwH2SJuX5UcDFJeRrZmZmZtYlhVeKI+LHkqYCI0gtxMdFxOyi8yMW3IkAABYZSURBVDUzMzOzYvjhHSspImYBs8rIy8zMzMysXqVUis3MzMysdbRgQ3EpQ7KZmZmZmTU1txSbmZmZWX1asKnYLcVmZmZm1uu5pdjMzMzM6tKK4xS7pdjMzMzMej23FJuZmZlZXVpxnGJFRKPL0Kk336V5C2dmZryzeEmji9D0+vf1j7LVDNz1hEYXoem9MfvcpquCLnjxzdLqaFtuOKCU/XdLsZmZmZnVpelq6d3AX1/NzMzMrNdzpdjMzMzMej13nzAzMzOz+rRg/wm3FJuZmZlZr+eWYjMzMzOrix/eYWZmZmbWgtxSbGZmZmZ1acWHd7il2MzMzMx6PbcUm5mZmVldWrCh2C3FZmZmZmZuKTYzMzOzurhPsZmZmZlZC3JLsZmZmZnVqfWait1SbGZmZma9nluKzczMzKwu7lNsZmZmZtaC3FJsZmZmZnVpwYZitxSbmZmZmblSbGZmZma9nrtPmJmZmVldfKOdmZmZmVkLckuxmZmZmdVFLXirnVuKzczMzKzXc0uxmZmZmdWn9RqK3VJsZmZmZuaWYjMzMzOrSws2FLul2MzMzMzMLcVmZmZmVhePU9yL3XPXdA75xMc5aP+PcfGFExpdnKbj+NTmGFXn+NTmGFV3xmlj+djee/CpQw9udFGalo+hjvXpI2Zc9U1++ZPPAzDxe5/hoUnf5sFrT+H804+mXz9Xl3oD/5W7YPHixZz1ve9w3vkXMWnyr7j5pht56sknG12spuH41OYYVef41OYY1XbwIaMY/1NX9DrjY6hzJ/zTPjy+YOHS+at//QA7HHomwz95FmsM6M9xh+7ewNI1J5X4ryyuFHfB/Hlz2XTTzXnfppvSf7XV2P/ATzD1ztsbXaym4fjU5hhV5/jU5hjVtvPwXVl3vfUbXYym5WOoY5sMXp/9Rwxj4qTfLE2bcvcjS6cfnP8smwwe2IiiWckKrRRL6ivpyiLzKMOfFi5ko403Wjo/eMgQFi5cWGWN3sXxqc0xqs7xqc0xslXlY6hjZ590OGN/cj1LlsQK7/Xr14ejPvFhbv3NIx2s2cupxFdJCq0UR8RiYJCk1YrMp2jBiieKWrGH+UpyfGpzjKpzfGpzjGxV+Rha0QF7bsefXnqN2Y8+3+H7Pzn5SO6Z9ST3zH6q5JJZI5Qx+sQzwD2SJgOL2hIj4scdLSxpDDAG4NzzLuCz/zKmhCJWN2TIRvzxD39cOv+nhQsZPHhwA0vUXByf2hyj6hyf2hwjW1U+hlb0kR234qC9t2f/EcNYfbX+rLvWAC757rEcf+rlnDLmAAYNXJsjv3tRo4vZlFrx61QZfYp/D9yY81qn4tWhiJgQEcMjYngzVIgBhm23Pc899wy/+93zvPP229x806/Ye599G12spuH41OYYVef41OYY2aryMbSi08ZPZuv9v83QT5zOsd+ayNQHfsvxp17O6EM/wsd2/xDHnnwpESu2sFtrKrylOCLOAJC0TpqN14vOs7v169ePk8eexhfGfI4lSxYz6tDD2XrrbRpdrKbh+NTmGFXn+NTmGNV2yje+xswH7+eVV17hwI+OZMwXT2DUYUc0ulhNw8dQ140/5dM894eXmHrZ1wD4vzvm8P0JNze4VFY0Ff0NSNJ2wBXABjnpReDYiHi41rpvvttBBygzM2sa7yxe0ugiNL3+fT3QUzUDdz2h0UVoem/MPrfpeiv8ZdG7pdXR3rNWv1L2v4wzdQLw1YjYPCI2B74GXFhCvmZmZmZmXVLGjXZrRcSdbTMRMVXSWiXka2ZmZmYFKPOhGmUpo1L8tKRvk7pQABwDLCghXzMzMzOzLimj+8TxwCDgOmBSnj6uhHzNzMzMrABSea+ylDH6xMvAv0taF1jSE0efMDMzM7PWVnhLsaTtJc0G5gEPS5qZR6QwMzMzM2sKZXSfuIAVR5+YUEK+ZmZmZmZd4tEnzMzMzKwuZfb1LYtHnzAzMzOzXs+jT5iZmZlZXVTiv7KUNvpE0fmYmZmZma2swirFkm4AOn0udkQcUlTeZmZmZlYc9ymuz3/m/w8DNgKuzPNHAc8UmK+ZmZmZWV0KqxRHxDQASWdGxF4Vb90gaXpR+ZqZmZlZsVqwobiUG+0GSdqqbUbSlqSb7czMzMzMmkIZQ7J9BZgq6ek8vwXwryXka2ZmZmbWJWVUiu8EtgGG5vnHSsjTzMzMzIrSgv0nyug+MSMi3oqIh/LrLWBGCfmamZmZmXVJkUOybQRsAqwhaSeWfadYF1izqHzNzMzMrFhlPlSjLEV2n/g4MBp4H/DjivTXgFMKzNfMzMzMrC5FDsl2GXCZpMMj4pdF5WNmZmZm5fLDO1bOdpKGtU+MiO+UkLeZmZmZWU1lVIpfr5geABwEPFpCvmZmZmZWgBZsKC6+UhwRP6qcl/SfwOSi8zUzMzMz66oyWorbWxPYquZSZmZmZtacWrCpuPBKsaR5QOTZPsBg4Myi8zUzMzMz66oyWooPAgYCewLrAzdFxMwS8jUzMzOzAjTbOMWS9gd+AvQFLoqIH9S7jTKeaPePwBXAhkB/YKKkE0vI18zMzMxanKS+wP8ABwDbAkdJ2rbe7ZTRUvw5YLeIWAQg6YekxzyPLyFvMzMzM+tmTTZO8YeBJyPiaQBJV5MaZR+pZyNltBQLWFwxv5iW7J5tZmZmZg2wCfB8xfzvclpdymgpngjcJ2lSnh8FXNyVFQf0a67Ks6QxETGh0eVoZo5RdY5PbY5Rbc0UowH9ymhbqU8zxadZNVOM3ph9bqOLsIJmik+zKrOOJmkMMKYiaUK7v09HZYkO0qrnE1H3OnWTtDMwglTo6RExu/BMCyDpwYgY3uhyNDPHqDrHpzbHqDbHqDrHpzbHqDrHp2eR9BFgXER8PM+fDBAR369nO6WMUxwRs4BZZeRlZmZmZr3KA8A2krYEXgA+DfxTvRtpxMM7zMzMzMy6RUS8K+kEYAppSLZLIuLherfjSnF93L+oNseoOsenNseoNseoOsenNseoOsenh4mIm4CbVmUbpfQpNjMzMzNrZs1327CZmZmZWclarlIsaQtJ8+tYfrSk91bMPyNpw2JK11zqjVV3rWu9j6RRK/N0oVbT0Xkjabikc/L0SEm717sNs/Yk/bukRyX9TNLqkm6TNEfSkZIu8vm44vW/3XsjJd1YdpmssdynGEYD84Hfd3UFSf0i4t3CStSD9dTYSBoHvA6sSxo28LYqyx4CbLsyz1XvxUYBN9LB04V66jHTXSLiQeDBPDuSdBz+pmEFKoCk1yNi7VwBOScijqiyrM+v7vFF4ICIWCBpN6B/ROyY3/t5A8vVTEZT5/W/Xr39862nabmW4qyfpMskzZX0C0lrSjpN0gOS5kuaoOQIYDjws/wNeo28/omSZkmaJ2kopEpTXu8W4HJJAyRNzMvMlrRPXq6z9NGSrpd0g6QFkk6Q9NW8zL2SNmhEoOg4VrtImiZppqQpkjbO+7CLpIckzQD+rW0Ded+ulXQDcEuO7dk51vMkHZmX6yx9ZM7vGkm/lfQDSUdLuj8v9/6yghERp1WrEOdlJhd5wVZ6hnvTk3RM/hvNkXSBpL6SXpf0vXyc3CtpSG75PAQ4Oy/7fklTJZ0laRowNp8T/fN211X6xWaIpJk5bQdJIWmzPP9UPlYPlnRfPo9uy+v0kfSEpEF52T6SnlST/QIkaatc7pMk3ShpC+DzwFdynPbM+zMpx/MhLWtF7ivpQkkPS7ql7bMrx/bmfO7eVfH5damkcyT9RtLTSp99pYuI31erEOdlfH7VKV9L5ufXlyWdD2wFTJb0TeBKYMd259/wvO7+Ste7hyTdntPWknSJ0jVztqR/bNze1aeDWCz3y4qkrytdz1e4/udYPCbpbuCwinU2ULp+z82fa39XI325+kK5EbBVEhEt9QK2ID3FZI88fwnwdWCDimWuAA7O01OB4RXvPQOcmKe/CFyUp8cBM4E18vzXgIl5eijwHDCgSvpo4ElgHWAQ8Crw+bzcfwFfbpJYnURqpRqU044kDW0CMBfYO0+fDczP06NJj1TcIM8fDtxKGhZlSI7BxlXSRwKv5OnVSWMMnpG39SXgvwva/7HA48BtwFX5OLkUOKLiWDiDNMb2PGBoxf6em6cvBc7JMXu6Yt0+wHnAw6QW0pva3uukLM8ApwF3k8ZX3BG4N8d8EjAwL9dZ+tR8HE0HHgV2Ba4DngC+W0DsPgTcQGp9Iu/rsfl4aju3/gM4tSJOR1SsPxU4r2J+IjAqT48BfpSnHya13p9AGofyaGBzYEZ+fyDLbhj+XMV6p5PPKWA/4Jdln19Vzrn5wAeB2fnvORK4Mb8/Dvh6xfI/r9iPvsB6eRvvAjvm9GuAY/L07cA2efrvgTsq4n9tPi63BZ4seb9fr9z/PH0fMKzdMbELPr/qje0upM+ntYC1c0x2yvu8YV5m6TFWsT/DSdei54Etc3rbZ/hZFcfU+sBvgbUaff6sQizmVyzzddJDHpbGIU8PyLHYhvSgsWtYdl6OB07P0/sCc2qkj6OivuBXz3m1akvx8xFxT56+kvQ0vX1yi9I80sE7rMr61+X/Z5I+xNtMjog38vQIUuWaiHgMeBb4QJV0gDsj4rWI+DOpUnxDTp/XLp8ytY/Vx4HtgFslzQFOBd4naT1g/YiYlpe9ot12bo2Il/L0COCqiFgcEQuBaaSLSGfpAA9ExB8i4i3gKeCWnF5IbCTtQro47kRqEdi1k0VfjIidgZ+SPkw7sjFp3w4C2lq4DiOVe3tSZe0jXSjWmxExIiKuJrUufDMi/o4Ug9PzMp2lA7wdEXsB5wP/R2rN3w4YLek9Xci/Hv9AugA9kI+TfyC1TL1NqqTAiudPe5U/4V4EHJenjyNVkiFVhvYA9iJdqPcC9gTuyu+/D5iSz+uTWHZeX0KqpAMcX7G9ZjCI9Pc5JiLm1Fh2X9KxRz5vXs3pCyrWnQlsIWltYHfg2vw3uYB0bLa5PiKWRMQjpC+ljXY18CkApV+j3hsRMztYrjeeX/UYAUyKiEUR8Trp+rVnF9fdjdRdbAFAxWf4fsC38nE0lVRh3KxbS12MVYnFUNJ59UREBOl6WLndtuv6HcB78jWxs3RYvr5gPUSrVorbjzMXpFaFIyJie+BC0knembfy/4tZvt/1oorpzp75Xe1Z4G9VTC+pmF9C4/p3t4/Va8DDEbFjfm0fEfuR9qva+H09LTZ7kj48/xYRfwUmd7JcZ1+QKnVU2RgBXJvT/wjc2YUy/Ryggy8glwF7dZZesX7bPswj/Q3bvmQ8DWzahfzrIeCyiuPkgxExDngnX1BgxfOnvaXHTP5itoWkvYG+EdH2c+ddpL/V5qSKyA6k2E7P748ntSpuD/wr+byOiOeBhZL2JbWY/npVd7gbvUpqkdpjFbZReb60xbkP8ErF32THiPhQJ+tUOxfLcg3wyTz9KVJLdkd64/lVj1X5W3b2uS7g8IrjaLOIeHQV8ilLR7FYn+XrOtWu/Z1d4zrablRJh+WvidZDtGqleDOl52ADHEX6yQzgxdyaUtmn7TVSl4Z6TSf9lIukD5C+RT9eJb1ZtY/VvcCgtjRJ/SUNi4hXgFcljcjLHl1lm9OBI5X6mA4iXVjur5LeKF0ZpLuzL0gdLQPLPiRX5kK1qh+ilV8k2n/J6O4vFrcDR0gaDEv71m1eZfmunGeXk7qxVLbqTgeOAZ6IiCXAS8CBQNuvG+uRutsAfKbd9i4itfZcExGLa+RdprdJNx4eK6n9Y0jbx+l24AuQ+sJKWrezjeYvdwskfTIvL0k7dGvJu1FEvAD8JffDPJLUctyR3nh+1WM6MEqpj/1awKEs+yWllhnA3kqPxkXL7m2ZQrq3Rjl9p24uc1E6isWvgcGS3iNpddIvDm0qz7fHgC217B6Wo9ptt+26PpL0C+Jfq6RbD9WqleJHgc9ImgtsQPr58ULSN/zrSX0T21wKnK/lb7TrivNIN7vMI7VAjM6tBp2lN6v2sRpP+tLwQ0kPAXNIP8lC+ln7f5RutKv2s9AkUp+8h4A7gG/k1pzO0hthOnBovrliHeDgbt7+3cDhSjd5DSH16euS/BP5y5Lafvb7Z2BaZ+ndWOYuy612p5JurJxL6iu+cZVVrgZOyjftdHbj5M9IfYSvqsjnmTzZ1jJ8N6k19OU8P47UXeAu4MV225tM6lfYTF0nAIiIRaSL81dIFfs2N5COyzn57/wlUteveaRfK6p1+4J0gf5sPncfBpr9BqmrgW8A60XEvDrWa+nzqx4RMYt0Hbuf1E/7ooiY3cV1/0zqw39dPmbaujSdCfQH5uab1M7s7nIXoZNYPAB8J8/fSKr8trmUfP0nfdEaA/wq32j3bMVy44Dh+bPuByz7At5ZuvVUK9sZ2S+/evqLZTfa3cKyGzIvZfkb7dpuVBkOTM3To1n+RqDKG8jabijqQ+p7+Ajpi9ivgY9VKcvSvPJ85Q0/19PxjUCV6VNZdsPISDq4qabR8e7C3+MI4Ipu3N5w4K5G75dfHd9ol+eHkG4aPL0izeeXX3751ZCXH/NsVhBJa0fE6/kmnPtJo3w0qmW8qUkaDxwAHBgRv+2G7X2L1O3g6Ii4u9by1vP4/DKz7uZKsVlBJE0l3eSxGvAfEXFpQwtk1kJ8fplZd3Ol2KxEkiYBW7ZL/mZETGlEecxaic8vM1sVrhSbmZmZWa/XqqNPmJmZmZl1mSvFZmZmZtbruVJsZi1B0uI8vu98SddKWnMVtjVS0o15+pA8mkVny64v6Ysrkcc4SZ09OtzMzErmSrGZtYo3Ij2SdjvSU+M+X/lmfsJb3Z95ETE5In5QZZH1gborxWZm1lxcKTazVnQXsLWkLSQ9Kuk8YBawqaT9JM2QNCu3KK8NIGl/SY/lp1kd1rYhSaMlnZunh0iaJOmh/Nqd9CSr9+dW6rPzcidJekDSXElnVGxrrKTHJd0GfLC0aJiZWU2uFJtZS5HUj/QgkLbHBn8QuDwidgIWkR5P/dGI2Bl4EPiqpAGkR8EfDOwJbNTJ5s8hPRJ4B2Bn0qOUvwU8lVupT5K0H7AN8GHSE9J2kbSXpF2ATwM7kSrdu3bzrpuZ2Sro1+gCmJl1kzUkzcnTdwEXA+8Fno2Ie3P6bsC2wD2SID34YQYwFFgQEU8ASLoSGNNBHvsCxwJExGLgVUkD2y2zX37NzvNrkyrJ6wCTIuJvOY/Jq7S3ZmbWrVwpNrNW8UZE7FiZkCu+iyqTgFsj4qh2y+0IdNeg7QK+HxEXtMvjy92Yh5mZdTN3nzCz3uReYA9JWwNIWlPSB4DHgC0lvT8vd1Qn698OfCGv21fSusBrpFbgNlOA4yv6Km8iaTAwHThU0hqS1iF11TAzsybhSrGZ9RoR8WdgNHCVpLmkSvLQiHiT1F3iV/lGu2c72cSXgH0kzQNmAsMi4i+k7hjzJZ0dEbcA/wvMyMv9AlgnImYBPwfmAL8kdfEwM7Mm4cc8m5mZmVmv55ZiMzMzM+v1XCk2MzMzs17PlWIzMzMz6/VcKTYzMzOzXs+VYjMzMzPr9VwpNjMzM7Nez5ViMzMzM+v1XCk2MzMzs17v/wH2sR48UBuIQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 936x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cmarray = confusion_matrix(trueValues, predValues, labels=classLabels)\n",
    "\n",
    "df_cm = pd.DataFrame(cmarray, index = classLabels,\n",
    "                  columns = classLabels)\n",
    "\n",
    "plt.figure(figsize = (13,10))\n",
    "ax = sn.heatmap(df_cm, annot=True, cmap=\"Blues\")\n",
    "ax.set(xlabel='Predicted', ylabel='Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33,  0,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0, 37,  0,  4,  0,  1,  0,  0],\n",
       "       [ 0,  1, 39,  0,  1,  2,  1,  1],\n",
       "       [ 2,  0,  2, 15,  0,  3,  3,  0],\n",
       "       [ 2,  0,  2,  1, 37,  0,  1,  0],\n",
       "       [ 0,  1,  1,  1,  0, 28,  3,  0],\n",
       "       [ 2,  0,  0,  3,  0,  2, 30,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0, 42]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "----------\n",
      "train loss: 0.6326, acc: 0.7809\n",
      "valid loss: 0.9737, acc: 0.7195\n",
      "steps: 0 lr: 0.0200\n",
      "1\n",
      "steps: 782 lr: 0.0200\n",
      "Epoch 2/120\n",
      "----------\n",
      "train loss: 0.7287, acc: 0.7566\n",
      "valid loss: 0.8029, acc: 0.7360\n",
      "steps: 391 lr: 0.0100\n",
      "steps: 391 lr: 0.0100\n",
      "Epoch 3/120\n",
      "----------\n",
      "train loss: 0.4648, acc: 0.8487\n",
      "valid loss: 0.7042, acc: 0.7492\n",
      "steps: 0 lr: 0.0000\n",
      "2\n",
      "steps: 1564 lr: 0.0200\n",
      "Epoch 4/120\n",
      "----------\n",
      "train loss: 0.6912, acc: 0.7701\n",
      "valid loss: 0.9500, acc: 0.6997\n",
      "steps: 1173 lr: 0.0171\n",
      "steps: 1173 lr: 0.0171\n",
      "Epoch 5/120\n",
      "----------\n",
      "train loss: 0.5648, acc: 0.8174\n",
      "valid loss: 0.8046, acc: 0.7459\n",
      "steps: 782 lr: 0.0100\n",
      "steps: 782 lr: 0.0100\n",
      "Epoch 6/120\n",
      "----------\n",
      "train loss: 0.3745, acc: 0.8750\n",
      "valid loss: 0.6919, acc: 0.7690\n",
      "steps: 391 lr: 0.0029\n",
      "steps: 391 lr: 0.0029\n",
      "Epoch 7/120\n",
      "----------\n",
      "train loss: 0.2481, acc: 0.9239\n",
      "valid loss: 0.6762, acc: 0.7756\n",
      "steps: 0 lr: 0.0000\n",
      "3\n",
      "steps: 2346 lr: 0.0200\n",
      "Epoch 8/120\n",
      "----------\n",
      "train loss: 0.6044, acc: 0.7934\n",
      "valid loss: 0.8551, acc: 0.7096\n",
      "steps: 1955 lr: 0.0187\n",
      "steps: 1955 lr: 0.0187\n",
      "Epoch 9/120\n",
      "----------\n",
      "train loss: 0.5533, acc: 0.8094\n",
      "valid loss: 0.9137, acc: 0.6964\n",
      "steps: 1564 lr: 0.0150\n",
      "steps: 1564 lr: 0.0150\n",
      "Epoch 10/120\n",
      "----------\n",
      "train loss: 0.4266, acc: 0.8644\n",
      "valid loss: 0.7888, acc: 0.7228\n",
      "steps: 1173 lr: 0.0100\n",
      "steps: 1173 lr: 0.0100\n",
      "Epoch 11/120\n",
      "----------\n",
      "train loss: 0.2804, acc: 0.9143\n",
      "valid loss: 0.7068, acc: 0.7624\n",
      "steps: 782 lr: 0.0050\n",
      "steps: 782 lr: 0.0050\n",
      "Epoch 12/120\n",
      "----------\n",
      "train loss: 0.1880, acc: 0.9440\n",
      "valid loss: 0.6726, acc: 0.7789\n",
      "steps: 391 lr: 0.0013\n",
      "steps: 391 lr: 0.0013\n",
      "Epoch 13/120\n",
      "----------\n",
      "train loss: 0.1496, acc: 0.9632\n",
      "valid loss: 0.6575, acc: 0.7690\n",
      "steps: 0 lr: 0.0000\n",
      "4\n",
      "steps: 3128 lr: 0.0200\n",
      "Epoch 14/120\n",
      "----------\n",
      "train loss: 0.5125, acc: 0.8254\n",
      "valid loss: 0.9880, acc: 0.6898\n",
      "steps: 2737 lr: 0.0192\n",
      "steps: 2737 lr: 0.0192\n",
      "Epoch 15/120\n",
      "----------\n",
      "train loss: 0.4961, acc: 0.8379\n",
      "valid loss: 0.8797, acc: 0.7162\n",
      "steps: 2346 lr: 0.0171\n",
      "steps: 2346 lr: 0.0171\n",
      "Epoch 16/120\n",
      "----------\n",
      "train loss: 0.3597, acc: 0.8884\n",
      "valid loss: 0.8032, acc: 0.7558\n",
      "steps: 1955 lr: 0.0138\n",
      "steps: 1955 lr: 0.0138\n",
      "Epoch 17/120\n",
      "----------\n",
      "train loss: 0.2654, acc: 0.9117\n",
      "valid loss: 0.6730, acc: 0.7789\n",
      "steps: 1564 lr: 0.0100\n",
      "steps: 1564 lr: 0.0100\n",
      "Epoch 18/120\n",
      "----------\n",
      "train loss: 0.2039, acc: 0.9428\n",
      "valid loss: 0.7012, acc: 0.7723\n",
      "steps: 1173 lr: 0.0062\n",
      "steps: 1173 lr: 0.0062\n",
      "Epoch 19/120\n",
      "----------\n",
      "train loss: 0.1260, acc: 0.9683\n",
      "valid loss: 0.6658, acc: 0.7855\n",
      "steps: 782 lr: 0.0029\n",
      "steps: 782 lr: 0.0029\n",
      "Epoch 20/120\n",
      "----------\n",
      "train loss: 0.1082, acc: 0.9709\n",
      "valid loss: 0.6799, acc: 0.7888\n",
      "steps: 391 lr: 0.0008\n",
      "steps: 391 lr: 0.0008\n",
      "Epoch 21/120\n",
      "----------\n",
      "train loss: 0.0869, acc: 0.9808\n",
      "valid loss: 0.6556, acc: 0.8020\n",
      "steps: 0 lr: 0.0000\n",
      "5\n",
      "steps: 3910 lr: 0.0200\n",
      "Epoch 22/120\n",
      "----------\n",
      "train loss: 0.3822, acc: 0.8785\n",
      "valid loss: 1.1590, acc: 0.6799\n",
      "steps: 3519 lr: 0.0195\n",
      "steps: 3519 lr: 0.0195\n",
      "Epoch 23/120\n",
      "----------\n",
      "train loss: 0.3720, acc: 0.8746\n",
      "valid loss: 1.0681, acc: 0.6997\n",
      "steps: 3128 lr: 0.0181\n",
      "steps: 3128 lr: 0.0181\n",
      "Epoch 24/120\n",
      "----------\n",
      "train loss: 0.3283, acc: 0.8973\n",
      "valid loss: 0.8513, acc: 0.7393\n",
      "steps: 2737 lr: 0.0159\n",
      "steps: 2737 lr: 0.0159\n",
      "Epoch 25/120\n",
      "----------\n",
      "train loss: 0.2319, acc: 0.9261\n",
      "valid loss: 0.8334, acc: 0.7393\n",
      "steps: 2346 lr: 0.0131\n",
      "steps: 2346 lr: 0.0131\n",
      "Epoch 26/120\n",
      "----------\n",
      "train loss: 0.1722, acc: 0.9488\n",
      "valid loss: 0.6952, acc: 0.7855\n",
      "steps: 1955 lr: 0.0100\n",
      "steps: 1955 lr: 0.0100\n",
      "Epoch 27/120\n",
      "----------\n",
      "train loss: 0.1291, acc: 0.9626\n",
      "valid loss: 0.6919, acc: 0.7822\n",
      "steps: 1564 lr: 0.0069\n",
      "steps: 1564 lr: 0.0069\n",
      "Epoch 28/120\n",
      "----------\n",
      "train loss: 0.0825, acc: 0.9792\n",
      "valid loss: 0.6720, acc: 0.8185\n",
      "steps: 1173 lr: 0.0041\n",
      "steps: 1173 lr: 0.0041\n",
      "Epoch 29/120\n",
      "----------\n",
      "train loss: 0.0674, acc: 0.9846\n",
      "valid loss: 0.6571, acc: 0.8053\n",
      "steps: 782 lr: 0.0019\n",
      "steps: 782 lr: 0.0019\n",
      "Epoch 30/120\n",
      "----------\n",
      "train loss: 0.0522, acc: 0.9914\n",
      "valid loss: 0.7135, acc: 0.8152\n",
      "steps: 391 lr: 0.0005\n",
      "steps: 391 lr: 0.0005\n",
      "Epoch 31/120\n",
      "----------\n",
      "train loss: 0.0544, acc: 0.9888\n",
      "valid loss: 0.6445, acc: 0.8152\n",
      "steps: 0 lr: 0.0000\n",
      "6\n",
      "steps: 4692 lr: 0.0200\n",
      "Epoch 32/120\n",
      "----------\n",
      "train loss: 0.2020, acc: 0.9309\n",
      "valid loss: 1.0361, acc: 0.7063\n",
      "steps: 4301 lr: 0.0197\n",
      "steps: 4301 lr: 0.0197\n",
      "Epoch 33/120\n",
      "----------\n",
      "train loss: 0.3556, acc: 0.8794\n",
      "valid loss: 1.1252, acc: 0.6898\n",
      "steps: 3910 lr: 0.0187\n",
      "steps: 3910 lr: 0.0187\n",
      "Epoch 34/120\n",
      "----------\n",
      "train loss: 0.2766, acc: 0.9105\n",
      "valid loss: 0.9466, acc: 0.7360\n",
      "steps: 3519 lr: 0.0171\n",
      "steps: 3519 lr: 0.0171\n",
      "Epoch 35/120\n",
      "----------\n",
      "train loss: 0.1928, acc: 0.9431\n",
      "valid loss: 0.8465, acc: 0.7492\n",
      "steps: 3128 lr: 0.0150\n",
      "steps: 3128 lr: 0.0150\n",
      "Epoch 36/120\n",
      "----------\n",
      "train loss: 0.1417, acc: 0.9594\n",
      "valid loss: 0.7677, acc: 0.7888\n",
      "steps: 2737 lr: 0.0126\n",
      "steps: 2737 lr: 0.0126\n",
      "Epoch 37/120\n",
      "----------\n",
      "train loss: 0.1256, acc: 0.9623\n",
      "valid loss: 0.7560, acc: 0.7888\n",
      "steps: 2346 lr: 0.0100\n",
      "steps: 2346 lr: 0.0100\n",
      "Epoch 38/120\n",
      "----------\n",
      "train loss: 0.0807, acc: 0.9815\n",
      "valid loss: 0.7334, acc: 0.7789\n",
      "steps: 1955 lr: 0.0074\n",
      "steps: 1955 lr: 0.0074\n",
      "Epoch 39/120\n",
      "----------\n",
      "train loss: 0.0544, acc: 0.9885\n",
      "valid loss: 0.7192, acc: 0.8086\n",
      "steps: 1564 lr: 0.0050\n",
      "steps: 1564 lr: 0.0050\n",
      "Epoch 40/120\n",
      "----------\n",
      "train loss: 0.0496, acc: 0.9904\n",
      "valid loss: 0.6823, acc: 0.7954\n",
      "steps: 1173 lr: 0.0029\n",
      "steps: 1173 lr: 0.0029\n",
      "Epoch 41/120\n",
      "----------\n",
      "train loss: 0.0351, acc: 0.9936\n",
      "valid loss: 0.6920, acc: 0.8020\n",
      "steps: 782 lr: 0.0013\n",
      "steps: 782 lr: 0.0013\n",
      "Epoch 42/120\n",
      "----------\n",
      "train loss: 0.0404, acc: 0.9920\n",
      "valid loss: 0.6853, acc: 0.7921\n",
      "steps: 391 lr: 0.0003\n",
      "steps: 391 lr: 0.0003\n",
      "Epoch 43/120\n",
      "----------\n",
      "train loss: 0.0365, acc: 0.9930\n",
      "valid loss: 0.7034, acc: 0.7888\n",
      "steps: 0 lr: 0.0000\n",
      "7\n",
      "steps: 5474 lr: 0.0200\n",
      "Epoch 44/120\n",
      "----------\n",
      "train loss: 0.1395, acc: 0.9600\n",
      "valid loss: 0.8810, acc: 0.7624\n",
      "steps: 5083 lr: 0.0198\n",
      "steps: 5083 lr: 0.0198\n",
      "Epoch 45/120\n",
      "----------\n",
      "train loss: 0.2061, acc: 0.9367\n",
      "valid loss: 1.1640, acc: 0.6997\n",
      "steps: 4692 lr: 0.0190\n",
      "steps: 4692 lr: 0.0190\n",
      "Epoch 46/120\n",
      "----------\n",
      "train loss: 0.1802, acc: 0.9424\n",
      "valid loss: 0.8368, acc: 0.7591\n",
      "steps: 4301 lr: 0.0178\n",
      "steps: 4301 lr: 0.0178\n",
      "Epoch 47/120\n",
      "----------\n",
      "train loss: 0.1534, acc: 0.9549\n",
      "valid loss: 0.8799, acc: 0.7525\n",
      "steps: 3910 lr: 0.0162\n",
      "steps: 3910 lr: 0.0162\n",
      "Epoch 48/120\n",
      "----------\n",
      "train loss: 0.1222, acc: 0.9642\n",
      "valid loss: 0.8249, acc: 0.7756\n",
      "steps: 3519 lr: 0.0143\n",
      "steps: 3519 lr: 0.0143\n",
      "Epoch 49/120\n",
      "----------\n",
      "train loss: 0.0656, acc: 0.9834\n",
      "valid loss: 0.8457, acc: 0.7525\n",
      "steps: 3128 lr: 0.0122\n",
      "steps: 3128 lr: 0.0122\n",
      "Epoch 50/120\n",
      "----------\n",
      "train loss: 0.0744, acc: 0.9802\n",
      "valid loss: 0.9004, acc: 0.7459\n",
      "steps: 2737 lr: 0.0100\n",
      "steps: 2737 lr: 0.0100\n",
      "Epoch 51/120\n",
      "----------\n",
      "train loss: 0.0461, acc: 0.9885\n",
      "valid loss: 0.8468, acc: 0.7591\n",
      "steps: 2346 lr: 0.0078\n",
      "steps: 2346 lr: 0.0078\n",
      "Epoch 52/120\n",
      "----------\n",
      "train loss: 0.0319, acc: 0.9942\n",
      "valid loss: 0.8123, acc: 0.7690\n",
      "steps: 1955 lr: 0.0057\n",
      "steps: 1955 lr: 0.0057\n",
      "Epoch 53/120\n",
      "----------\n",
      "train loss: 0.0283, acc: 0.9946\n",
      "valid loss: 0.8287, acc: 0.7723\n",
      "steps: 1564 lr: 0.0038\n",
      "steps: 1564 lr: 0.0038\n",
      "Epoch 54/120\n",
      "----------\n",
      "train loss: 0.0241, acc: 0.9952\n",
      "valid loss: 0.8064, acc: 0.7723\n",
      "steps: 1173 lr: 0.0022\n",
      "steps: 1173 lr: 0.0022\n",
      "Epoch 55/120\n",
      "----------\n",
      "train loss: 0.0230, acc: 0.9965\n",
      "valid loss: 0.8379, acc: 0.7624\n",
      "steps: 782 lr: 0.0010\n",
      "steps: 782 lr: 0.0010\n",
      "Epoch 56/120\n",
      "----------\n",
      "train loss: 0.0207, acc: 0.9971\n",
      "valid loss: 0.8291, acc: 0.7558\n",
      "steps: 391 lr: 0.0003\n",
      "steps: 391 lr: 0.0003\n",
      "Epoch 57/120\n",
      "----------\n",
      "train loss: 0.0220, acc: 0.9962\n",
      "valid loss: 0.8378, acc: 0.7822\n",
      "steps: 0 lr: 0.0000\n",
      "8\n",
      "steps: 6256 lr: 0.0200\n",
      "Epoch 58/120\n",
      "----------\n",
      "train loss: 0.0701, acc: 0.9815\n",
      "valid loss: 1.1681, acc: 0.6964\n",
      "steps: 5865 lr: 0.0198\n",
      "steps: 5865 lr: 0.0198\n",
      "Epoch 59/120\n",
      "----------\n",
      "train loss: 0.1341, acc: 0.9565\n",
      "valid loss: 0.9788, acc: 0.7360\n",
      "steps: 5474 lr: 0.0192\n",
      "steps: 5474 lr: 0.0192\n",
      "Epoch 60/120\n",
      "----------\n",
      "train loss: 0.1626, acc: 0.9469\n",
      "valid loss: 0.8721, acc: 0.7459\n",
      "steps: 5083 lr: 0.0183\n",
      "steps: 5083 lr: 0.0183\n",
      "Epoch 61/120\n",
      "----------\n",
      "train loss: 0.1155, acc: 0.9664\n",
      "valid loss: 0.9836, acc: 0.7360\n",
      "steps: 4692 lr: 0.0171\n",
      "steps: 4692 lr: 0.0171\n",
      "Epoch 62/120\n",
      "----------\n",
      "train loss: 0.1169, acc: 0.9645\n",
      "valid loss: 0.8570, acc: 0.7822\n",
      "steps: 4301 lr: 0.0156\n",
      "steps: 4301 lr: 0.0156\n",
      "Epoch 63/120\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0642, acc: 0.9843\n",
      "valid loss: 0.7184, acc: 0.7888\n",
      "steps: 3910 lr: 0.0138\n",
      "steps: 3910 lr: 0.0138\n",
      "Epoch 64/120\n",
      "----------\n",
      "train loss: 0.0555, acc: 0.9885\n",
      "valid loss: 0.8223, acc: 0.7723\n",
      "steps: 3519 lr: 0.0120\n",
      "steps: 3519 lr: 0.0120\n",
      "Epoch 65/120\n",
      "----------\n",
      "train loss: 0.0392, acc: 0.9914\n",
      "valid loss: 0.8600, acc: 0.7789\n",
      "steps: 3128 lr: 0.0100\n",
      "steps: 3128 lr: 0.0100\n",
      "Epoch 66/120\n",
      "----------\n",
      "train loss: 0.0298, acc: 0.9939\n",
      "valid loss: 0.8017, acc: 0.7789\n",
      "steps: 2737 lr: 0.0081\n",
      "steps: 2737 lr: 0.0081\n",
      "Epoch 67/120\n",
      "----------\n",
      "train loss: 0.0266, acc: 0.9939\n",
      "valid loss: 0.7652, acc: 0.7921\n",
      "steps: 2346 lr: 0.0062\n",
      "steps: 2346 lr: 0.0062\n",
      "Epoch 68/120\n",
      "----------\n",
      "train loss: 0.0218, acc: 0.9952\n",
      "valid loss: 0.7526, acc: 0.8152\n",
      "steps: 1955 lr: 0.0044\n",
      "steps: 1955 lr: 0.0044\n",
      "Epoch 69/120\n",
      "----------\n",
      "train loss: 0.0175, acc: 0.9968\n",
      "valid loss: 0.7056, acc: 0.8020\n",
      "steps: 1564 lr: 0.0029\n",
      "steps: 1564 lr: 0.0029\n",
      "Epoch 70/120\n",
      "----------\n",
      "train loss: 0.0196, acc: 0.9949\n",
      "valid loss: 0.7031, acc: 0.8218\n",
      "steps: 1173 lr: 0.0017\n",
      "steps: 1173 lr: 0.0017\n",
      "Epoch 71/120\n",
      "----------\n",
      "train loss: 0.0136, acc: 0.9981\n",
      "valid loss: 0.7309, acc: 0.8086\n",
      "steps: 782 lr: 0.0008\n",
      "steps: 782 lr: 0.0008\n",
      "Epoch 72/120\n",
      "----------\n",
      "train loss: 0.0153, acc: 0.9981\n",
      "valid loss: 0.7140, acc: 0.8119\n",
      "steps: 391 lr: 0.0002\n",
      "steps: 391 lr: 0.0002\n",
      "Epoch 73/120\n",
      "----------\n",
      "train loss: 0.0142, acc: 0.9984\n",
      "valid loss: 0.7026, acc: 0.8086\n",
      "steps: 0 lr: 0.0000\n",
      "9\n",
      "steps: 7038 lr: 0.0200\n",
      "Epoch 74/120\n",
      "----------\n",
      "train loss: 0.0213, acc: 0.9962\n",
      "valid loss: 0.7544, acc: 0.8020\n",
      "steps: 6647 lr: 0.0198\n",
      "steps: 6647 lr: 0.0198\n",
      "Epoch 75/120\n",
      "----------\n",
      "train loss: 0.0724, acc: 0.9779\n",
      "valid loss: 1.0044, acc: 0.7360\n",
      "steps: 6256 lr: 0.0194\n",
      "steps: 6256 lr: 0.0194\n",
      "Epoch 76/120\n",
      "----------\n",
      "train loss: 0.0987, acc: 0.9712\n",
      "valid loss: 0.9687, acc: 0.7558\n",
      "steps: 5865 lr: 0.0187\n",
      "steps: 5865 lr: 0.0187\n",
      "Epoch 77/120\n",
      "----------\n",
      "train loss: 0.0822, acc: 0.9747\n",
      "valid loss: 0.9721, acc: 0.7393\n",
      "steps: 5474 lr: 0.0177\n",
      "steps: 5474 lr: 0.0177\n",
      "Epoch 78/120\n",
      "----------\n",
      "train loss: 0.0847, acc: 0.9715\n",
      "valid loss: 0.9364, acc: 0.7525\n",
      "steps: 5083 lr: 0.0164\n",
      "steps: 5083 lr: 0.0164\n",
      "Epoch 79/120\n",
      "----------\n",
      "train loss: 0.0551, acc: 0.9859\n",
      "valid loss: 0.8514, acc: 0.7591\n",
      "steps: 4692 lr: 0.0150\n",
      "steps: 4692 lr: 0.0150\n",
      "Epoch 80/120\n",
      "----------\n",
      "train loss: 0.0452, acc: 0.9878\n",
      "valid loss: 0.8621, acc: 0.7525\n",
      "steps: 4301 lr: 0.0134\n",
      "steps: 4301 lr: 0.0134\n",
      "Epoch 81/120\n",
      "----------\n",
      "train loss: 0.0424, acc: 0.9875\n",
      "valid loss: 0.9183, acc: 0.7657\n",
      "steps: 3910 lr: 0.0117\n",
      "steps: 3910 lr: 0.0117\n",
      "Epoch 82/120\n",
      "----------\n",
      "train loss: 0.0308, acc: 0.9930\n",
      "valid loss: 0.8256, acc: 0.7987\n",
      "steps: 3519 lr: 0.0100\n",
      "steps: 3519 lr: 0.0100\n",
      "Epoch 83/120\n",
      "----------\n",
      "train loss: 0.0177, acc: 0.9974\n",
      "valid loss: 0.8055, acc: 0.8053\n",
      "steps: 3128 lr: 0.0083\n",
      "steps: 3128 lr: 0.0083\n",
      "Epoch 84/120\n",
      "----------\n",
      "train loss: 0.0162, acc: 0.9981\n",
      "valid loss: 0.7544, acc: 0.7954\n",
      "steps: 2737 lr: 0.0066\n",
      "steps: 2737 lr: 0.0066\n",
      "Epoch 85/120\n",
      "----------\n",
      "train loss: 0.0141, acc: 0.9974\n",
      "valid loss: 0.7310, acc: 0.8020\n",
      "steps: 2346 lr: 0.0050\n",
      "steps: 2346 lr: 0.0050\n",
      "Epoch 86/120\n",
      "----------\n",
      "train loss: 0.0127, acc: 0.9978\n",
      "valid loss: 0.7319, acc: 0.7921\n",
      "steps: 1955 lr: 0.0036\n",
      "steps: 1955 lr: 0.0036\n",
      "Epoch 87/120\n",
      "----------\n",
      "train loss: 0.0124, acc: 0.9978\n",
      "valid loss: 0.7331, acc: 0.8119\n",
      "steps: 1564 lr: 0.0023\n",
      "steps: 1564 lr: 0.0023\n",
      "Epoch 88/120\n",
      "----------\n",
      "train loss: 0.0107, acc: 0.9978\n",
      "valid loss: 0.7048, acc: 0.8119\n",
      "steps: 1173 lr: 0.0013\n",
      "steps: 1173 lr: 0.0013\n",
      "Epoch 89/120\n",
      "----------\n",
      "train loss: 0.0116, acc: 0.9984\n",
      "valid loss: 0.7406, acc: 0.8020\n",
      "steps: 782 lr: 0.0006\n",
      "steps: 782 lr: 0.0006\n",
      "Epoch 90/120\n",
      "----------\n",
      "train loss: 0.0131, acc: 0.9971\n",
      "valid loss: 0.7336, acc: 0.8152\n",
      "steps: 391 lr: 0.0002\n",
      "steps: 391 lr: 0.0002\n",
      "Epoch 91/120\n",
      "----------\n",
      "train loss: 0.0101, acc: 0.9990\n",
      "valid loss: 0.7131, acc: 0.8185\n",
      "steps: 0 lr: 0.0000\n",
      "10\n",
      "steps: 7820 lr: 0.0200\n",
      "Epoch 92/120\n",
      "----------\n",
      "train loss: 0.0122, acc: 0.9974\n",
      "valid loss: 0.8673, acc: 0.7822\n",
      "steps: 7429 lr: 0.0199\n",
      "steps: 7429 lr: 0.0199\n",
      "Epoch 93/120\n",
      "----------\n",
      "train loss: 0.0358, acc: 0.9901\n",
      "valid loss: 0.8577, acc: 0.7756\n",
      "steps: 7038 lr: 0.0195\n",
      "steps: 7038 lr: 0.0195\n",
      "Epoch 94/120\n",
      "----------\n",
      "train loss: 0.0700, acc: 0.9802\n",
      "valid loss: 0.8037, acc: 0.7789\n",
      "steps: 6647 lr: 0.0189\n",
      "steps: 6647 lr: 0.0189\n",
      "Epoch 95/120\n",
      "----------\n",
      "train loss: 0.0783, acc: 0.9757\n",
      "valid loss: 0.9417, acc: 0.7822\n",
      "steps: 6256 lr: 0.0181\n",
      "steps: 6256 lr: 0.0181\n",
      "Epoch 96/120\n",
      "----------\n",
      "train loss: 0.0828, acc: 0.9757\n",
      "valid loss: 0.9195, acc: 0.7855\n",
      "steps: 5865 lr: 0.0171\n",
      "steps: 5865 lr: 0.0171\n",
      "Epoch 97/120\n",
      "----------\n",
      "train loss: 0.0648, acc: 0.9815\n",
      "valid loss: 0.9023, acc: 0.7657\n",
      "steps: 5474 lr: 0.0159\n",
      "steps: 5474 lr: 0.0159\n",
      "Epoch 98/120\n",
      "----------\n",
      "train loss: 0.0390, acc: 0.9888\n",
      "valid loss: 0.7941, acc: 0.7855\n",
      "steps: 5083 lr: 0.0145\n",
      "steps: 5083 lr: 0.0145\n",
      "Epoch 99/120\n",
      "----------\n",
      "train loss: 0.0257, acc: 0.9942\n",
      "valid loss: 0.7954, acc: 0.8086\n",
      "steps: 4692 lr: 0.0131\n",
      "steps: 4692 lr: 0.0131\n",
      "Epoch 100/120\n",
      "----------\n",
      "train loss: 0.0202, acc: 0.9955\n",
      "valid loss: 0.7791, acc: 0.7954\n",
      "steps: 4301 lr: 0.0116\n",
      "steps: 4301 lr: 0.0116\n",
      "Epoch 101/120\n",
      "----------\n",
      "train loss: 0.0165, acc: 0.9974\n",
      "valid loss: 0.7951, acc: 0.8053\n",
      "steps: 3910 lr: 0.0100\n",
      "steps: 3910 lr: 0.0100\n",
      "Epoch 102/120\n",
      "----------\n",
      "train loss: 0.0152, acc: 0.9978\n",
      "valid loss: 0.8021, acc: 0.8086\n",
      "steps: 3519 lr: 0.0084\n",
      "steps: 3519 lr: 0.0084\n",
      "Epoch 103/120\n",
      "----------\n",
      "train loss: 0.0115, acc: 0.9978\n",
      "valid loss: 0.7894, acc: 0.7954\n",
      "steps: 3128 lr: 0.0069\n",
      "steps: 3128 lr: 0.0069\n",
      "Epoch 104/120\n",
      "----------\n",
      "train loss: 0.0099, acc: 0.9990\n",
      "valid loss: 0.8197, acc: 0.7987\n",
      "steps: 2737 lr: 0.0055\n",
      "steps: 2737 lr: 0.0055\n",
      "Epoch 105/120\n",
      "----------\n",
      "train loss: 0.0102, acc: 0.9990\n",
      "valid loss: 0.7783, acc: 0.7987\n",
      "steps: 2346 lr: 0.0041\n",
      "steps: 2346 lr: 0.0041\n",
      "Epoch 106/120\n",
      "----------\n",
      "train loss: 0.0090, acc: 0.9987\n",
      "valid loss: 0.7718, acc: 0.8053\n",
      "steps: 1955 lr: 0.0029\n",
      "steps: 1955 lr: 0.0029\n",
      "Epoch 107/120\n",
      "----------\n",
      "train loss: 0.0043, acc: 1.0000\n",
      "valid loss: 0.7793, acc: 0.7921\n",
      "steps: 1564 lr: 0.0019\n",
      "steps: 1564 lr: 0.0019\n",
      "Epoch 108/120\n",
      "----------\n",
      "train loss: 0.0097, acc: 0.9984\n",
      "valid loss: 0.7730, acc: 0.7954\n",
      "steps: 1173 lr: 0.0011\n",
      "steps: 1173 lr: 0.0011\n",
      "Epoch 109/120\n",
      "----------\n",
      "train loss: 0.0068, acc: 0.9994\n",
      "valid loss: 0.7871, acc: 0.7888\n",
      "steps: 782 lr: 0.0005\n",
      "steps: 782 lr: 0.0005\n",
      "Epoch 110/120\n",
      "----------\n",
      "train loss: 0.0071, acc: 0.9987\n",
      "valid loss: 0.7563, acc: 0.7987\n",
      "steps: 391 lr: 0.0001\n",
      "steps: 391 lr: 0.0001\n",
      "Epoch 111/120\n",
      "----------\n",
      "train loss: 0.0049, acc: 1.0000\n",
      "valid loss: 0.7679, acc: 0.8053\n",
      "steps: 0 lr: 0.0000\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# sgdwr\n",
    "# lr 0.02\n",
    "# mom 0.9\n",
    "best_acc, model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=120, cycle_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'room_classifier0zwx43qw.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0K\troom_classifier.pth\r\n",
      "4.0K\troom_classifier0zwx43qw.pth\r\n",
      "4.0K\troom_classifier2.pth\r\n",
      "4.0K\troom_classifier3.pth\r\n",
      "4.0K\troom_classifierlr2e-2regular.pth\r\n",
      "91M\troomclassifierd61d94g7.pth\r\n",
      "91M\troomclassifierdgupdu59.pth\r\n",
      "91M\troomclassifierfn1xqkbh.pth\r\n",
      "91M\troomclassifierfsb8sukn.pth\r\n",
      "91M\troomclassifierjw0x9qpx.pth\r\n",
      "91M\troomclassifiermensrhso.pth\r\n",
      "91M\troomclassifiermensrhsomtbz5bf8.pth\r\n",
      "91M\troomclassifiero4mp46rv.pth\r\n",
      "91M\ttestsave.pth\r\n",
      "91M\ttestsavea.pth\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh *.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 1.4430, acc: 0.4934\n",
      "valid loss: 1.2199, acc: 0.5908\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 1.1164, acc: 0.6140\n",
      "valid loss: 1.2407, acc: 0.5776\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 0.9594, acc: 0.6789\n",
      "valid loss: 0.8487, acc: 0.7030\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.8500, acc: 0.7026\n",
      "valid loss: 0.9182, acc: 0.6601\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.8059, acc: 0.7339\n",
      "valid loss: 1.0327, acc: 0.6535\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.7445, acc: 0.7483\n",
      "valid loss: 0.8386, acc: 0.7228\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.6838, acc: 0.7781\n",
      "valid loss: 0.8549, acc: 0.7228\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.6487, acc: 0.7835\n",
      "valid loss: 0.9472, acc: 0.7129\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.6143, acc: 0.8014\n",
      "valid loss: 0.8453, acc: 0.7294\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.5849, acc: 0.8072\n",
      "valid loss: 0.7919, acc: 0.7558\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.5959, acc: 0.8014\n",
      "valid loss: 1.0540, acc: 0.6898\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.5619, acc: 0.8190\n",
      "valid loss: 1.0896, acc: 0.6502\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.5248, acc: 0.8232\n",
      "valid loss: 0.9277, acc: 0.7294\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.4942, acc: 0.8337\n",
      "valid loss: 0.7993, acc: 0.7162\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.5000, acc: 0.8430\n",
      "valid loss: 0.8545, acc: 0.7228\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.4749, acc: 0.8462\n",
      "valid loss: 0.8453, acc: 0.7393\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.4712, acc: 0.8494\n",
      "valid loss: 0.7465, acc: 0.7690\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.4657, acc: 0.8494\n",
      "valid loss: 1.1834, acc: 0.6238\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.4734, acc: 0.8503\n",
      "valid loss: 0.7755, acc: 0.7492\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.3889, acc: 0.8801\n",
      "valid loss: 0.9972, acc: 0.6964\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.4457, acc: 0.8618\n",
      "valid loss: 0.9749, acc: 0.6997\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.3978, acc: 0.8810\n",
      "valid loss: 0.7084, acc: 0.7789\n",
      "Epoch 23/100\n",
      "----------\n",
      "train loss: 0.4011, acc: 0.8810\n",
      "valid loss: 1.0192, acc: 0.6667\n",
      "Epoch 24/100\n",
      "----------\n",
      "train loss: 0.3762, acc: 0.8916\n",
      "valid loss: 0.9171, acc: 0.6832\n",
      "Epoch 25/100\n",
      "----------\n",
      "train loss: 0.4145, acc: 0.8743\n",
      "valid loss: 0.7071, acc: 0.7657\n",
      "Epoch 26/100\n",
      "----------\n",
      "train loss: 0.4174, acc: 0.8730\n",
      "valid loss: 1.1453, acc: 0.6634\n",
      "Epoch 27/100\n",
      "----------\n",
      "train loss: 0.4026, acc: 0.8839\n",
      "valid loss: 0.8730, acc: 0.6997\n",
      "Epoch 28/100\n",
      "----------\n",
      "train loss: 0.4176, acc: 0.8750\n",
      "valid loss: 0.7898, acc: 0.7426\n",
      "Epoch 29/100\n",
      "----------\n",
      "train loss: 0.3970, acc: 0.8836\n",
      "valid loss: 0.9126, acc: 0.7327\n",
      "Epoch 30/100\n",
      "----------\n",
      "train loss: 0.3724, acc: 0.8862\n",
      "valid loss: 0.9393, acc: 0.6733\n",
      "Epoch 31/100\n",
      "----------\n",
      "train loss: 0.3714, acc: 0.8890\n",
      "valid loss: 1.3141, acc: 0.6271\n",
      "Epoch 32/100\n",
      "----------\n",
      "train loss: 0.3735, acc: 0.8865\n",
      "valid loss: 1.3854, acc: 0.6205\n",
      "Epoch 33/100\n",
      "----------\n",
      "train loss: 0.4040, acc: 0.8756\n",
      "valid loss: 0.8645, acc: 0.7063\n",
      "Epoch 34/100\n",
      "----------\n",
      "train loss: 0.3535, acc: 0.8970\n",
      "valid loss: 0.8811, acc: 0.7228\n",
      "Epoch 35/100\n",
      "----------\n",
      "train loss: 0.3525, acc: 0.8970\n",
      "valid loss: 0.9198, acc: 0.7360\n",
      "Epoch 36/100\n",
      "----------\n",
      "train loss: 0.4230, acc: 0.8647\n",
      "valid loss: 0.9566, acc: 0.6964\n",
      "Epoch 37/100\n",
      "----------\n",
      "train loss: 0.3573, acc: 0.8996\n",
      "valid loss: 0.8711, acc: 0.6931\n",
      "Epoch 38/100\n",
      "----------\n",
      "train loss: 0.3941, acc: 0.8890\n",
      "valid loss: 0.9354, acc: 0.7129\n",
      "Epoch 39/100\n",
      "----------\n",
      "train loss: 0.3860, acc: 0.8855\n",
      "valid loss: 0.9982, acc: 0.6964\n",
      "Epoch 40/100\n",
      "----------\n",
      "train loss: 0.3296, acc: 0.9098\n",
      "valid loss: 0.8019, acc: 0.7492\n",
      "Epoch 41/100\n",
      "----------\n",
      "train loss: 0.4068, acc: 0.8772\n",
      "valid loss: 0.8833, acc: 0.7195\n",
      "Epoch 42/100\n",
      "----------\n",
      "train loss: 0.3537, acc: 0.8973\n",
      "valid loss: 0.9874, acc: 0.7195\n",
      "Epoch 43/100\n",
      "----------\n",
      "train loss: 0.4001, acc: 0.8782\n",
      "valid loss: 1.3098, acc: 0.6238\n",
      "Epoch 44/100\n",
      "----------\n",
      "train loss: 0.3388, acc: 0.9053\n",
      "valid loss: 0.8131, acc: 0.7558\n",
      "Epoch 45/100\n",
      "----------\n",
      "train loss: 0.3527, acc: 0.9021\n",
      "valid loss: 1.0277, acc: 0.7096\n",
      "Epoch 46/100\n",
      "----------\n",
      "train loss: 0.3760, acc: 0.8929\n",
      "valid loss: 1.2233, acc: 0.6271\n",
      "Epoch 47/100\n",
      "----------\n",
      "train loss: 0.3817, acc: 0.8906\n",
      "valid loss: 1.0706, acc: 0.6535\n",
      "Epoch 48/100\n",
      "----------\n",
      "train loss: 0.3611, acc: 0.9034\n",
      "valid loss: 1.0108, acc: 0.6997\n",
      "Epoch 49/100\n",
      "----------\n",
      "train loss: 0.3226, acc: 0.9137\n",
      "valid loss: 0.8287, acc: 0.7426\n",
      "Epoch 50/100\n",
      "----------\n",
      "train loss: 0.3082, acc: 0.9165\n",
      "valid loss: 1.3756, acc: 0.6139\n",
      "Epoch 51/100\n",
      "----------\n",
      "train loss: 0.4177, acc: 0.8759\n",
      "valid loss: 0.9539, acc: 0.7162\n",
      "Epoch 52/100\n",
      "----------\n",
      "train loss: 0.3822, acc: 0.8878\n",
      "valid loss: 0.9317, acc: 0.6799\n",
      "Epoch 53/100\n",
      "----------\n",
      "train loss: 0.3741, acc: 0.8964\n",
      "valid loss: 1.0981, acc: 0.6733\n",
      "Epoch 54/100\n",
      "----------\n",
      "train loss: 0.3382, acc: 0.9053\n",
      "valid loss: 0.8616, acc: 0.7030\n",
      "Epoch 55/100\n",
      "----------\n",
      "train loss: 0.2885, acc: 0.9194\n",
      "valid loss: 0.9702, acc: 0.6997\n",
      "Epoch 56/100\n",
      "----------\n",
      "train loss: 0.3557, acc: 0.8916\n",
      "valid loss: 0.8125, acc: 0.7360\n",
      "Epoch 57/100\n",
      "----------\n",
      "train loss: 0.3523, acc: 0.9009\n",
      "valid loss: 1.1156, acc: 0.6634\n",
      "Epoch 58/100\n",
      "----------\n",
      "train loss: 0.3312, acc: 0.9089\n",
      "valid loss: 0.9296, acc: 0.7261\n",
      "Epoch 59/100\n",
      "----------\n",
      "train loss: 0.3833, acc: 0.8887\n",
      "valid loss: 0.9492, acc: 0.7195\n",
      "Epoch 60/100\n",
      "----------\n",
      "train loss: 0.3517, acc: 0.8980\n",
      "valid loss: 0.9122, acc: 0.7360\n",
      "Epoch 61/100\n",
      "----------\n",
      "train loss: 0.3031, acc: 0.9162\n",
      "valid loss: 0.9554, acc: 0.6832\n",
      "Epoch 62/100\n",
      "----------\n",
      "train loss: 0.3068, acc: 0.9130\n",
      "valid loss: 1.0741, acc: 0.6634\n",
      "Epoch 63/100\n",
      "----------\n",
      "train loss: 0.3450, acc: 0.9009\n",
      "valid loss: 1.0769, acc: 0.6700\n",
      "Epoch 64/100\n",
      "----------\n",
      "train loss: 0.3073, acc: 0.9169\n",
      "valid loss: 0.8863, acc: 0.7459\n",
      "Epoch 65/100\n",
      "----------\n",
      "train loss: 0.3550, acc: 0.8973\n",
      "valid loss: 0.8564, acc: 0.7360\n",
      "Epoch 66/100\n",
      "----------\n",
      "train loss: 0.3633, acc: 0.8967\n",
      "valid loss: 0.9375, acc: 0.7162\n",
      "Epoch 67/100\n",
      "----------\n",
      "train loss: 0.3163, acc: 0.9149\n",
      "valid loss: 0.9194, acc: 0.7129\n",
      "Epoch 68/100\n",
      "----------\n",
      "train loss: 0.3094, acc: 0.9178\n",
      "valid loss: 0.9641, acc: 0.7129\n",
      "Epoch 69/100\n",
      "----------\n",
      "train loss: 0.3313, acc: 0.9063\n",
      "valid loss: 0.7932, acc: 0.7393\n",
      "Epoch 70/100\n",
      "----------\n",
      "train loss: 0.3494, acc: 0.8983\n",
      "valid loss: 1.0670, acc: 0.6568\n",
      "Epoch 71/100\n",
      "----------\n",
      "train loss: 0.3220, acc: 0.9111\n",
      "valid loss: 0.9030, acc: 0.7261\n",
      "Epoch 72/100\n",
      "----------\n",
      "train loss: 0.3322, acc: 0.9105\n",
      "valid loss: 0.9284, acc: 0.7063\n",
      "Epoch 73/100\n",
      "----------\n",
      "train loss: 0.3201, acc: 0.9121\n",
      "valid loss: 0.8451, acc: 0.7393\n",
      "Epoch 74/100\n",
      "----------\n",
      "train loss: 0.2973, acc: 0.9181\n",
      "valid loss: 0.8751, acc: 0.7228\n",
      "Epoch 75/100\n",
      "----------\n",
      "train loss: 0.3410, acc: 0.9041\n",
      "valid loss: 0.9964, acc: 0.6733\n",
      "Epoch 76/100\n",
      "----------\n",
      "train loss: 0.3576, acc: 0.8961\n",
      "valid loss: 0.9385, acc: 0.7129\n",
      "Epoch 77/100\n",
      "----------\n",
      "train loss: 0.3349, acc: 0.9053\n",
      "valid loss: 0.8404, acc: 0.7294\n",
      "Epoch 78/100\n",
      "----------\n",
      "train loss: 0.2952, acc: 0.9239\n",
      "valid loss: 0.9786, acc: 0.7030\n",
      "Epoch 79/100\n",
      "----------\n",
      "train loss: 0.3186, acc: 0.9172\n",
      "valid loss: 0.9811, acc: 0.7063\n",
      "Epoch 80/100\n",
      "----------\n",
      "train loss: 0.3183, acc: 0.9124\n",
      "valid loss: 0.8456, acc: 0.7393\n",
      "Epoch 81/100\n",
      "----------\n",
      "train loss: 0.2968, acc: 0.9181\n",
      "valid loss: 0.9783, acc: 0.6931\n",
      "Epoch 82/100\n",
      "----------\n",
      "train loss: 0.3185, acc: 0.9105\n",
      "valid loss: 1.1773, acc: 0.6370\n",
      "Epoch 83/100\n",
      "----------\n",
      "train loss: 0.3296, acc: 0.9089\n",
      "valid loss: 1.0304, acc: 0.6766\n",
      "Epoch 84/100\n",
      "----------\n",
      "train loss: 0.2798, acc: 0.9236\n",
      "valid loss: 1.0159, acc: 0.7063\n",
      "Epoch 85/100\n",
      "----------\n",
      "train loss: 0.3552, acc: 0.9012\n",
      "valid loss: 0.9505, acc: 0.7162\n",
      "Epoch 86/100\n",
      "----------\n",
      "train loss: 0.3369, acc: 0.9041\n",
      "valid loss: 0.8894, acc: 0.7096\n",
      "Epoch 87/100\n",
      "----------\n",
      "train loss: 0.3002, acc: 0.9194\n",
      "valid loss: 0.8361, acc: 0.7360\n",
      "Epoch 88/100\n",
      "----------\n",
      "train loss: 0.2916, acc: 0.9210\n",
      "valid loss: 0.8255, acc: 0.7294\n",
      "Epoch 89/100\n",
      "----------\n",
      "train loss: 0.3029, acc: 0.9175\n",
      "valid loss: 0.9175, acc: 0.7129\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.2857, acc: 0.9197\n",
      "valid loss: 1.0573, acc: 0.6733\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.3597, acc: 0.8929\n",
      "valid loss: 0.9189, acc: 0.7393\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.2557, acc: 0.9357\n",
      "valid loss: 1.0094, acc: 0.6964\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.3400, acc: 0.9050\n",
      "valid loss: 0.8899, acc: 0.7294\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.3029, acc: 0.9165\n",
      "valid loss: 0.9048, acc: 0.7327\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.3186, acc: 0.9041\n",
      "valid loss: 1.0251, acc: 0.6964\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.2731, acc: 0.9245\n",
      "valid loss: 1.2173, acc: 0.6205\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.2882, acc: 0.9239\n",
      "valid loss: 1.2275, acc: 0.6469\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.2871, acc: 0.9239\n",
      "valid loss: 0.9353, acc: 0.7030\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.3184, acc: 0.9124\n",
      "valid loss: 1.0058, acc: 0.6931\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.2758, acc: 0.9252\n",
      "valid loss: 0.9177, acc: 0.7294\n"
     ]
    }
   ],
   "source": [
    "# sgd\n",
    "# lr 0.04\n",
    "# wd 1e-3, mom 0.5\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=100, cycle_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'roomclassifiero4mp46rv.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 1.6415, acc: 0.4049\n",
      "valid loss: 1.4189, acc: 0.4851\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 1.2733, acc: 0.5526\n",
      "valid loss: 1.2364, acc: 0.5710\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 1.1270, acc: 0.6143\n",
      "valid loss: 1.4781, acc: 0.5281\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.9807, acc: 0.6713\n",
      "valid loss: 1.2693, acc: 0.5974\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.9600, acc: 0.6697\n",
      "valid loss: 1.1332, acc: 0.6337\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.8572, acc: 0.7090\n",
      "valid loss: 0.9492, acc: 0.6667\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.8228, acc: 0.7259\n",
      "valid loss: 0.9079, acc: 0.7063\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.7429, acc: 0.7477\n",
      "valid loss: 0.8208, acc: 0.7030\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.6868, acc: 0.7717\n",
      "valid loss: 1.0799, acc: 0.6634\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.6299, acc: 0.7889\n",
      "valid loss: 0.8470, acc: 0.7195\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.6027, acc: 0.7960\n",
      "valid loss: 0.8835, acc: 0.7162\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.5544, acc: 0.8184\n",
      "valid loss: 0.7648, acc: 0.7459\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.5264, acc: 0.8200\n",
      "valid loss: 0.9099, acc: 0.7030\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.5071, acc: 0.8366\n",
      "valid loss: 0.9374, acc: 0.7228\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.4442, acc: 0.8494\n",
      "valid loss: 0.6888, acc: 0.7888\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.4152, acc: 0.8670\n",
      "valid loss: 0.8472, acc: 0.7063\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.4221, acc: 0.8618\n",
      "valid loss: 0.8237, acc: 0.7327\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.4035, acc: 0.8641\n",
      "valid loss: 0.9826, acc: 0.7030\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.3516, acc: 0.8833\n",
      "valid loss: 0.6759, acc: 0.7888\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.3197, acc: 0.9009\n",
      "valid loss: 0.8305, acc: 0.7756\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.3536, acc: 0.8878\n",
      "valid loss: 0.7986, acc: 0.7657\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.2880, acc: 0.9130\n",
      "valid loss: 0.8481, acc: 0.7426\n",
      "Epoch 23/100\n",
      "----------\n",
      "train loss: 0.2505, acc: 0.9220\n",
      "valid loss: 0.8797, acc: 0.7393\n",
      "Epoch 24/100\n",
      "----------\n",
      "train loss: 0.2655, acc: 0.9143\n",
      "valid loss: 0.7976, acc: 0.7525\n",
      "Epoch 25/100\n",
      "----------\n",
      "train loss: 0.2404, acc: 0.9261\n",
      "valid loss: 0.8196, acc: 0.7657\n",
      "Epoch 26/100\n",
      "----------\n",
      "train loss: 0.2318, acc: 0.9312\n",
      "valid loss: 1.0053, acc: 0.7228\n",
      "Epoch 27/100\n",
      "----------\n",
      "train loss: 0.2037, acc: 0.9389\n",
      "valid loss: 0.8823, acc: 0.7525\n",
      "Epoch 28/100\n",
      "----------\n",
      "train loss: 0.2248, acc: 0.9284\n",
      "valid loss: 1.1090, acc: 0.6766\n",
      "Epoch 29/100\n",
      "----------\n",
      "train loss: 0.2117, acc: 0.9360\n",
      "valid loss: 0.7795, acc: 0.7888\n",
      "Epoch 30/100\n",
      "----------\n",
      "train loss: 0.1905, acc: 0.9424\n",
      "valid loss: 0.8747, acc: 0.7393\n",
      "Epoch 31/100\n",
      "----------\n",
      "train loss: 0.1684, acc: 0.9482\n",
      "valid loss: 0.8664, acc: 0.7393\n",
      "Epoch 32/100\n",
      "----------\n",
      "train loss: 0.1663, acc: 0.9498\n",
      "valid loss: 0.8291, acc: 0.7591\n",
      "Epoch 33/100\n",
      "----------\n",
      "train loss: 0.1559, acc: 0.9533\n",
      "valid loss: 1.0293, acc: 0.7129\n",
      "Epoch 34/100\n",
      "----------\n",
      "train loss: 0.1556, acc: 0.9520\n",
      "valid loss: 0.7394, acc: 0.7921\n",
      "Epoch 35/100\n",
      "----------\n",
      "train loss: 0.1219, acc: 0.9651\n",
      "valid loss: 0.8333, acc: 0.7459\n",
      "Epoch 36/100\n",
      "----------\n",
      "train loss: 0.1467, acc: 0.9508\n",
      "valid loss: 0.8853, acc: 0.7426\n",
      "Epoch 37/100\n",
      "----------\n",
      "train loss: 0.1411, acc: 0.9565\n",
      "valid loss: 0.8937, acc: 0.7624\n",
      "Epoch 38/100\n",
      "----------\n",
      "train loss: 0.1614, acc: 0.9498\n",
      "valid loss: 1.2584, acc: 0.6865\n",
      "Epoch 39/100\n",
      "----------\n",
      "train loss: 0.1672, acc: 0.9524\n",
      "valid loss: 0.9211, acc: 0.7459\n",
      "Epoch 40/100\n",
      "----------\n",
      "train loss: 0.1269, acc: 0.9648\n",
      "valid loss: 0.8875, acc: 0.7558\n",
      "Epoch 41/100\n",
      "----------\n",
      "train loss: 0.1113, acc: 0.9677\n",
      "valid loss: 0.8406, acc: 0.7789\n",
      "Epoch 42/100\n",
      "----------\n",
      "train loss: 0.1276, acc: 0.9639\n",
      "valid loss: 0.6999, acc: 0.8020\n",
      "Epoch 43/100\n",
      "----------\n",
      "train loss: 0.1070, acc: 0.9722\n",
      "valid loss: 0.9559, acc: 0.7657\n",
      "Epoch 44/100\n",
      "----------\n",
      "train loss: 0.0969, acc: 0.9725\n",
      "valid loss: 0.7380, acc: 0.7657\n",
      "Epoch 45/100\n",
      "----------\n",
      "train loss: 0.0890, acc: 0.9760\n",
      "valid loss: 0.8038, acc: 0.7921\n",
      "Epoch 46/100\n",
      "----------\n",
      "train loss: 0.1035, acc: 0.9696\n",
      "valid loss: 1.0416, acc: 0.7261\n",
      "Epoch 47/100\n",
      "----------\n",
      "train loss: 0.1360, acc: 0.9565\n",
      "valid loss: 1.1544, acc: 0.7426\n",
      "Epoch 48/100\n",
      "----------\n",
      "train loss: 0.1233, acc: 0.9642\n",
      "valid loss: 1.0153, acc: 0.7228\n",
      "Epoch 49/100\n",
      "----------\n",
      "train loss: 0.1039, acc: 0.9699\n",
      "valid loss: 0.7822, acc: 0.7624\n",
      "Epoch 50/100\n",
      "----------\n",
      "train loss: 0.0923, acc: 0.9747\n",
      "valid loss: 0.7869, acc: 0.7921\n",
      "Epoch 51/100\n",
      "----------\n",
      "train loss: 0.0995, acc: 0.9741\n",
      "valid loss: 1.0329, acc: 0.7525\n",
      "Epoch 52/100\n",
      "----------\n",
      "train loss: 0.0803, acc: 0.9789\n",
      "valid loss: 0.7702, acc: 0.7789\n",
      "Epoch 53/100\n",
      "----------\n",
      "train loss: 0.0916, acc: 0.9754\n",
      "valid loss: 0.8859, acc: 0.7492\n",
      "Epoch 54/100\n",
      "----------\n",
      "train loss: 0.0760, acc: 0.9795\n",
      "valid loss: 0.8511, acc: 0.7624\n",
      "Epoch 55/100\n",
      "----------\n",
      "train loss: 0.1269, acc: 0.9642\n",
      "valid loss: 0.8477, acc: 0.7492\n",
      "Epoch 56/100\n",
      "----------\n",
      "train loss: 0.1049, acc: 0.9683\n",
      "valid loss: 0.8734, acc: 0.7624\n",
      "Epoch 57/100\n",
      "----------\n",
      "train loss: 0.0753, acc: 0.9805\n",
      "valid loss: 1.1700, acc: 0.7195\n",
      "Epoch 58/100\n",
      "----------\n",
      "train loss: 0.0942, acc: 0.9728\n",
      "valid loss: 0.8347, acc: 0.7591\n",
      "Epoch 59/100\n",
      "----------\n",
      "train loss: 0.0828, acc: 0.9738\n",
      "valid loss: 0.9345, acc: 0.7558\n",
      "Epoch 60/100\n",
      "----------\n",
      "train loss: 0.0771, acc: 0.9776\n",
      "valid loss: 0.8212, acc: 0.7558\n",
      "Epoch 61/100\n",
      "----------\n",
      "train loss: 0.0756, acc: 0.9789\n",
      "valid loss: 0.8375, acc: 0.7756\n",
      "Epoch 62/100\n",
      "----------\n",
      "train loss: 0.0890, acc: 0.9747\n",
      "valid loss: 0.9795, acc: 0.7459\n",
      "Epoch 63/100\n",
      "----------\n",
      "train loss: 0.0728, acc: 0.9815\n",
      "valid loss: 0.7780, acc: 0.7954\n",
      "Epoch 64/100\n",
      "----------\n",
      "train loss: 0.0717, acc: 0.9831\n",
      "valid loss: 0.7890, acc: 0.7789\n",
      "Epoch 65/100\n",
      "----------\n",
      "train loss: 0.0671, acc: 0.9834\n",
      "valid loss: 0.8869, acc: 0.7657\n",
      "Epoch 66/100\n",
      "----------\n",
      "train loss: 0.0637, acc: 0.9843\n",
      "valid loss: 0.8670, acc: 0.7921\n",
      "Epoch 67/100\n",
      "----------\n",
      "train loss: 0.0699, acc: 0.9811\n",
      "valid loss: 0.9000, acc: 0.7855\n",
      "Epoch 68/100\n",
      "----------\n",
      "train loss: 0.0728, acc: 0.9773\n",
      "valid loss: 0.8399, acc: 0.7888\n",
      "Epoch 69/100\n",
      "----------\n",
      "train loss: 0.0684, acc: 0.9818\n",
      "valid loss: 0.8499, acc: 0.7558\n",
      "Epoch 70/100\n",
      "----------\n",
      "train loss: 0.0581, acc: 0.9866\n",
      "valid loss: 1.1727, acc: 0.7195\n",
      "Epoch 71/100\n",
      "----------\n",
      "train loss: 0.0920, acc: 0.9722\n",
      "valid loss: 0.8625, acc: 0.7591\n",
      "Epoch 72/100\n",
      "----------\n",
      "train loss: 0.0937, acc: 0.9722\n",
      "valid loss: 0.9036, acc: 0.7690\n",
      "Epoch 73/100\n",
      "----------\n",
      "train loss: 0.0774, acc: 0.9776\n",
      "valid loss: 0.8434, acc: 0.7558\n",
      "Epoch 74/100\n",
      "----------\n",
      "train loss: 0.0813, acc: 0.9760\n",
      "valid loss: 0.8045, acc: 0.7789\n",
      "Epoch 75/100\n",
      "----------\n",
      "train loss: 0.0866, acc: 0.9760\n",
      "valid loss: 0.7560, acc: 0.7690\n",
      "Epoch 76/100\n",
      "----------\n",
      "train loss: 0.0691, acc: 0.9811\n",
      "valid loss: 0.7916, acc: 0.7789\n",
      "Epoch 77/100\n",
      "----------\n",
      "train loss: 0.0750, acc: 0.9773\n",
      "valid loss: 0.9292, acc: 0.7459\n",
      "Epoch 78/100\n",
      "----------\n",
      "train loss: 0.0669, acc: 0.9815\n",
      "valid loss: 0.8495, acc: 0.7591\n",
      "Epoch 79/100\n",
      "----------\n",
      "train loss: 0.0412, acc: 0.9907\n",
      "valid loss: 0.8017, acc: 0.7723\n",
      "Epoch 80/100\n",
      "----------\n",
      "train loss: 0.0463, acc: 0.9885\n",
      "valid loss: 1.0013, acc: 0.7426\n",
      "Epoch 81/100\n",
      "----------\n",
      "train loss: 0.0788, acc: 0.9805\n",
      "valid loss: 0.8209, acc: 0.7789\n",
      "Epoch 82/100\n",
      "----------\n",
      "train loss: 0.0830, acc: 0.9760\n",
      "valid loss: 0.8152, acc: 0.7624\n",
      "Epoch 83/100\n",
      "----------\n",
      "train loss: 0.0698, acc: 0.9824\n",
      "valid loss: 0.8936, acc: 0.7426\n",
      "Epoch 84/100\n",
      "----------\n",
      "train loss: 0.0645, acc: 0.9808\n",
      "valid loss: 0.9169, acc: 0.7492\n",
      "Epoch 85/100\n",
      "----------\n",
      "train loss: 0.0456, acc: 0.9907\n",
      "valid loss: 0.8371, acc: 0.7525\n",
      "Epoch 86/100\n",
      "----------\n",
      "train loss: 0.0401, acc: 0.9917\n",
      "valid loss: 0.7489, acc: 0.7987\n",
      "Epoch 87/100\n",
      "----------\n",
      "train loss: 0.0396, acc: 0.9917\n",
      "valid loss: 0.9036, acc: 0.7591\n",
      "Epoch 88/100\n",
      "----------\n",
      "train loss: 0.0747, acc: 0.9789\n",
      "valid loss: 0.7849, acc: 0.7624\n",
      "Epoch 89/100\n",
      "----------\n",
      "train loss: 0.0990, acc: 0.9719\n",
      "valid loss: 0.9674, acc: 0.7096\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.0793, acc: 0.9783\n",
      "valid loss: 0.8288, acc: 0.7690\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.0777, acc: 0.9779\n",
      "valid loss: 1.0213, acc: 0.7327\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.1064, acc: 0.9706\n",
      "valid loss: 0.8144, acc: 0.7756\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.0519, acc: 0.9898\n",
      "valid loss: 0.8969, acc: 0.7426\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0410, acc: 0.9894\n",
      "valid loss: 0.7760, acc: 0.7690\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.0368, acc: 0.9910\n",
      "valid loss: 0.8848, acc: 0.7723\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.0253, acc: 0.9958\n",
      "valid loss: 0.7887, acc: 0.7690\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.0362, acc: 0.9914\n",
      "valid loss: 0.9515, acc: 0.7393\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.0725, acc: 0.9795\n",
      "valid loss: 0.8768, acc: 0.7789\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.1084, acc: 0.9683\n",
      "valid loss: 0.9141, acc: 0.7426\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.0609, acc: 0.9859\n",
      "valid loss: 0.9030, acc: 0.7426\n"
     ]
    }
   ],
   "source": [
    "# sgd\n",
    "# LR 0.07 mom 0.5 wd 1e-4\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=100, cycle_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'roomclassifierd61d94g7.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 1.6077, acc: 0.4218\n",
      "valid loss: 2.4986, acc: 0.2970\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 1.2485, acc: 0.5654\n",
      "valid loss: 1.3808, acc: 0.5347\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 1.0792, acc: 0.6284\n",
      "valid loss: 0.8973, acc: 0.7030\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.9359, acc: 0.6757\n",
      "valid loss: 1.4245, acc: 0.5842\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.8619, acc: 0.6968\n",
      "valid loss: 0.8538, acc: 0.6898\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.7883, acc: 0.7349\n",
      "valid loss: 1.0097, acc: 0.6634\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.7091, acc: 0.7605\n",
      "valid loss: 1.1751, acc: 0.6601\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.6796, acc: 0.7742\n",
      "valid loss: 0.9359, acc: 0.7162\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.6281, acc: 0.7880\n",
      "valid loss: 1.0054, acc: 0.6733\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.5771, acc: 0.8164\n",
      "valid loss: 1.0936, acc: 0.6601\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.5324, acc: 0.8222\n",
      "valid loss: 0.8841, acc: 0.7096\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.4947, acc: 0.8350\n",
      "valid loss: 0.8499, acc: 0.7294\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.4537, acc: 0.8548\n",
      "valid loss: 1.0461, acc: 0.7063\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.4532, acc: 0.8580\n",
      "valid loss: 1.0420, acc: 0.6733\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.4073, acc: 0.8657\n",
      "valid loss: 0.7305, acc: 0.7723\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.3691, acc: 0.8878\n",
      "valid loss: 1.0407, acc: 0.6964\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.3411, acc: 0.8894\n",
      "valid loss: 0.8839, acc: 0.7063\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.3085, acc: 0.9021\n",
      "valid loss: 0.9487, acc: 0.7063\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.3180, acc: 0.8887\n",
      "valid loss: 1.2882, acc: 0.6502\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.2723, acc: 0.9130\n",
      "valid loss: 0.8268, acc: 0.7360\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.2645, acc: 0.9226\n",
      "valid loss: 0.8310, acc: 0.7624\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.2226, acc: 0.9364\n",
      "valid loss: 0.7490, acc: 0.7591\n",
      "Epoch 23/100\n",
      "----------\n",
      "train loss: 0.2278, acc: 0.9284\n",
      "valid loss: 0.7167, acc: 0.8020\n",
      "Epoch 24/100\n",
      "----------\n",
      "train loss: 0.2395, acc: 0.9245\n",
      "valid loss: 0.9112, acc: 0.7228\n",
      "Epoch 25/100\n",
      "----------\n",
      "train loss: 0.1938, acc: 0.9386\n",
      "valid loss: 0.8775, acc: 0.7558\n",
      "Epoch 26/100\n",
      "----------\n",
      "train loss: 0.1924, acc: 0.9367\n",
      "valid loss: 1.0313, acc: 0.7327\n",
      "Epoch 27/100\n",
      "----------\n",
      "train loss: 0.1888, acc: 0.9488\n",
      "valid loss: 0.7833, acc: 0.7657\n",
      "Epoch 28/100\n",
      "----------\n",
      "train loss: 0.1647, acc: 0.9485\n",
      "valid loss: 0.7915, acc: 0.7657\n",
      "Epoch 29/100\n",
      "----------\n",
      "train loss: 0.1479, acc: 0.9591\n",
      "valid loss: 0.6668, acc: 0.7657\n",
      "Epoch 30/100\n",
      "----------\n",
      "train loss: 0.1415, acc: 0.9613\n",
      "valid loss: 0.8181, acc: 0.7525\n",
      "Epoch 31/100\n",
      "----------\n",
      "train loss: 0.1307, acc: 0.9648\n",
      "valid loss: 0.9733, acc: 0.7129\n",
      "Epoch 32/100\n",
      "----------\n",
      "train loss: 0.1488, acc: 0.9562\n",
      "valid loss: 0.7808, acc: 0.7855\n",
      "Epoch 33/100\n",
      "----------\n",
      "train loss: 0.1520, acc: 0.9555\n",
      "valid loss: 0.7839, acc: 0.7888\n",
      "Epoch 34/100\n",
      "----------\n",
      "train loss: 0.1138, acc: 0.9671\n",
      "valid loss: 0.8370, acc: 0.7888\n",
      "Epoch 35/100\n",
      "----------\n",
      "train loss: 0.1125, acc: 0.9677\n",
      "valid loss: 0.7484, acc: 0.7789\n",
      "Epoch 36/100\n",
      "----------\n",
      "train loss: 0.1116, acc: 0.9715\n",
      "valid loss: 0.7850, acc: 0.7822\n",
      "Epoch 37/100\n",
      "----------\n",
      "train loss: 0.1009, acc: 0.9751\n",
      "valid loss: 0.7678, acc: 0.7789\n",
      "Epoch 38/100\n",
      "----------\n",
      "train loss: 0.1048, acc: 0.9715\n",
      "valid loss: 0.7868, acc: 0.7921\n",
      "Epoch 39/100\n",
      "----------\n",
      "train loss: 0.1145, acc: 0.9699\n",
      "valid loss: 0.8551, acc: 0.7558\n",
      "Epoch 40/100\n",
      "----------\n",
      "train loss: 0.1139, acc: 0.9658\n",
      "valid loss: 0.7514, acc: 0.8086\n",
      "Epoch 41/100\n",
      "----------\n",
      "train loss: 0.0989, acc: 0.9738\n",
      "valid loss: 0.7365, acc: 0.7657\n",
      "Epoch 42/100\n",
      "----------\n",
      "train loss: 0.1042, acc: 0.9744\n",
      "valid loss: 0.8335, acc: 0.7591\n",
      "Epoch 43/100\n",
      "----------\n",
      "train loss: 0.0849, acc: 0.9763\n",
      "valid loss: 0.7419, acc: 0.7723\n",
      "Epoch 44/100\n",
      "----------\n",
      "train loss: 0.0683, acc: 0.9837\n",
      "valid loss: 0.7328, acc: 0.7954\n",
      "Epoch 45/100\n",
      "----------\n",
      "train loss: 0.0580, acc: 0.9866\n",
      "valid loss: 1.0983, acc: 0.7030\n",
      "Epoch 46/100\n",
      "----------\n",
      "train loss: 0.0855, acc: 0.9757\n",
      "valid loss: 0.8156, acc: 0.7624\n",
      "Epoch 47/100\n",
      "----------\n",
      "train loss: 0.0675, acc: 0.9821\n",
      "valid loss: 0.9090, acc: 0.7624\n",
      "Epoch 48/100\n",
      "----------\n",
      "train loss: 0.0667, acc: 0.9827\n",
      "valid loss: 0.7306, acc: 0.7690\n",
      "Epoch 49/100\n",
      "----------\n",
      "train loss: 0.0884, acc: 0.9760\n",
      "valid loss: 0.7440, acc: 0.7789\n",
      "Epoch 50/100\n",
      "----------\n",
      "train loss: 0.0499, acc: 0.9894\n",
      "valid loss: 0.8177, acc: 0.7624\n",
      "Epoch 51/100\n",
      "----------\n",
      "train loss: 0.0806, acc: 0.9802\n",
      "valid loss: 0.7871, acc: 0.7360\n",
      "Epoch 52/100\n",
      "----------\n",
      "train loss: 0.0847, acc: 0.9776\n",
      "valid loss: 0.6921, acc: 0.7987\n",
      "Epoch 53/100\n",
      "----------\n",
      "train loss: 0.0908, acc: 0.9744\n",
      "valid loss: 1.0162, acc: 0.7393\n",
      "Epoch 54/100\n",
      "----------\n",
      "train loss: 0.1000, acc: 0.9722\n",
      "valid loss: 0.7793, acc: 0.7723\n",
      "Epoch 55/100\n",
      "----------\n",
      "train loss: 0.0672, acc: 0.9831\n",
      "valid loss: 0.7679, acc: 0.7921\n",
      "Epoch 56/100\n",
      "----------\n",
      "train loss: 0.0646, acc: 0.9831\n",
      "valid loss: 0.8092, acc: 0.7954\n",
      "Epoch 57/100\n",
      "----------\n",
      "train loss: 0.0701, acc: 0.9818\n",
      "valid loss: 0.8775, acc: 0.7888\n",
      "Epoch 58/100\n",
      "----------\n",
      "train loss: 0.0781, acc: 0.9805\n",
      "valid loss: 0.9833, acc: 0.7327\n",
      "Epoch 59/100\n",
      "----------\n",
      "train loss: 0.0668, acc: 0.9815\n",
      "valid loss: 0.8516, acc: 0.7690\n",
      "Epoch 60/100\n",
      "----------\n",
      "train loss: 0.0552, acc: 0.9878\n",
      "valid loss: 0.8136, acc: 0.7756\n",
      "Epoch 61/100\n",
      "----------\n",
      "train loss: 0.0594, acc: 0.9862\n",
      "valid loss: 0.7567, acc: 0.7987\n",
      "Epoch 62/100\n",
      "----------\n",
      "train loss: 0.0577, acc: 0.9853\n",
      "valid loss: 0.7031, acc: 0.7888\n",
      "Epoch 63/100\n",
      "----------\n",
      "train loss: 0.0589, acc: 0.9866\n",
      "valid loss: 0.7259, acc: 0.8086\n",
      "Epoch 64/100\n",
      "----------\n",
      "train loss: 0.0741, acc: 0.9767\n",
      "valid loss: 0.9686, acc: 0.7624\n",
      "Epoch 65/100\n",
      "----------\n",
      "train loss: 0.0540, acc: 0.9882\n",
      "valid loss: 0.6917, acc: 0.7921\n",
      "Epoch 66/100\n",
      "----------\n",
      "train loss: 0.0349, acc: 0.9930\n",
      "valid loss: 0.6735, acc: 0.7954\n",
      "Epoch 67/100\n",
      "----------\n",
      "train loss: 0.0270, acc: 0.9952\n",
      "valid loss: 0.6489, acc: 0.8251\n",
      "Epoch 68/100\n",
      "----------\n",
      "train loss: 0.0453, acc: 0.9878\n",
      "valid loss: 0.9079, acc: 0.7393\n",
      "Epoch 69/100\n",
      "----------\n",
      "train loss: 0.0757, acc: 0.9786\n",
      "valid loss: 0.8276, acc: 0.7492\n",
      "Epoch 70/100\n",
      "----------\n",
      "train loss: 0.0610, acc: 0.9837\n",
      "valid loss: 0.7546, acc: 0.7954\n",
      "Epoch 71/100\n",
      "----------\n",
      "train loss: 0.0729, acc: 0.9808\n",
      "valid loss: 0.7096, acc: 0.7987\n",
      "Epoch 72/100\n",
      "----------\n",
      "train loss: 0.0599, acc: 0.9834\n",
      "valid loss: 0.7357, acc: 0.8086\n",
      "Epoch 73/100\n",
      "----------\n",
      "train loss: 0.0673, acc: 0.9831\n",
      "valid loss: 0.8264, acc: 0.7756\n",
      "Epoch 74/100\n",
      "----------\n",
      "train loss: 0.0756, acc: 0.9789\n",
      "valid loss: 0.7904, acc: 0.7690\n",
      "Epoch 75/100\n",
      "----------\n",
      "train loss: 0.0816, acc: 0.9760\n",
      "valid loss: 0.8079, acc: 0.7558\n",
      "Epoch 76/100\n",
      "----------\n",
      "train loss: 0.0465, acc: 0.9901\n",
      "valid loss: 0.8119, acc: 0.7657\n",
      "Epoch 77/100\n",
      "----------\n",
      "train loss: 0.0573, acc: 0.9834\n",
      "valid loss: 0.8170, acc: 0.7756\n",
      "Epoch 78/100\n",
      "----------\n",
      "train loss: 0.0690, acc: 0.9821\n",
      "valid loss: 0.7191, acc: 0.7987\n",
      "Epoch 79/100\n",
      "----------\n",
      "train loss: 0.0646, acc: 0.9837\n",
      "valid loss: 0.7639, acc: 0.7888\n",
      "Epoch 80/100\n",
      "----------\n",
      "train loss: 0.0473, acc: 0.9885\n",
      "valid loss: 0.8556, acc: 0.7624\n",
      "Epoch 81/100\n",
      "----------\n",
      "train loss: 0.0636, acc: 0.9840\n",
      "valid loss: 0.8430, acc: 0.7591\n",
      "Epoch 82/100\n",
      "----------\n",
      "train loss: 0.0749, acc: 0.9805\n",
      "valid loss: 0.7902, acc: 0.7855\n",
      "Epoch 83/100\n",
      "----------\n",
      "train loss: 0.0493, acc: 0.9869\n",
      "valid loss: 0.7075, acc: 0.7921\n",
      "Epoch 84/100\n",
      "----------\n",
      "train loss: 0.0585, acc: 0.9843\n",
      "valid loss: 0.7620, acc: 0.7888\n",
      "Epoch 85/100\n",
      "----------\n",
      "train loss: 0.0281, acc: 0.9958\n",
      "valid loss: 0.7440, acc: 0.8053\n",
      "Epoch 86/100\n",
      "----------\n",
      "train loss: 0.0669, acc: 0.9815\n",
      "valid loss: 0.8667, acc: 0.7690\n",
      "Epoch 87/100\n",
      "----------\n",
      "train loss: 0.0660, acc: 0.9827\n",
      "valid loss: 0.7491, acc: 0.7855\n",
      "Epoch 88/100\n",
      "----------\n",
      "train loss: 0.0517, acc: 0.9882\n",
      "valid loss: 0.8536, acc: 0.7789\n",
      "Epoch 89/100\n",
      "----------\n",
      "train loss: 0.1023, acc: 0.9715\n",
      "valid loss: 1.0693, acc: 0.7558\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.0942, acc: 0.9738\n",
      "valid loss: 0.9767, acc: 0.7360\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.0442, acc: 0.9898\n",
      "valid loss: 0.7546, acc: 0.7921\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.0318, acc: 0.9930\n",
      "valid loss: 0.8639, acc: 0.7591\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.0356, acc: 0.9917\n",
      "valid loss: 0.7468, acc: 0.8053\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0437, acc: 0.9894\n",
      "valid loss: 0.7940, acc: 0.7888\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.0537, acc: 0.9850\n",
      "valid loss: 0.9064, acc: 0.7690\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.0591, acc: 0.9827\n",
      "valid loss: 0.7452, acc: 0.7756\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.0424, acc: 0.9901\n",
      "valid loss: 0.7976, acc: 0.7756\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.0510, acc: 0.9894\n",
      "valid loss: 1.0071, acc: 0.7162\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.0451, acc: 0.9901\n",
      "valid loss: 0.8105, acc: 0.8086\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.0305, acc: 0.9933\n",
      "valid loss: 0.7709, acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "# sgd\n",
    "# LR 0.06 --\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'roomclassifierjw0x9qpx.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 1.5011, acc: 0.4637\n",
      "valid loss: 1.3025, acc: 0.5347\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 1.1601, acc: 0.6073\n",
      "valid loss: 0.9797, acc: 0.6766\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 0.9888, acc: 0.6684\n",
      "valid loss: 1.0652, acc: 0.6304\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.8903, acc: 0.6984\n",
      "valid loss: 1.1270, acc: 0.6403\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.7900, acc: 0.7343\n",
      "valid loss: 0.8147, acc: 0.7195\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.7151, acc: 0.7672\n",
      "valid loss: 0.7903, acc: 0.7162\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.6628, acc: 0.7928\n",
      "valid loss: 1.0088, acc: 0.6865\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.6102, acc: 0.7909\n",
      "valid loss: 0.9951, acc: 0.6931\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.5796, acc: 0.8043\n",
      "valid loss: 0.9306, acc: 0.7327\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.5111, acc: 0.8315\n",
      "valid loss: 0.8786, acc: 0.7129\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.4647, acc: 0.8417\n",
      "valid loss: 1.3184, acc: 0.6667\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.4325, acc: 0.8542\n",
      "valid loss: 0.8450, acc: 0.7426\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.4094, acc: 0.8666\n",
      "valid loss: 0.8753, acc: 0.7525\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.3729, acc: 0.8769\n",
      "valid loss: 0.6942, acc: 0.7822\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.3663, acc: 0.8826\n",
      "valid loss: 0.8025, acc: 0.7624\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.3105, acc: 0.8980\n",
      "valid loss: 0.8079, acc: 0.7261\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.2709, acc: 0.9165\n",
      "valid loss: 0.7550, acc: 0.7360\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.2352, acc: 0.9280\n",
      "valid loss: 0.8438, acc: 0.7294\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.2506, acc: 0.9213\n",
      "valid loss: 0.7851, acc: 0.7525\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.2154, acc: 0.9312\n",
      "valid loss: 0.6736, acc: 0.7723\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.2152, acc: 0.9344\n",
      "valid loss: 0.8173, acc: 0.7558\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.1830, acc: 0.9472\n",
      "valid loss: 0.9271, acc: 0.7195\n",
      "Epoch 23/100\n",
      "----------\n",
      "train loss: 0.2140, acc: 0.9354\n",
      "valid loss: 0.6804, acc: 0.7954\n",
      "Epoch 24/100\n",
      "----------\n",
      "train loss: 0.1631, acc: 0.9524\n",
      "valid loss: 0.7855, acc: 0.7525\n",
      "Epoch 25/100\n",
      "----------\n",
      "train loss: 0.1490, acc: 0.9587\n",
      "valid loss: 0.7014, acc: 0.7822\n",
      "Epoch 26/100\n",
      "----------\n",
      "train loss: 0.1449, acc: 0.9575\n",
      "valid loss: 0.7454, acc: 0.7591\n",
      "Epoch 27/100\n",
      "----------\n",
      "train loss: 0.1486, acc: 0.9539\n",
      "valid loss: 0.6768, acc: 0.7723\n",
      "Epoch 28/100\n",
      "----------\n",
      "train loss: 0.1412, acc: 0.9575\n",
      "valid loss: 0.9512, acc: 0.7558\n",
      "Epoch 29/100\n",
      "----------\n",
      "train loss: 0.1455, acc: 0.9571\n",
      "valid loss: 0.9291, acc: 0.7162\n",
      "Epoch 30/100\n",
      "----------\n",
      "train loss: 0.1474, acc: 0.9578\n",
      "valid loss: 0.7766, acc: 0.7756\n",
      "Epoch 31/100\n",
      "----------\n",
      "train loss: 0.1296, acc: 0.9671\n",
      "valid loss: 0.7193, acc: 0.7822\n",
      "Epoch 32/100\n",
      "----------\n",
      "train loss: 0.1038, acc: 0.9696\n",
      "valid loss: 0.8179, acc: 0.7756\n",
      "Epoch 33/100\n",
      "----------\n",
      "train loss: 0.1054, acc: 0.9719\n",
      "valid loss: 0.8144, acc: 0.7690\n",
      "Epoch 34/100\n",
      "----------\n",
      "train loss: 0.1123, acc: 0.9712\n",
      "valid loss: 1.0417, acc: 0.7129\n",
      "Epoch 35/100\n",
      "----------\n",
      "train loss: 0.0732, acc: 0.9843\n",
      "valid loss: 0.7526, acc: 0.7987\n",
      "Epoch 36/100\n",
      "----------\n",
      "train loss: 0.1105, acc: 0.9693\n",
      "valid loss: 0.8628, acc: 0.7492\n",
      "Epoch 37/100\n",
      "----------\n",
      "train loss: 0.1003, acc: 0.9741\n",
      "valid loss: 0.9715, acc: 0.7360\n",
      "Epoch 38/100\n",
      "----------\n",
      "train loss: 0.0691, acc: 0.9834\n",
      "valid loss: 0.7467, acc: 0.7822\n",
      "Epoch 39/100\n",
      "----------\n",
      "train loss: 0.0784, acc: 0.9786\n",
      "valid loss: 0.7264, acc: 0.7954\n",
      "Epoch 40/100\n",
      "----------\n",
      "train loss: 0.0769, acc: 0.9802\n",
      "valid loss: 0.8131, acc: 0.7723\n",
      "Epoch 41/100\n",
      "----------\n",
      "train loss: 0.0624, acc: 0.9853\n",
      "valid loss: 0.8326, acc: 0.7756\n",
      "Epoch 42/100\n",
      "----------\n",
      "train loss: 0.0655, acc: 0.9831\n",
      "valid loss: 0.6842, acc: 0.8218\n",
      "Epoch 43/100\n",
      "----------\n",
      "train loss: 0.0634, acc: 0.9840\n",
      "valid loss: 0.7182, acc: 0.8086\n",
      "Epoch 44/100\n",
      "----------\n",
      "train loss: 0.0571, acc: 0.9853\n",
      "valid loss: 0.7961, acc: 0.7591\n",
      "Epoch 45/100\n",
      "----------\n",
      "train loss: 0.0799, acc: 0.9773\n",
      "valid loss: 0.7286, acc: 0.7822\n",
      "Epoch 46/100\n",
      "----------\n",
      "train loss: 0.0606, acc: 0.9859\n",
      "valid loss: 0.6916, acc: 0.8020\n",
      "Epoch 47/100\n",
      "----------\n",
      "train loss: 0.0749, acc: 0.9805\n",
      "valid loss: 0.8563, acc: 0.7789\n",
      "Epoch 48/100\n",
      "----------\n",
      "train loss: 0.0789, acc: 0.9783\n",
      "valid loss: 0.6615, acc: 0.8119\n",
      "Epoch 49/100\n",
      "----------\n",
      "train loss: 0.0680, acc: 0.9834\n",
      "valid loss: 0.7707, acc: 0.8086\n",
      "Epoch 50/100\n",
      "----------\n",
      "train loss: 0.0594, acc: 0.9843\n",
      "valid loss: 0.7037, acc: 0.8086\n",
      "Epoch 51/100\n",
      "----------\n",
      "train loss: 0.0526, acc: 0.9875\n",
      "valid loss: 0.6443, acc: 0.8251\n",
      "Epoch 52/100\n",
      "----------\n",
      "train loss: 0.0351, acc: 0.9936\n",
      "valid loss: 0.6192, acc: 0.8185\n",
      "Epoch 53/100\n",
      "----------\n",
      "train loss: 0.0632, acc: 0.9837\n",
      "valid loss: 0.8595, acc: 0.7393\n",
      "Epoch 54/100\n",
      "----------\n",
      "train loss: 0.0729, acc: 0.9821\n",
      "valid loss: 0.8672, acc: 0.7525\n",
      "Epoch 55/100\n",
      "----------\n",
      "train loss: 0.0776, acc: 0.9789\n",
      "valid loss: 0.7298, acc: 0.7888\n",
      "Epoch 56/100\n",
      "----------\n",
      "train loss: 0.0731, acc: 0.9821\n",
      "valid loss: 0.8648, acc: 0.7822\n",
      "Epoch 57/100\n",
      "----------\n",
      "train loss: 0.0697, acc: 0.9821\n",
      "valid loss: 0.7718, acc: 0.7954\n",
      "Epoch 58/100\n",
      "----------\n",
      "train loss: 0.0901, acc: 0.9719\n",
      "valid loss: 0.8179, acc: 0.7690\n",
      "Epoch 59/100\n",
      "----------\n",
      "train loss: 0.0506, acc: 0.9872\n",
      "valid loss: 0.6877, acc: 0.7855\n",
      "Epoch 60/100\n",
      "----------\n",
      "train loss: 0.0441, acc: 0.9910\n",
      "valid loss: 0.7528, acc: 0.8020\n",
      "Epoch 61/100\n",
      "----------\n",
      "train loss: 0.0309, acc: 0.9933\n",
      "valid loss: 0.6714, acc: 0.8119\n",
      "Epoch 62/100\n",
      "----------\n",
      "train loss: 0.0319, acc: 0.9936\n",
      "valid loss: 0.6288, acc: 0.8218\n",
      "Epoch 63/100\n",
      "----------\n",
      "train loss: 0.0409, acc: 0.9898\n",
      "valid loss: 0.7833, acc: 0.7921\n",
      "Epoch 64/100\n",
      "----------\n",
      "train loss: 0.0404, acc: 0.9914\n",
      "valid loss: 0.7265, acc: 0.7723\n",
      "Epoch 65/100\n",
      "----------\n",
      "train loss: 0.0459, acc: 0.9885\n",
      "valid loss: 0.7088, acc: 0.7987\n",
      "Epoch 66/100\n",
      "----------\n",
      "train loss: 0.0371, acc: 0.9926\n",
      "valid loss: 0.6525, acc: 0.8152\n",
      "Epoch 67/100\n",
      "----------\n",
      "train loss: 0.0214, acc: 0.9962\n",
      "valid loss: 0.6284, acc: 0.8251\n",
      "Epoch 68/100\n",
      "----------\n",
      "train loss: 0.0310, acc: 0.9942\n",
      "valid loss: 0.7214, acc: 0.7855\n",
      "Epoch 69/100\n",
      "----------\n",
      "train loss: 0.0225, acc: 0.9971\n",
      "valid loss: 0.6324, acc: 0.8086\n",
      "Epoch 70/100\n",
      "----------\n",
      "train loss: 0.0549, acc: 0.9843\n",
      "valid loss: 0.7150, acc: 0.7888\n",
      "Epoch 71/100\n",
      "----------\n",
      "train loss: 0.0466, acc: 0.9872\n",
      "valid loss: 0.7806, acc: 0.7855\n",
      "Epoch 72/100\n",
      "----------\n",
      "train loss: 0.0882, acc: 0.9763\n",
      "valid loss: 0.7229, acc: 0.7789\n",
      "Epoch 73/100\n",
      "----------\n",
      "train loss: 0.0792, acc: 0.9783\n",
      "valid loss: 0.7666, acc: 0.8152\n",
      "Epoch 74/100\n",
      "----------\n",
      "train loss: 0.0364, acc: 0.9920\n",
      "valid loss: 0.7438, acc: 0.8152\n",
      "Epoch 75/100\n",
      "----------\n",
      "train loss: 0.0568, acc: 0.9850\n",
      "valid loss: 0.7401, acc: 0.8053\n",
      "Epoch 76/100\n",
      "----------\n",
      "train loss: 0.0778, acc: 0.9811\n",
      "valid loss: 0.7479, acc: 0.7921\n",
      "Epoch 77/100\n",
      "----------\n",
      "train loss: 0.0496, acc: 0.9875\n",
      "valid loss: 0.6684, acc: 0.8185\n",
      "Epoch 78/100\n",
      "----------\n",
      "train loss: 0.0406, acc: 0.9898\n",
      "valid loss: 1.4660, acc: 0.6535\n",
      "Epoch 79/100\n",
      "----------\n",
      "train loss: 0.0291, acc: 0.9933\n",
      "valid loss: 0.6831, acc: 0.8086\n",
      "Epoch 80/100\n",
      "----------\n",
      "train loss: 0.0361, acc: 0.9920\n",
      "valid loss: 0.7360, acc: 0.7822\n",
      "Epoch 81/100\n",
      "----------\n",
      "train loss: 0.0274, acc: 0.9926\n",
      "valid loss: 0.6850, acc: 0.8152\n",
      "Epoch 82/100\n",
      "----------\n",
      "train loss: 0.0219, acc: 0.9958\n",
      "valid loss: 0.7407, acc: 0.8185\n",
      "Epoch 83/100\n",
      "----------\n",
      "train loss: 0.0330, acc: 0.9923\n",
      "valid loss: 0.7779, acc: 0.7921\n",
      "Epoch 84/100\n",
      "----------\n",
      "train loss: 0.0417, acc: 0.9898\n",
      "valid loss: 0.9892, acc: 0.7459\n",
      "Epoch 85/100\n",
      "----------\n",
      "train loss: 0.0446, acc: 0.9901\n",
      "valid loss: 0.8194, acc: 0.7855\n",
      "Epoch 86/100\n",
      "----------\n",
      "train loss: 0.0592, acc: 0.9834\n",
      "valid loss: 0.7489, acc: 0.7987\n",
      "Epoch 87/100\n",
      "----------\n",
      "train loss: 0.0683, acc: 0.9824\n",
      "valid loss: 0.7907, acc: 0.7690\n",
      "Epoch 88/100\n",
      "----------\n",
      "train loss: 0.0708, acc: 0.9802\n",
      "valid loss: 0.8061, acc: 0.7855\n",
      "Epoch 89/100\n",
      "----------\n",
      "train loss: 0.0482, acc: 0.9882\n",
      "valid loss: 0.8201, acc: 0.8086\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.0273, acc: 0.9939\n",
      "valid loss: 0.7993, acc: 0.7921\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.0435, acc: 0.9894\n",
      "valid loss: 0.9392, acc: 0.7393\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.0606, acc: 0.9853\n",
      "valid loss: 0.9020, acc: 0.7624\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.0525, acc: 0.9875\n",
      "valid loss: 0.7280, acc: 0.8185\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0317, acc: 0.9933\n",
      "valid loss: 0.6584, acc: 0.8152\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.1022, acc: 0.9687\n",
      "valid loss: 0.8961, acc: 0.7690\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.0521, acc: 0.9885\n",
      "valid loss: 0.8539, acc: 0.7756\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.0358, acc: 0.9923\n",
      "valid loss: 0.8199, acc: 0.8020\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.0453, acc: 0.9901\n",
      "valid loss: 0.8233, acc: 0.7888\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.0343, acc: 0.9920\n",
      "valid loss: 0.6909, acc: 0.8119\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.0797, acc: 0.9767\n",
      "valid loss: 0.8167, acc: 0.7756\n"
     ]
    }
   ],
   "source": [
    "#sgd\n",
    "#LR 0.05\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'roomclassifiermensrhso.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 0.3781, acc: 0.8794\n",
      "valid loss: 0.8820, acc: 0.7096\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 0.3253, acc: 0.8983\n",
      "valid loss: 0.8797, acc: 0.7459\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 0.2961, acc: 0.9057\n",
      "valid loss: 0.7519, acc: 0.7690\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.2726, acc: 0.9159\n",
      "valid loss: 0.7435, acc: 0.7690\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.2229, acc: 0.9332\n",
      "valid loss: 0.7686, acc: 0.7888\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.2287, acc: 0.9312\n",
      "valid loss: 0.7515, acc: 0.7822\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.1853, acc: 0.9460\n",
      "valid loss: 0.8466, acc: 0.7657\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.1689, acc: 0.9492\n",
      "valid loss: 0.7711, acc: 0.7624\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.1495, acc: 0.9578\n",
      "valid loss: 0.7601, acc: 0.7723\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.1654, acc: 0.9527\n",
      "valid loss: 0.6509, acc: 0.7954\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.1535, acc: 0.9549\n",
      "valid loss: 0.8558, acc: 0.7888\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.1583, acc: 0.9530\n",
      "valid loss: 0.7730, acc: 0.7954\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.1711, acc: 0.9482\n",
      "valid loss: 0.7998, acc: 0.7756\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.1065, acc: 0.9738\n",
      "valid loss: 0.6237, acc: 0.8284\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.1516, acc: 0.9527\n",
      "valid loss: 0.7691, acc: 0.7690\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.1249, acc: 0.9645\n",
      "valid loss: 0.7114, acc: 0.8053\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.1156, acc: 0.9667\n",
      "valid loss: 0.6510, acc: 0.8119\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.0946, acc: 0.9751\n",
      "valid loss: 0.6581, acc: 0.8152\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.0784, acc: 0.9792\n",
      "valid loss: 0.7286, acc: 0.7855\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.1252, acc: 0.9661\n",
      "valid loss: 0.7752, acc: 0.7888\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.0871, acc: 0.9811\n",
      "valid loss: 0.6765, acc: 0.8119\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.1209, acc: 0.9648\n",
      "valid loss: 0.7566, acc: 0.7756\n",
      "Epoch 23/100\n",
      "----------\n",
      "train loss: 0.0763, acc: 0.9811\n",
      "valid loss: 0.8601, acc: 0.7723\n",
      "Epoch 24/100\n",
      "----------\n",
      "train loss: 0.0865, acc: 0.9805\n",
      "valid loss: 0.6808, acc: 0.8053\n",
      "Epoch 25/100\n",
      "----------\n",
      "train loss: 0.0665, acc: 0.9846\n",
      "valid loss: 0.8302, acc: 0.7657\n",
      "Epoch 26/100\n",
      "----------\n",
      "train loss: 0.0611, acc: 0.9840\n",
      "valid loss: 0.8376, acc: 0.7921\n",
      "Epoch 27/100\n",
      "----------\n",
      "train loss: 0.0680, acc: 0.9846\n",
      "valid loss: 0.7035, acc: 0.7921\n",
      "Epoch 28/100\n",
      "----------\n",
      "train loss: 0.0817, acc: 0.9783\n",
      "valid loss: 0.6645, acc: 0.8152\n",
      "Epoch 29/100\n",
      "----------\n",
      "train loss: 0.0628, acc: 0.9859\n",
      "valid loss: 0.6883, acc: 0.8020\n",
      "Epoch 30/100\n",
      "----------\n",
      "train loss: 0.0436, acc: 0.9894\n",
      "valid loss: 0.6766, acc: 0.8152\n",
      "Epoch 31/100\n",
      "----------\n",
      "train loss: 0.0474, acc: 0.9875\n",
      "valid loss: 0.7504, acc: 0.7921\n",
      "Epoch 32/100\n",
      "----------\n",
      "train loss: 0.0687, acc: 0.9821\n",
      "valid loss: 0.7820, acc: 0.7921\n",
      "Epoch 33/100\n",
      "----------\n",
      "train loss: 0.0664, acc: 0.9834\n",
      "valid loss: 0.7603, acc: 0.7921\n",
      "Epoch 34/100\n",
      "----------\n",
      "train loss: 0.0413, acc: 0.9923\n",
      "valid loss: 0.6829, acc: 0.8020\n",
      "Epoch 35/100\n",
      "----------\n",
      "train loss: 0.0352, acc: 0.9914\n",
      "valid loss: 0.8661, acc: 0.7756\n",
      "Epoch 36/100\n",
      "----------\n",
      "train loss: 0.0473, acc: 0.9898\n",
      "valid loss: 1.1233, acc: 0.7492\n",
      "Epoch 37/100\n",
      "----------\n",
      "train loss: 0.0533, acc: 0.9850\n",
      "valid loss: 0.8743, acc: 0.7657\n",
      "Epoch 38/100\n",
      "----------\n",
      "train loss: 0.0512, acc: 0.9875\n",
      "valid loss: 0.9750, acc: 0.7261\n",
      "Epoch 39/100\n",
      "----------\n",
      "train loss: 0.0619, acc: 0.9827\n",
      "valid loss: 0.6702, acc: 0.8284\n",
      "Epoch 40/100\n",
      "----------\n",
      "train loss: 0.0552, acc: 0.9846\n",
      "valid loss: 0.6964, acc: 0.8119\n",
      "Epoch 41/100\n",
      "----------\n",
      "train loss: 0.0753, acc: 0.9779\n",
      "valid loss: 0.7825, acc: 0.7789\n",
      "Epoch 42/100\n",
      "----------\n",
      "train loss: 0.0829, acc: 0.9767\n",
      "valid loss: 0.6891, acc: 0.8086\n",
      "Epoch 43/100\n",
      "----------\n",
      "train loss: 0.0537, acc: 0.9878\n",
      "valid loss: 0.7712, acc: 0.7756\n",
      "Epoch 44/100\n",
      "----------\n",
      "train loss: 0.0752, acc: 0.9824\n",
      "valid loss: 0.7007, acc: 0.8119\n",
      "Epoch 45/100\n",
      "----------\n",
      "train loss: 0.0463, acc: 0.9898\n",
      "valid loss: 0.7728, acc: 0.8053\n",
      "Epoch 46/100\n",
      "----------\n",
      "train loss: 0.0444, acc: 0.9910\n",
      "valid loss: 0.7918, acc: 0.7987\n",
      "Epoch 47/100\n",
      "----------\n",
      "train loss: 0.0636, acc: 0.9831\n",
      "valid loss: 0.8537, acc: 0.7723\n",
      "Epoch 48/100\n",
      "----------\n",
      "train loss: 0.0368, acc: 0.9923\n",
      "valid loss: 0.7220, acc: 0.8020\n",
      "Epoch 49/100\n",
      "----------\n",
      "train loss: 0.0573, acc: 0.9869\n",
      "valid loss: 0.7246, acc: 0.8119\n",
      "Epoch 50/100\n",
      "----------\n",
      "train loss: 0.0442, acc: 0.9894\n",
      "valid loss: 0.7130, acc: 0.8020\n",
      "Epoch 51/100\n",
      "----------\n",
      "train loss: 0.0358, acc: 0.9904\n",
      "valid loss: 0.6899, acc: 0.8251\n",
      "Epoch 52/100\n",
      "----------\n",
      "train loss: 0.0334, acc: 0.9914\n",
      "valid loss: 0.6933, acc: 0.8119\n",
      "Epoch 53/100\n",
      "----------\n",
      "train loss: 0.0328, acc: 0.9910\n",
      "valid loss: 0.8648, acc: 0.7855\n",
      "Epoch 54/100\n",
      "----------\n",
      "train loss: 0.0448, acc: 0.9907\n",
      "valid loss: 0.8093, acc: 0.7855\n",
      "Epoch 55/100\n",
      "----------\n",
      "train loss: 0.0361, acc: 0.9923\n",
      "valid loss: 0.6059, acc: 0.8383\n",
      "Epoch 56/100\n",
      "----------\n",
      "train loss: 0.0242, acc: 0.9962\n",
      "valid loss: 0.7353, acc: 0.8152\n",
      "Epoch 57/100\n",
      "----------\n",
      "train loss: 0.0216, acc: 0.9962\n",
      "valid loss: 0.7123, acc: 0.8284\n",
      "Epoch 58/100\n",
      "----------\n",
      "train loss: 0.0285, acc: 0.9955\n",
      "valid loss: 0.7577, acc: 0.8119\n",
      "Epoch 59/100\n",
      "----------\n",
      "train loss: 0.0334, acc: 0.9917\n",
      "valid loss: 0.7476, acc: 0.8086\n",
      "Epoch 60/100\n",
      "----------\n",
      "train loss: 0.0284, acc: 0.9939\n",
      "valid loss: 0.7121, acc: 0.8152\n",
      "Epoch 61/100\n",
      "----------\n",
      "train loss: 0.0490, acc: 0.9894\n",
      "valid loss: 0.7951, acc: 0.8251\n",
      "Epoch 62/100\n",
      "----------\n",
      "train loss: 0.0207, acc: 0.9974\n",
      "valid loss: 0.7401, acc: 0.7921\n",
      "Epoch 63/100\n",
      "----------\n",
      "train loss: 0.0187, acc: 0.9974\n",
      "valid loss: 0.6592, acc: 0.8350\n",
      "Epoch 64/100\n",
      "----------\n",
      "train loss: 0.0246, acc: 0.9942\n",
      "valid loss: 0.7708, acc: 0.8218\n",
      "Epoch 65/100\n",
      "----------\n",
      "train loss: 0.0254, acc: 0.9946\n",
      "valid loss: 0.7308, acc: 0.8119\n",
      "Epoch 66/100\n",
      "----------\n",
      "train loss: 0.0247, acc: 0.9949\n",
      "valid loss: 0.7238, acc: 0.8284\n",
      "Epoch 67/100\n",
      "----------\n",
      "train loss: 0.0232, acc: 0.9946\n",
      "valid loss: 0.7821, acc: 0.8284\n",
      "Epoch 68/100\n",
      "----------\n",
      "train loss: 0.0227, acc: 0.9942\n",
      "valid loss: 0.7616, acc: 0.7954\n",
      "Epoch 69/100\n",
      "----------\n",
      "train loss: 0.0150, acc: 0.9978\n",
      "valid loss: 0.7017, acc: 0.8251\n",
      "Epoch 70/100\n",
      "----------\n",
      "train loss: 0.0178, acc: 0.9971\n",
      "valid loss: 0.6606, acc: 0.8086\n",
      "Epoch 71/100\n",
      "----------\n",
      "train loss: 0.0118, acc: 0.9987\n",
      "valid loss: 0.7194, acc: 0.8185\n",
      "Epoch 72/100\n",
      "----------\n",
      "train loss: 0.0141, acc: 0.9981\n",
      "valid loss: 0.7444, acc: 0.8152\n",
      "Epoch 73/100\n",
      "----------\n",
      "train loss: 0.0240, acc: 0.9952\n",
      "valid loss: 0.7306, acc: 0.8053\n",
      "Epoch 74/100\n",
      "----------\n",
      "train loss: 0.0300, acc: 0.9920\n",
      "valid loss: 0.8507, acc: 0.7855\n",
      "Epoch 75/100\n",
      "----------\n",
      "train loss: 0.0593, acc: 0.9840\n",
      "valid loss: 0.8527, acc: 0.7723\n",
      "Epoch 76/100\n",
      "----------\n",
      "train loss: 0.0467, acc: 0.9901\n",
      "valid loss: 0.7299, acc: 0.8119\n",
      "Epoch 77/100\n",
      "----------\n",
      "train loss: 0.0334, acc: 0.9933\n",
      "valid loss: 0.9263, acc: 0.7525\n",
      "Epoch 78/100\n",
      "----------\n",
      "train loss: 0.0535, acc: 0.9843\n",
      "valid loss: 0.8073, acc: 0.7789\n",
      "Epoch 79/100\n",
      "----------\n",
      "train loss: 0.0713, acc: 0.9837\n",
      "valid loss: 0.7953, acc: 0.7690\n",
      "Epoch 80/100\n",
      "----------\n",
      "train loss: 0.0220, acc: 0.9962\n",
      "valid loss: 0.8026, acc: 0.7921\n",
      "Epoch 81/100\n",
      "----------\n",
      "train loss: 0.0315, acc: 0.9930\n",
      "valid loss: 0.8421, acc: 0.8086\n",
      "Epoch 82/100\n",
      "----------\n",
      "train loss: 0.0312, acc: 0.9917\n",
      "valid loss: 0.9742, acc: 0.7723\n",
      "Epoch 83/100\n",
      "----------\n",
      "train loss: 0.0208, acc: 0.9962\n",
      "valid loss: 0.8202, acc: 0.7954\n",
      "Epoch 84/100\n",
      "----------\n",
      "train loss: 0.0142, acc: 0.9984\n",
      "valid loss: 0.7910, acc: 0.7921\n",
      "Epoch 85/100\n",
      "----------\n",
      "train loss: 0.0147, acc: 0.9978\n",
      "valid loss: 0.8735, acc: 0.7723\n",
      "Epoch 86/100\n",
      "----------\n",
      "train loss: 0.0517, acc: 0.9878\n",
      "valid loss: 0.7106, acc: 0.8119\n",
      "Epoch 87/100\n",
      "----------\n",
      "train loss: 0.0356, acc: 0.9917\n",
      "valid loss: 0.9316, acc: 0.7624\n",
      "Epoch 88/100\n",
      "----------\n",
      "train loss: 0.0224, acc: 0.9952\n",
      "valid loss: 0.7470, acc: 0.7987\n",
      "Epoch 89/100\n",
      "----------\n",
      "train loss: 0.0246, acc: 0.9946\n",
      "valid loss: 0.7675, acc: 0.8053\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.0678, acc: 0.9805\n",
      "valid loss: 0.8966, acc: 0.7822\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.0747, acc: 0.9792\n",
      "valid loss: 0.8095, acc: 0.8053\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.0769, acc: 0.9767\n",
      "valid loss: 0.9197, acc: 0.7657\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.0903, acc: 0.9738\n",
      "valid loss: 0.9126, acc: 0.7558\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0484, acc: 0.9888\n",
      "valid loss: 0.7639, acc: 0.7855\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.0413, acc: 0.9898\n",
      "valid loss: 0.7888, acc: 0.7855\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.0556, acc: 0.9837\n",
      "valid loss: 0.8864, acc: 0.7921\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.0554, acc: 0.9856\n",
      "valid loss: 0.8684, acc: 0.7789\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.0536, acc: 0.9866\n",
      "valid loss: 0.7853, acc: 0.7855\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.0494, acc: 0.9866\n",
      "valid loss: 0.9417, acc: 0.7459\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.0430, acc: 0.9901\n",
      "valid loss: 0.7735, acc: 0.7855\n"
     ]
    }
   ],
   "source": [
    "#sgd\n",
    "#LR 0.04\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'roomclassifiermensrhsomtbz5bf8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 0.2548, acc: 0.9178\n",
      "valid loss: 0.7797, acc: 0.7921\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 0.2513, acc: 0.9175\n",
      "valid loss: 0.6481, acc: 0.7789\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 0.2301, acc: 0.9322\n",
      "valid loss: 0.6523, acc: 0.7690\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.2266, acc: 0.9300\n",
      "valid loss: 0.9113, acc: 0.7393\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.1915, acc: 0.9450\n",
      "valid loss: 0.7971, acc: 0.7690\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.1751, acc: 0.9472\n",
      "valid loss: 0.6945, acc: 0.7954\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.1511, acc: 0.9581\n",
      "valid loss: 0.6829, acc: 0.7987\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.1411, acc: 0.9632\n",
      "valid loss: 0.8980, acc: 0.7492\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.1414, acc: 0.9591\n",
      "valid loss: 0.9169, acc: 0.7426\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.1517, acc: 0.9584\n",
      "valid loss: 0.6927, acc: 0.8086\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.0888, acc: 0.9776\n",
      "valid loss: 0.6798, acc: 0.8086\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.1004, acc: 0.9744\n",
      "valid loss: 0.6864, acc: 0.7921\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.0920, acc: 0.9760\n",
      "valid loss: 0.7220, acc: 0.8218\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.0980, acc: 0.9744\n",
      "valid loss: 0.6552, acc: 0.8053\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.0634, acc: 0.9878\n",
      "valid loss: 0.6142, acc: 0.8119\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.0769, acc: 0.9824\n",
      "valid loss: 0.8023, acc: 0.7657\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.0583, acc: 0.9875\n",
      "valid loss: 0.6416, acc: 0.8251\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.0523, acc: 0.9872\n",
      "valid loss: 0.7357, acc: 0.7921\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.0549, acc: 0.9869\n",
      "valid loss: 0.6904, acc: 0.7987\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.0568, acc: 0.9856\n",
      "valid loss: 0.6031, acc: 0.8152\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.0562, acc: 0.9862\n",
      "valid loss: 0.6326, acc: 0.8152\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.0255, acc: 0.9984\n",
      "valid loss: 0.6512, acc: 0.8251\n",
      "Epoch 23/100\n",
      "----------\n",
      "train loss: 0.0486, acc: 0.9891\n",
      "valid loss: 0.6949, acc: 0.8152\n",
      "Epoch 24/100\n",
      "----------\n",
      "train loss: 0.0773, acc: 0.9789\n",
      "valid loss: 0.7882, acc: 0.8119\n",
      "Epoch 25/100\n",
      "----------\n",
      "train loss: 0.0478, acc: 0.9894\n",
      "valid loss: 0.6777, acc: 0.8185\n",
      "Epoch 26/100\n",
      "----------\n",
      "train loss: 0.0620, acc: 0.9846\n",
      "valid loss: 0.7392, acc: 0.7921\n",
      "Epoch 27/100\n",
      "----------\n",
      "train loss: 0.0572, acc: 0.9872\n",
      "valid loss: 0.7586, acc: 0.7987\n",
      "Epoch 28/100\n",
      "----------\n",
      "train loss: 0.0409, acc: 0.9894\n",
      "valid loss: 0.7946, acc: 0.8020\n",
      "Epoch 29/100\n",
      "----------\n",
      "train loss: 0.0526, acc: 0.9885\n",
      "valid loss: 0.7510, acc: 0.8020\n",
      "Epoch 30/100\n",
      "----------\n",
      "train loss: 0.0429, acc: 0.9904\n",
      "valid loss: 0.8598, acc: 0.7789\n",
      "Epoch 31/100\n",
      "----------\n",
      "train loss: 0.0589, acc: 0.9866\n",
      "valid loss: 0.7981, acc: 0.7723\n",
      "Epoch 32/100\n",
      "----------\n",
      "train loss: 0.0607, acc: 0.9843\n",
      "valid loss: 0.8492, acc: 0.7756\n",
      "Epoch 33/100\n",
      "----------\n",
      "train loss: 0.0414, acc: 0.9910\n",
      "valid loss: 0.8235, acc: 0.7690\n",
      "Epoch 34/100\n",
      "----------\n",
      "train loss: 0.0419, acc: 0.9917\n",
      "valid loss: 0.6707, acc: 0.8284\n",
      "Epoch 35/100\n",
      "----------\n",
      "train loss: 0.0308, acc: 0.9936\n",
      "valid loss: 0.6990, acc: 0.8152\n",
      "Epoch 36/100\n",
      "----------\n",
      "train loss: 0.0345, acc: 0.9917\n",
      "valid loss: 0.7206, acc: 0.7789\n",
      "Epoch 37/100\n",
      "----------\n",
      "train loss: 0.0507, acc: 0.9875\n",
      "valid loss: 0.8187, acc: 0.7756\n",
      "Epoch 38/100\n",
      "----------\n",
      "train loss: 0.0432, acc: 0.9888\n",
      "valid loss: 0.7176, acc: 0.7921\n",
      "Epoch 39/100\n",
      "----------\n",
      "train loss: 0.0480, acc: 0.9875\n",
      "valid loss: 0.7321, acc: 0.7987\n",
      "Epoch 40/100\n",
      "----------\n",
      "train loss: 0.0395, acc: 0.9920\n",
      "valid loss: 0.6691, acc: 0.8251\n",
      "Epoch 41/100\n",
      "----------\n",
      "train loss: 0.0331, acc: 0.9930\n",
      "valid loss: 0.6912, acc: 0.8053\n",
      "Epoch 42/100\n",
      "----------\n",
      "train loss: 0.0302, acc: 0.9926\n",
      "valid loss: 0.7134, acc: 0.8053\n",
      "Epoch 43/100\n",
      "----------\n",
      "train loss: 0.0427, acc: 0.9878\n",
      "valid loss: 0.6915, acc: 0.8317\n",
      "Epoch 44/100\n",
      "----------\n",
      "train loss: 0.0343, acc: 0.9926\n",
      "valid loss: 0.8186, acc: 0.7888\n",
      "Epoch 45/100\n",
      "----------\n",
      "train loss: 0.0296, acc: 0.9952\n",
      "valid loss: 0.7228, acc: 0.7954\n",
      "Epoch 46/100\n",
      "----------\n",
      "train loss: 0.0496, acc: 0.9875\n",
      "valid loss: 0.7702, acc: 0.8086\n",
      "Epoch 47/100\n",
      "----------\n",
      "train loss: 0.0586, acc: 0.9831\n",
      "valid loss: 0.6537, acc: 0.8218\n",
      "Epoch 48/100\n",
      "----------\n",
      "train loss: 0.0360, acc: 0.9907\n",
      "valid loss: 0.6593, acc: 0.8218\n",
      "Epoch 49/100\n",
      "----------\n",
      "train loss: 0.0459, acc: 0.9891\n",
      "valid loss: 0.6497, acc: 0.8317\n",
      "Epoch 50/100\n",
      "----------\n",
      "train loss: 0.0359, acc: 0.9920\n",
      "valid loss: 0.8219, acc: 0.7822\n",
      "Epoch 51/100\n",
      "----------\n",
      "train loss: 0.0271, acc: 0.9952\n",
      "valid loss: 0.7188, acc: 0.7954\n",
      "Epoch 52/100\n",
      "----------\n",
      "train loss: 0.0325, acc: 0.9917\n",
      "valid loss: 0.7542, acc: 0.7921\n",
      "Epoch 53/100\n",
      "----------\n",
      "train loss: 0.0236, acc: 0.9955\n",
      "valid loss: 0.7418, acc: 0.8218\n",
      "Epoch 54/100\n",
      "----------\n",
      "train loss: 0.0389, acc: 0.9891\n",
      "valid loss: 0.8043, acc: 0.7855\n",
      "Epoch 55/100\n",
      "----------\n",
      "train loss: 0.0293, acc: 0.9942\n",
      "valid loss: 0.7084, acc: 0.8218\n",
      "Epoch 56/100\n",
      "----------\n",
      "train loss: 0.0470, acc: 0.9882\n",
      "valid loss: 0.7709, acc: 0.7921\n",
      "Epoch 57/100\n",
      "----------\n",
      "train loss: 0.0454, acc: 0.9894\n",
      "valid loss: 0.7201, acc: 0.8152\n",
      "Epoch 58/100\n",
      "----------\n",
      "train loss: 0.0202, acc: 0.9974\n",
      "valid loss: 0.6974, acc: 0.7954\n",
      "Epoch 59/100\n",
      "----------\n",
      "train loss: 0.0207, acc: 0.9968\n",
      "valid loss: 0.7940, acc: 0.8053\n",
      "Epoch 60/100\n",
      "----------\n",
      "train loss: 0.0264, acc: 0.9942\n",
      "valid loss: 0.6951, acc: 0.8053\n",
      "Epoch 61/100\n",
      "----------\n",
      "train loss: 0.0223, acc: 0.9971\n",
      "valid loss: 0.7442, acc: 0.8086\n",
      "Epoch 62/100\n",
      "----------\n",
      "train loss: 0.0227, acc: 0.9958\n",
      "valid loss: 0.7312, acc: 0.7888\n",
      "Epoch 63/100\n",
      "----------\n",
      "train loss: 0.0194, acc: 0.9974\n",
      "valid loss: 0.8784, acc: 0.7888\n",
      "Epoch 64/100\n",
      "----------\n",
      "train loss: 0.0482, acc: 0.9885\n",
      "valid loss: 0.8413, acc: 0.7954\n",
      "Epoch 65/100\n",
      "----------\n",
      "train loss: 0.0380, acc: 0.9891\n",
      "valid loss: 0.8205, acc: 0.7789\n",
      "Epoch 66/100\n",
      "----------\n",
      "train loss: 0.0288, acc: 0.9942\n",
      "valid loss: 0.8461, acc: 0.7723\n",
      "Epoch 67/100\n",
      "----------\n",
      "train loss: 0.0212, acc: 0.9958\n",
      "valid loss: 0.7618, acc: 0.8053\n",
      "Epoch 68/100\n",
      "----------\n",
      "train loss: 0.0289, acc: 0.9942\n",
      "valid loss: 0.9169, acc: 0.7954\n",
      "Epoch 69/100\n",
      "----------\n",
      "train loss: 0.0461, acc: 0.9891\n",
      "valid loss: 0.7040, acc: 0.8020\n",
      "Epoch 70/100\n",
      "----------\n",
      "train loss: 0.0271, acc: 0.9949\n",
      "valid loss: 0.6270, acc: 0.8218\n",
      "Epoch 71/100\n",
      "----------\n",
      "train loss: 0.0172, acc: 0.9965\n",
      "valid loss: 0.6942, acc: 0.7987\n",
      "Epoch 72/100\n",
      "----------\n",
      "train loss: 0.0342, acc: 0.9920\n",
      "valid loss: 0.8033, acc: 0.7921\n",
      "Epoch 73/100\n",
      "----------\n",
      "train loss: 0.0255, acc: 0.9958\n",
      "valid loss: 0.7131, acc: 0.7987\n",
      "Epoch 74/100\n",
      "----------\n",
      "train loss: 0.0100, acc: 1.0000\n",
      "valid loss: 0.7241, acc: 0.8185\n",
      "Epoch 75/100\n",
      "----------\n",
      "train loss: 0.0140, acc: 0.9981\n",
      "valid loss: 0.7452, acc: 0.7954\n",
      "Epoch 76/100\n",
      "----------\n",
      "train loss: 0.0171, acc: 0.9965\n",
      "valid loss: 0.7750, acc: 0.7954\n",
      "Epoch 77/100\n",
      "----------\n",
      "train loss: 0.0228, acc: 0.9955\n",
      "valid loss: 0.7577, acc: 0.8152\n",
      "Epoch 78/100\n",
      "----------\n",
      "train loss: 0.0295, acc: 0.9952\n",
      "valid loss: 0.7776, acc: 0.8119\n",
      "Epoch 79/100\n",
      "----------\n",
      "train loss: 0.0302, acc: 0.9930\n",
      "valid loss: 0.7192, acc: 0.7855\n",
      "Epoch 80/100\n",
      "----------\n",
      "train loss: 0.0285, acc: 0.9936\n",
      "valid loss: 0.7778, acc: 0.8053\n",
      "Epoch 81/100\n",
      "----------\n",
      "train loss: 0.0311, acc: 0.9917\n",
      "valid loss: 0.7644, acc: 0.7954\n",
      "Epoch 82/100\n",
      "----------\n",
      "train loss: 0.0623, acc: 0.9843\n",
      "valid loss: 0.7565, acc: 0.7954\n",
      "Epoch 83/100\n",
      "----------\n",
      "train loss: 0.0570, acc: 0.9834\n",
      "valid loss: 0.8540, acc: 0.7822\n",
      "Epoch 84/100\n",
      "----------\n",
      "train loss: 0.0352, acc: 0.9939\n",
      "valid loss: 0.7091, acc: 0.7987\n",
      "Epoch 85/100\n",
      "----------\n",
      "train loss: 0.0192, acc: 0.9987\n",
      "valid loss: 0.7752, acc: 0.7921\n",
      "Epoch 86/100\n",
      "----------\n",
      "train loss: 0.0217, acc: 0.9965\n",
      "valid loss: 0.7685, acc: 0.7987\n",
      "Epoch 87/100\n",
      "----------\n",
      "train loss: 0.0173, acc: 0.9978\n",
      "valid loss: 0.7050, acc: 0.8284\n",
      "Epoch 88/100\n",
      "----------\n",
      "train loss: 0.0239, acc: 0.9942\n",
      "valid loss: 0.8113, acc: 0.7888\n",
      "Epoch 89/100\n",
      "----------\n",
      "train loss: 0.0445, acc: 0.9910\n",
      "valid loss: 0.7525, acc: 0.7921\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.0215, acc: 0.9971\n",
      "valid loss: 0.7507, acc: 0.8152\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.0319, acc: 0.9920\n",
      "valid loss: 0.8964, acc: 0.7822\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.0323, acc: 0.9936\n",
      "valid loss: 0.7958, acc: 0.7987\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.0411, acc: 0.9894\n",
      "valid loss: 0.7977, acc: 0.7789\n",
      "Epoch 94/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0200, acc: 0.9958\n",
      "valid loss: 0.7896, acc: 0.8053\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.0104, acc: 0.9994\n",
      "valid loss: 0.8058, acc: 0.7855\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.0072, acc: 0.9997\n",
      "valid loss: 0.7585, acc: 0.8185\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.0070, acc: 0.9997\n",
      "valid loss: 0.7587, acc: 0.8152\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.0139, acc: 0.9978\n",
      "valid loss: 0.7316, acc: 0.8251\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.0076, acc: 0.9994\n",
      "valid loss: 0.6956, acc: 0.8119\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.0069, acc: 0.9997\n",
      "valid loss: 0.7355, acc: 0.8086\n"
     ]
    }
   ],
   "source": [
    "#sgd\n",
    "#lr 0.03\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained,'roomclassifierfn1xqkbh.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train loss: 1.6578, acc: 0.4317\n",
      "valid loss: 1.0617, acc: 0.6700\n",
      "Epoch 2/100\n",
      "----------\n",
      "train loss: 1.0345, acc: 0.6613\n",
      "valid loss: 0.7347, acc: 0.7690\n",
      "Epoch 3/100\n",
      "----------\n",
      "train loss: 0.7628, acc: 0.7627\n",
      "valid loss: 0.6145, acc: 0.8020\n",
      "Epoch 4/100\n",
      "----------\n",
      "train loss: 0.6067, acc: 0.8200\n",
      "valid loss: 0.5212, acc: 0.8515\n",
      "Epoch 5/100\n",
      "----------\n",
      "train loss: 0.5186, acc: 0.8443\n",
      "valid loss: 0.5112, acc: 0.8383\n",
      "Epoch 6/100\n",
      "----------\n",
      "train loss: 0.4283, acc: 0.8791\n",
      "valid loss: 0.5206, acc: 0.8251\n",
      "Epoch 7/100\n",
      "----------\n",
      "train loss: 0.3437, acc: 0.9015\n",
      "valid loss: 0.5123, acc: 0.8350\n",
      "Epoch 8/100\n",
      "----------\n",
      "train loss: 0.2877, acc: 0.9175\n",
      "valid loss: 0.5233, acc: 0.8383\n",
      "Epoch 9/100\n",
      "----------\n",
      "train loss: 0.2508, acc: 0.9319\n",
      "valid loss: 0.5340, acc: 0.8218\n",
      "Epoch 10/100\n",
      "----------\n",
      "train loss: 0.2035, acc: 0.9536\n",
      "valid loss: 0.5167, acc: 0.8449\n",
      "Epoch 11/100\n",
      "----------\n",
      "train loss: 0.1820, acc: 0.9587\n",
      "valid loss: 0.5018, acc: 0.8350\n",
      "Epoch 12/100\n",
      "----------\n",
      "train loss: 0.1607, acc: 0.9607\n",
      "valid loss: 0.5529, acc: 0.8218\n",
      "Epoch 13/100\n",
      "----------\n",
      "train loss: 0.1513, acc: 0.9671\n",
      "valid loss: 0.5611, acc: 0.8284\n",
      "Epoch 14/100\n",
      "----------\n",
      "train loss: 0.1267, acc: 0.9738\n",
      "valid loss: 0.5270, acc: 0.8383\n",
      "Epoch 15/100\n",
      "----------\n",
      "train loss: 0.1131, acc: 0.9767\n",
      "valid loss: 0.5418, acc: 0.8251\n",
      "Epoch 16/100\n",
      "----------\n",
      "train loss: 0.1195, acc: 0.9731\n",
      "valid loss: 0.5068, acc: 0.8548\n",
      "Epoch 17/100\n",
      "----------\n",
      "train loss: 0.1086, acc: 0.9776\n",
      "valid loss: 0.5519, acc: 0.8350\n",
      "Epoch 18/100\n",
      "----------\n",
      "train loss: 0.0905, acc: 0.9815\n",
      "valid loss: 0.5464, acc: 0.8317\n",
      "Epoch 19/100\n",
      "----------\n",
      "train loss: 0.0881, acc: 0.9831\n",
      "valid loss: 0.5122, acc: 0.8515\n",
      "Epoch 20/100\n",
      "----------\n",
      "train loss: 0.0763, acc: 0.9862\n",
      "valid loss: 0.4687, acc: 0.8680\n",
      "Epoch 21/100\n",
      "----------\n",
      "train loss: 0.0766, acc: 0.9872\n",
      "valid loss: 0.5050, acc: 0.8449\n",
      "Epoch 22/100\n",
      "----------\n",
      "train loss: 0.0626, acc: 0.9891\n",
      "valid loss: 0.4659, acc: 0.8482\n",
      "Epoch 23/100\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-776ecea7f8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#lr 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-de7b3fd4dd92>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, dataloaders, scheduler, num_epochs, cycle_mult)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#sgd\n",
    "#lr 0.01\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 1.4664, acc: 0.5446\n",
      "Epoch 2/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 1.0685, acc: 0.6898\n",
      "Epoch 3/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.8681, acc: 0.7624\n",
      "Epoch 4/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.7197, acc: 0.7954\n",
      "Epoch 5/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.6285, acc: 0.8119\n",
      "Epoch 6/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5936, acc: 0.8251\n",
      "Epoch 7/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5329, acc: 0.8515\n",
      "Epoch 8/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5290, acc: 0.8515\n",
      "Epoch 9/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5503, acc: 0.8119\n",
      "Epoch 10/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5280, acc: 0.8152\n",
      "Epoch 11/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4937, acc: 0.8515\n",
      "Epoch 12/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5011, acc: 0.8185\n",
      "Epoch 13/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4808, acc: 0.8548\n",
      "Epoch 14/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4766, acc: 0.8647\n",
      "Epoch 15/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4786, acc: 0.8284\n",
      "Epoch 16/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4849, acc: 0.8251\n",
      "Epoch 17/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4937, acc: 0.8548\n",
      "Epoch 18/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5182, acc: 0.8350\n",
      "Epoch 19/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4990, acc: 0.8581\n",
      "Epoch 20/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4744, acc: 0.8581\n",
      "Epoch 21/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4879, acc: 0.8383\n",
      "Epoch 22/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5372, acc: 0.8317\n",
      "Epoch 23/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4978, acc: 0.8383\n",
      "Epoch 24/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4947, acc: 0.8350\n",
      "Epoch 25/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5125, acc: 0.8350\n",
      "Epoch 26/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5569, acc: 0.8152\n",
      "Epoch 27/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5232, acc: 0.8284\n",
      "Epoch 28/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4836, acc: 0.8416\n",
      "Epoch 29/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5252, acc: 0.8251\n",
      "Epoch 30/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5251, acc: 0.8383\n",
      "Epoch 31/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5080, acc: 0.8449\n",
      "Epoch 32/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5165, acc: 0.8416\n",
      "Epoch 33/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4993, acc: 0.8482\n",
      "Epoch 34/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4963, acc: 0.8449\n",
      "Epoch 35/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5048, acc: 0.8482\n",
      "Epoch 36/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5027, acc: 0.8350\n",
      "Epoch 37/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4858, acc: 0.8515\n",
      "Epoch 38/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5053, acc: 0.8416\n",
      "Epoch 39/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5062, acc: 0.8449\n",
      "Epoch 40/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5040, acc: 0.8482\n",
      "Epoch 41/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5073, acc: 0.8581\n",
      "Epoch 42/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5379, acc: 0.8350\n",
      "Epoch 43/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5159, acc: 0.8515\n",
      "Epoch 44/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4990, acc: 0.8449\n",
      "Epoch 45/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4948, acc: 0.8581\n",
      "Epoch 46/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5094, acc: 0.8680\n",
      "Epoch 47/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5039, acc: 0.8482\n",
      "Epoch 48/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5239, acc: 0.8515\n",
      "Epoch 49/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5161, acc: 0.8383\n",
      "Epoch 50/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5072, acc: 0.8515\n",
      "Epoch 51/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5054, acc: 0.8515\n",
      "Epoch 52/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4811, acc: 0.8680\n",
      "Epoch 53/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5274, acc: 0.8383\n",
      "Epoch 54/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5073, acc: 0.8482\n",
      "Epoch 55/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4866, acc: 0.8548\n",
      "Epoch 56/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4836, acc: 0.8548\n",
      "Epoch 57/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5108, acc: 0.8515\n",
      "Epoch 58/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4901, acc: 0.8614\n",
      "Epoch 59/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4794, acc: 0.8515\n",
      "Epoch 60/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5127, acc: 0.8482\n",
      "Epoch 61/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5189, acc: 0.8350\n",
      "Epoch 62/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5211, acc: 0.8548\n",
      "Epoch 63/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5226, acc: 0.8449\n",
      "Epoch 64/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5026, acc: 0.8515\n",
      "Epoch 65/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5091, acc: 0.8647\n",
      "Epoch 66/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4915, acc: 0.8515\n",
      "Epoch 67/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5068, acc: 0.8515\n",
      "Epoch 68/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4677, acc: 0.8680\n",
      "Epoch 69/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5352, acc: 0.8383\n",
      "Epoch 70/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4930, acc: 0.8449\n",
      "Epoch 71/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4798, acc: 0.8515\n",
      "Epoch 72/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5007, acc: 0.8383\n",
      "Epoch 73/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.4763, acc: 0.8581\n",
      "Epoch 74/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5085, acc: 0.8515\n",
      "Epoch 75/75\n",
      "----------\n",
      "steps: 0 lr: 0.00000000\n",
      "valid loss: 0.5062, acc: 0.8614\n"
     ]
    }
   ],
   "source": [
    "#AdamW -> Run this one\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=exp_lr_scheduler, num_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([64, 3, 7, 7]) and whose dimensions in the checkpoint are torch.Size([64, 3, 7, 7]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5133f6dc028d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testsave.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([64, 3, 7, 7]) and whose dimensions in the checkpoint are torch.Size([64, 3, 7, 7])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('testsave.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained, 'RoomClassifier86.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "valid loss: 1.1300, acc: 0.6238\n",
      "Epoch 2/100\n",
      "----------\n",
      "valid loss: 0.7879, acc: 0.7492\n",
      "Epoch 3/100\n",
      "----------\n",
      "valid loss: 0.6239, acc: 0.8152\n",
      "Epoch 4/100\n",
      "----------\n",
      "valid loss: 0.5654, acc: 0.8350\n",
      "Epoch 5/100\n",
      "----------\n",
      "valid loss: 0.5359, acc: 0.8251\n",
      "Epoch 6/100\n",
      "----------\n",
      "valid loss: 0.4949, acc: 0.8350\n",
      "Epoch 7/100\n",
      "----------\n",
      "valid loss: 0.5121, acc: 0.8251\n",
      "Epoch 8/100\n",
      "----------\n",
      "valid loss: 0.5022, acc: 0.8251\n",
      "Epoch 9/100\n",
      "----------\n",
      "valid loss: 0.5163, acc: 0.8251\n",
      "Epoch 10/100\n",
      "----------\n",
      "valid loss: 0.5242, acc: 0.8284\n",
      "Epoch 11/100\n",
      "----------\n",
      "valid loss: 0.5182, acc: 0.8515\n",
      "Epoch 12/100\n",
      "----------\n",
      "valid loss: 0.5318, acc: 0.8548\n",
      "Epoch 13/100\n",
      "----------\n",
      "valid loss: 0.5297, acc: 0.8548\n",
      "Epoch 14/100\n",
      "----------\n",
      "valid loss: 0.5249, acc: 0.8416\n",
      "Epoch 15/100\n",
      "----------\n",
      "valid loss: 0.5581, acc: 0.8350\n",
      "Epoch 16/100\n",
      "----------\n",
      "valid loss: 0.5545, acc: 0.8449\n",
      "Epoch 17/100\n",
      "----------\n",
      "valid loss: 0.5056, acc: 0.8614\n",
      "Epoch 18/100\n",
      "----------\n",
      "valid loss: 0.5081, acc: 0.8581\n",
      "Epoch 19/100\n",
      "----------\n",
      "valid loss: 0.5032, acc: 0.8647\n",
      "Epoch 20/100\n",
      "----------\n",
      "valid loss: 0.4986, acc: 0.8614\n",
      "Epoch 21/100\n",
      "----------\n",
      "valid loss: 0.5260, acc: 0.8614\n",
      "Epoch 22/100\n",
      "----------\n",
      "valid loss: 0.5407, acc: 0.8416\n",
      "Epoch 23/100\n",
      "----------\n",
      "valid loss: 0.5751, acc: 0.8251\n",
      "Epoch 24/100\n",
      "----------\n",
      "valid loss: 0.5247, acc: 0.8581\n",
      "Epoch 25/100\n",
      "----------\n",
      "valid loss: 0.5282, acc: 0.8383\n",
      "Epoch 26/100\n",
      "----------\n",
      "valid loss: 0.5217, acc: 0.8416\n",
      "Epoch 27/100\n",
      "----------\n",
      "valid loss: 0.5328, acc: 0.8416\n",
      "Epoch 28/100\n",
      "----------\n",
      "valid loss: 0.5390, acc: 0.8449\n",
      "Epoch 29/100\n",
      "----------\n",
      "valid loss: 0.5385, acc: 0.8416\n",
      "Epoch 30/100\n",
      "----------\n",
      "valid loss: 0.5280, acc: 0.8548\n",
      "Epoch 31/100\n",
      "----------\n",
      "valid loss: 0.5319, acc: 0.8416\n",
      "Epoch 32/100\n",
      "----------\n",
      "valid loss: 0.5658, acc: 0.8251\n",
      "Epoch 33/100\n",
      "----------\n",
      "valid loss: 0.5806, acc: 0.8383\n",
      "Epoch 34/100\n",
      "----------\n",
      "valid loss: 0.5766, acc: 0.8218\n",
      "Epoch 35/100\n",
      "----------\n",
      "valid loss: 0.5945, acc: 0.8119\n",
      "Epoch 36/100\n",
      "----------\n",
      "valid loss: 0.5628, acc: 0.8449\n",
      "Epoch 37/100\n",
      "----------\n",
      "valid loss: 0.5214, acc: 0.8548\n",
      "Epoch 38/100\n",
      "----------\n",
      "valid loss: 0.5565, acc: 0.8416\n",
      "Epoch 39/100\n",
      "----------\n",
      "valid loss: 0.5566, acc: 0.8284\n",
      "Epoch 40/100\n",
      "----------\n",
      "valid loss: 0.5845, acc: 0.8548\n",
      "Epoch 41/100\n",
      "----------\n",
      "valid loss: 0.5686, acc: 0.8449\n",
      "Epoch 42/100\n",
      "----------\n",
      "valid loss: 0.5696, acc: 0.8482\n",
      "Epoch 43/100\n",
      "----------\n",
      "valid loss: 0.5697, acc: 0.8449\n",
      "Epoch 44/100\n",
      "----------\n",
      "valid loss: 0.5463, acc: 0.8317\n",
      "Epoch 45/100\n",
      "----------\n",
      "valid loss: 0.5782, acc: 0.8350\n",
      "Epoch 46/100\n",
      "----------\n",
      "valid loss: 0.6097, acc: 0.8218\n",
      "Epoch 47/100\n",
      "----------\n",
      "valid loss: 0.5797, acc: 0.8416\n",
      "Epoch 48/100\n",
      "----------\n",
      "valid loss: 0.5906, acc: 0.8350\n",
      "Epoch 49/100\n",
      "----------\n",
      "valid loss: 0.5767, acc: 0.8350\n",
      "Epoch 50/100\n",
      "----------\n",
      "valid loss: 0.6058, acc: 0.8383\n",
      "Epoch 51/100\n",
      "----------\n",
      "valid loss: 0.5818, acc: 0.8581\n",
      "Epoch 52/100\n",
      "----------\n",
      "valid loss: 0.6061, acc: 0.8449\n",
      "Epoch 53/100\n",
      "----------\n",
      "valid loss: 0.5764, acc: 0.8350\n",
      "Epoch 54/100\n",
      "----------\n",
      "valid loss: 0.5923, acc: 0.8449\n",
      "Epoch 55/100\n",
      "----------\n",
      "valid loss: 0.6207, acc: 0.8251\n",
      "Epoch 56/100\n",
      "----------\n",
      "valid loss: 0.5993, acc: 0.8317\n",
      "Epoch 57/100\n",
      "----------\n",
      "valid loss: 0.6083, acc: 0.8317\n",
      "Epoch 58/100\n",
      "----------\n",
      "valid loss: 0.5873, acc: 0.8284\n",
      "Epoch 59/100\n",
      "----------\n",
      "valid loss: 0.5820, acc: 0.8383\n",
      "Epoch 60/100\n",
      "----------\n",
      "valid loss: 0.6014, acc: 0.8317\n",
      "Epoch 61/100\n",
      "----------\n",
      "valid loss: 0.5754, acc: 0.8350\n",
      "Epoch 62/100\n",
      "----------\n",
      "valid loss: 0.5842, acc: 0.8416\n",
      "Epoch 63/100\n",
      "----------\n",
      "valid loss: 0.5793, acc: 0.8317\n",
      "Epoch 64/100\n",
      "----------\n",
      "valid loss: 0.5826, acc: 0.8251\n",
      "Epoch 65/100\n",
      "----------\n",
      "valid loss: 0.5975, acc: 0.8416\n",
      "Epoch 66/100\n",
      "----------\n",
      "valid loss: 0.6152, acc: 0.8416\n",
      "Epoch 67/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests_with_retry encountered retryable exception: ('Connection aborted.', OSError(\"(104, 'ECONNRESET')\",)). args: ('https://api.wandb.ai/homedepot/homedepot/te64cyns/file_stream',), kwargs: {'json': {'files': {'output.log': {'offset': 192, 'content': ['2019-03-11T19:55:07.203346 valid loss: 0.5975, acc: 0.8416\\n', '2019-03-11T19:55:07.203542 Epoch 66/100\\n', '2019-03-11T19:55:07.203601 ----------\\n', '2019-03-11T19:55:29.706986 valid loss: 0.6152, acc: 0.8416\\n', '2019-03-11T19:55:29.707346 Epoch 67/100\\n', '2019-03-11T19:55:29.707435 ----------\\n']}, 'wandb-events.jsonl': {'offset': 48, 'content': ['{\"system.gpu.0.gpu\": 59.27, \"system.gpu.0.memory\": 27.4, \"system.gpu.0.memory_allocated\": 61.5, \"system.gpu.0.temp\": 49.87, \"system.cpu\": 55.62, \"system.memory\": 8.23, \"system.disk\": 6.6, \"system.proc.memory.availableMB\": 48020.33, \"system.proc.memory.rssMB\": 3374.06, \"system.proc.memory.percent\": 6.45, \"system.proc.cpu.threads\": 25.0, \"system.network.sent\": 1604463, \"system.network.recv\": 2787676, \"_wandb\": true, \"_timestamp\": 1552334131, \"_runtime\": 1498}\\n']}, 'wandb-history.jsonl': {'offset': 64, 'content': ['{\"epoch_loss\": 0.5975435062898661, \"epoch_acc\": 0.8415841584158416, \"_runtime\": 1473.5113427639008, \"_timestamp\": 1552334107.201678, \"_step\": 64}\\n', '{\"epoch_loss\": 0.6151737332737485, \"epoch_acc\": 0.8415841584158416, \"_runtime\": 1496.014747619629, \"_timestamp\": 1552334129.7050836, \"_step\": 65}\\n']}, 'wandb-summary.json': {'offset': 0, 'content': ['{\\n    \"epoch_loss\": 0.6151737332737485,\\n    \"epoch_acc\": 0.8415841584158416,\\n    \"_runtime\": 1496.014747619629,\\n    \"_timestamp\": 1552334129.7050836,\\n    \"_step\": 65\\n}\\n']}}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.5749, acc: 0.8416\n",
      "Epoch 68/100\n",
      "----------\n",
      "valid loss: 0.5958, acc: 0.8515\n",
      "Epoch 69/100\n",
      "----------\n",
      "valid loss: 0.5911, acc: 0.8416\n",
      "Epoch 70/100\n",
      "----------\n",
      "valid loss: 0.6404, acc: 0.8416\n",
      "Epoch 71/100\n",
      "----------\n",
      "valid loss: 0.6206, acc: 0.8416\n",
      "Epoch 72/100\n",
      "----------\n",
      "valid loss: 0.5969, acc: 0.8515\n",
      "Epoch 73/100\n",
      "----------\n",
      "valid loss: 0.6010, acc: 0.8449\n",
      "Epoch 74/100\n",
      "----------\n",
      "valid loss: 0.5962, acc: 0.8317\n",
      "Epoch 75/100\n",
      "----------\n",
      "valid loss: 0.6392, acc: 0.8218\n",
      "Epoch 76/100\n",
      "----------\n",
      "valid loss: 0.6121, acc: 0.8317\n",
      "Epoch 77/100\n",
      "----------\n",
      "valid loss: 0.6188, acc: 0.8284\n",
      "Epoch 78/100\n",
      "----------\n",
      "valid loss: 0.5735, acc: 0.8251\n",
      "Epoch 79/100\n",
      "----------\n",
      "valid loss: 0.6542, acc: 0.8251\n",
      "Epoch 80/100\n",
      "----------\n",
      "valid loss: 0.6034, acc: 0.8218\n",
      "Epoch 81/100\n",
      "----------\n",
      "valid loss: 0.6070, acc: 0.8383\n",
      "Epoch 82/100\n",
      "----------\n",
      "valid loss: 0.6207, acc: 0.8317\n",
      "Epoch 83/100\n",
      "----------\n",
      "valid loss: 0.6551, acc: 0.8482\n",
      "Epoch 84/100\n",
      "----------\n",
      "valid loss: 0.6207, acc: 0.8317\n",
      "Epoch 85/100\n",
      "----------\n",
      "valid loss: 0.5911, acc: 0.8515\n",
      "Epoch 86/100\n",
      "----------\n",
      "valid loss: 0.6371, acc: 0.8416\n",
      "Epoch 87/100\n",
      "----------\n",
      "valid loss: 0.6231, acc: 0.8449\n",
      "Epoch 88/100\n",
      "----------\n",
      "valid loss: 0.5906, acc: 0.8218\n",
      "Epoch 89/100\n",
      "----------\n",
      "valid loss: 0.5877, acc: 0.8515\n",
      "Epoch 90/100\n",
      "----------\n",
      "valid loss: 0.5911, acc: 0.8515\n",
      "Epoch 91/100\n",
      "----------\n",
      "valid loss: 0.6036, acc: 0.8581\n",
      "Epoch 92/100\n",
      "----------\n",
      "valid loss: 0.5658, acc: 0.8482\n",
      "Epoch 93/100\n",
      "----------\n",
      "valid loss: 0.5883, acc: 0.8581\n",
      "Epoch 94/100\n",
      "----------\n",
      "valid loss: 0.6470, acc: 0.8449\n",
      "Epoch 95/100\n",
      "----------\n",
      "valid loss: 0.6073, acc: 0.8581\n",
      "Epoch 96/100\n",
      "----------\n",
      "valid loss: 0.6132, acc: 0.8350\n",
      "Epoch 97/100\n",
      "----------\n",
      "valid loss: 0.5887, acc: 0.8449\n",
      "Epoch 98/100\n",
      "----------\n",
      "valid loss: 0.6778, acc: 0.8251\n",
      "Epoch 99/100\n",
      "----------\n",
      "valid loss: 0.6335, acc: 0.8449\n",
      "Epoch 100/100\n",
      "----------\n",
      "valid loss: 0.6980, acc: 0.8251\n"
     ]
    }
   ],
   "source": [
    "#AdamW constant learning rate at 1e-5\n",
    "model_trained = train_model(model, criterion, optimizer, dataloaders, scheduler=None, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (maskrcnn_benchmark)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
